"""
Word2GM Training Script

Simple training script that uses the TFRecord data generated by the dataprep pipeline.
"""

import os
import sys
import time
import argparse
from pathlib import Path

import tensorflow as tf
import numpy as np

# Add src to path
src_path = Path(__file__).parent.parent.parent
sys.path.insert(0, str(src_path))

from word2gm_fast.models.word2gm_model import Word2GMModel
from word2gm_fast.models.config import Word2GMConfig
from word2gm_fast.dataprep.tfrecord_io import read_triplets_from_tfrecord, read_vocab_from_tfrecord
from word2gm_fast.training.training_utils import train_step, log_training_metrics, summarize_dataset_pipeline
from word2gm_fast.utils.tf_silence import import_tensorflow_silently


def load_training_data(artifacts_dir: str, batch_size: int = 128) -> tuple:
    """
    Load training data from TFRecord files.
    
    Parameters
    ----------
    artifacts_dir : str
        Directory containing triplets.tfrecord and vocab.tfrecord files
    batch_size : int
        Batch size for training
        
    Returns
    -------
    tuple
        (dataset, vocab_size, vocab_table)
    """
    triplets_path = os.path.join(artifacts_dir, "triplets.tfrecord")
    vocab_path = os.path.join(artifacts_dir, "vocab.tfrecord")
    
    if not os.path.exists(triplets_path):
        raise FileNotFoundError(f"Triplets file not found: {triplets_path}")
    if not os.path.exists(vocab_path):
        raise FileNotFoundError(f"Vocabulary file not found: {vocab_path}")
    
    # Load vocabulary
    vocab_table = read_vocab_from_tfrecord(vocab_path)
    vocab_size = vocab_table.size()
    
    # Load triplets dataset
    dataset = read_triplets_from_tfrecord(triplets_path)
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    
    return dataset, vocab_size, vocab_table


def train_word2gm(
    artifacts_dir: str,
    output_dir: str,
    config: Word2GMConfig,
    verbose: bool = True,
    use_tensorboard: bool = True
):
    """
    Train Word2GM model on TFRecord data.
    
    Parameters
    ----------
    artifacts_dir : str
        Directory containing training artifacts (triplets.tfrecord, vocab.tfrecord)
    output_dir : str
        Directory to save trained model
    config : Word2GMConfig
        Training configuration
    verbose : bool
        Whether to print progress
    use_tensorboard : bool
        Whether to enable TensorBoard logging
    """
    os.makedirs(output_dir, exist_ok=True)
    
    if verbose:
        print("Loading training data...")
    
    # Load data
    dataset, vocab_size_actual, vocab_table = load_training_data(artifacts_dir, config.batch_size)
    
    # Update config with actual vocab size
    config.vocab_size = int(vocab_size_actual)
    
    if verbose:
        print(f"Vocabulary size: {config.vocab_size:,}")
        print("Dataset pipeline:")
        summarize_dataset_pipeline(dataset)
        print()
    
    # Create model
    model = Word2GMModel(config)
    
    # Create optimizer
    if config.adagrad:
        optimizer = tf.keras.optimizers.Adagrad(learning_rate=config.learning_rate)
    else:
        optimizer = tf.keras.optimizers.SGD(
            learning_rate=config.learning_rate, 
            momentum=0.9, 
            nesterov=True
        )
    
    # Setup TensorBoard (optional)
    summary_writer = None
    if use_tensorboard:
        tensorboard_dir = os.path.join(output_dir, "tensorboard")
        os.makedirs(tensorboard_dir, exist_ok=True)
        summary_writer = tf.summary.create_file_writer(tensorboard_dir)
        if verbose:
            print(f"TensorBoard logs: {tensorboard_dir}")
    
    if verbose:
        print(f"Model created with {config.num_mixtures} mixture components")
        print(f"Optimizer: {'Adagrad' if config.adagrad else 'SGD'}")
        print(f"Learning rate: {config.learning_rate}")
        print()
    
    # Training loop
    best_loss = float('inf')
    
    for epoch in range(config.epochs_to_train):
        epoch_start = time.time()
        epoch_loss = 0.0
        num_batches = 0
        
        if verbose:
            print(f"Epoch {epoch + 1}/{config.epochs_to_train}")
        
        for batch_idx, (word_ids, pos_ids, neg_ids) in enumerate(dataset):
            # Training step using optimized function from training_utils
            loss, grads = train_step(
                model, optimizer, word_ids, pos_ids, neg_ids,
                normclip=config.normclip,
                norm_cap=config.norm_cap,
                lower_sig=config.lower_sig,
                upper_sig=config.upper_sig,
                wout=config.wout
            )
            
            epoch_loss += loss
            num_batches += 1
            
            # TensorBoard logging
            if summary_writer and batch_idx % 100 == 0:
                global_step = epoch * 1000 + batch_idx
                with summary_writer.as_default():
                    tf.summary.scalar("batch_loss", loss, step=global_step)
                    tf.summary.scalar("learning_rate", optimizer.learning_rate, step=global_step)
                    
                    # Detailed metrics every 500 steps
                    if batch_idx % 500 == 0:
                        log_training_metrics(model, grads, step=global_step, summary_writer=summary_writer)
            
            # Print progress
            if verbose and batch_idx % 100 == 0 and batch_idx > 0:
                avg_loss = epoch_loss / num_batches
                print(f"  Batch {batch_idx}: loss = {loss:.6f}, avg = {avg_loss:.6f}")
        
        # Epoch summary
        avg_loss = epoch_loss / max(1, num_batches)
        epoch_time = time.time() - epoch_start
        
        # TensorBoard epoch logging
        if summary_writer:
            with summary_writer.as_default():
                tf.summary.scalar("epoch_loss", avg_loss, step=epoch)
                tf.summary.scalar("epoch_time", epoch_time, step=epoch)
        
        if verbose:
            print(f"  Epoch {epoch + 1} complete: avg_loss = {avg_loss:.6f}, time = {epoch_time:.1f}s")
            
            # Log some model statistics
            print(f"  Mean norm: {tf.reduce_mean(tf.norm(model.mus, axis=-1)):.4f}")
            print(f"  Mean variance: {tf.reduce_mean(tf.exp(model.logsigmas)):.4f}")
            print()
        
        # Save best model
        if avg_loss < best_loss:
            best_loss = avg_loss
            model_path = os.path.join(output_dir, "best_model")
            model.save_weights(model_path)
            
            if verbose:
                print(f"  New best model saved: {best_loss:.6f}")
    
    # Save final model
    final_model_path = os.path.join(output_dir, "final_model")
    model.save_weights(final_model_path)
    
    # Save config
    import json
    config_path = os.path.join(output_dir, "config.json")
    with open(config_path, 'w') as f:
        json.dump(config.__dict__, f, indent=2)
    
    # Cleanup TensorBoard
    if summary_writer:
        summary_writer.flush()
        summary_writer.close()
    
    if verbose:
        print(f"Training complete! Models saved to {output_dir}")
        print(f"Best loss: {best_loss:.6f}")
        if use_tensorboard:
            print(f"View training metrics: tensorboard --logdir {os.path.join(output_dir, 'tensorboard')}")
    
    return model


def main():
    """Main training script."""
    parser = argparse.ArgumentParser(description="Train Word2GM model")
    parser.add_argument("--artifacts_dir", required=True, 
                       help="Directory containing triplets.tfrecord and vocab.tfrecord")
    parser.add_argument("--output_dir", required=True,
                       help="Directory to save trained model")
    
    # Model parameters
    parser.add_argument("--embedding_size", type=int, default=50,
                       help="Embedding dimension")
    parser.add_argument("--num_mixtures", type=int, default=2,
                       help="Number of mixture components")
    parser.add_argument("--spherical", action="store_true", default=True,
                       help="Use spherical covariances")
    
    # Training parameters
    parser.add_argument("--learning_rate", type=float, default=0.05,
                       help="Learning rate")
    parser.add_argument("--batch_size", type=int, default=128,
                       help="Batch size")
    parser.add_argument("--epochs", type=int, default=10,
                       help="Number of epochs")
    parser.add_argument("--adagrad", action="store_true", default=True,
                       help="Use Adagrad optimizer")
    
    # Regularization
    parser.add_argument("--var_scale", type=float, default=0.05,
                       help="Variance scale for initialization")
    parser.add_argument("--normclip", action="store_true", default=True,
                       help="Enable norm clipping")
    
    # Monitoring
    parser.add_argument("--tensorboard", action="store_true", default=True,
                       help="Enable TensorBoard logging")
    
    args = parser.parse_args()
    
    # Create config
    config = Word2GMConfig(
        vocab_size=0,  # Will be set from data
        embedding_size=args.embedding_size,
        num_mixtures=args.num_mixtures,
        spherical=args.spherical,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size,
        epochs_to_train=args.epochs,
        adagrad=args.adagrad,
        var_scale=args.var_scale,
        normclip=args.normclip
    )
    
    # Force CPU mode for compatibility
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
    tf = import_tensorflow_silently(force_cpu=True)
    
    print("Word2GM Training")
    print("=" * 50)
    print(f"Artifacts directory: {args.artifacts_dir}")
    print(f"Output directory: {args.output_dir}")
    print(f"Configuration: {config}")
    print()
    
    # Train model
    model = train_word2gm(
        args.artifacts_dir, 
        args.output_dir, 
        config,
        use_tensorboard=args.tensorboard
    )
    
    print("Training complete!")


if __name__ == "__main__":
    main()
