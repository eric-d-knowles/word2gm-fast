"""
Word2GM Training Script

Notebook-friendly training script for Word2GM using TFRecord data generated by
the dataprep pipeline. No CLI functionality; all configuration is via function
arguments.
"""

import os

import sys
import time
from pathlib import Path

import tensorflow as tf
import numpy as np

# Add src to path (for notebook use, this is usually not needed, but kept for compatibility)
src_path = Path(__file__).parent.parent.parent
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from word2gm_fast.models.word2gm_model import Word2GMModel
from word2gm_fast.models.config import Word2GMConfig
from word2gm_fast.utils.tfrecord_io import (
    read_triplets_from_tfrecord,
    read_vocab_from_tfrecord
)
from word2gm_fast.training.training_utils import (
    train_step,
    log_training_metrics,
    summarize_dataset_pipeline
)
from word2gm_fast.utils.tf_silence import import_tensorflow_silently


def load_training_data(artifacts_dir: str, batch_size: int = 128) -> tuple:
    """
    Load training data from TFRecord files.

    Parameters
    ----------
    artifacts_dir : str
        Directory containing triplets.tfrecord and vocab.tfrecord.
    batch_size : int, optional
        Batch size for the dataset (default: 128).

    Returns
    -------
    tuple
        (dataset, vocab_size, vocab_table)
    """
    # Mixed precision policy is now configurable
    pass  # Policy set in run_notebook_training or by user
    triplets_path = os.path.join(artifacts_dir, "triplets.tfrecord")
    vocab_path = os.path.join(artifacts_dir, "vocab.tfrecord")
    if not os.path.exists(triplets_path):
        raise FileNotFoundError(f"Triplets file not found: {triplets_path}")
    if not os.path.exists(vocab_path):
        raise FileNotFoundError(f"Vocabulary file not found: {vocab_path}")
    vocab_table = read_vocab_from_tfrecord(vocab_path)
    vocab_size = vocab_table.size()
    dataset = read_triplets_from_tfrecord(triplets_path)
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return dataset, vocab_size, vocab_table


def train_word2gm(
    artifacts_dir: str,
    output_dir: str,
    config: Word2GMConfig,
    verbose: bool = True,
    use_tensorboard: bool = True
):
    """
    Train Word2GM model on TFRecord data.

    Parameters
    ----------
    artifacts_dir : str
        Directory containing training artifacts (triplets.tfrecord,
        vocab.tfrecord).
    output_dir : str
        Directory to save trained model.
    config : Word2GMConfig
        Training configuration.
    verbose : bool, optional
        Whether to print progress (default: True).
    use_tensorboard : bool, optional
        Whether to enable TensorBoard logging (default: True).

    Returns
    -------
    model : Word2GMModel
        The trained Word2GM model instance.
    """
    os.makedirs(output_dir, exist_ok=True)
    
    if verbose:
        print("Loading training data...")
    
    # Load data
    dataset, vocab_size_actual, vocab_table = load_training_data(
        artifacts_dir, config.batch_size
    )
    
    # Update config with actual vocab size
    config.vocab_size = int(vocab_size_actual)
    
    if verbose:
        print(f"Vocabulary size: {config.vocab_size:,}")
        print("Dataset pipeline:")
        summarize_dataset_pipeline(dataset)
        print()
    
    # Create model
    model = Word2GMModel(config)
    
    # Create optimizer
    if config.adagrad:
        optimizer = tf.keras.optimizers.Adagrad(learning_rate=config.learning_rate)
    else:
        optimizer = tf.keras.optimizers.SGD(
            learning_rate=config.learning_rate,
            momentum=0.9,
            nesterov=True
        )
    
    # Setup TensorBoard (optional)
    summary_writer = None
    if use_tensorboard:
        tensorboard_dir = os.path.join(output_dir, "tensorboard")
        os.makedirs(tensorboard_dir, exist_ok=True)
        summary_writer = tf.summary.create_file_writer(tensorboard_dir)
        if verbose:
            print(f"TensorBoard logs: {tensorboard_dir}")
    
    if verbose:
        print(f"Model created with {config.num_mixtures} mixture components")
        print(f"Optimizer: {'Adagrad' if config.adagrad else 'SGD'}")
        print(f"Learning rate: {config.learning_rate}")
        print()
    
    # Training loop
    best_loss = float('inf')
    
    for epoch in range(config.epochs_to_train):
        epoch_start = time.time()
        epoch_loss = 0.0
        num_batches = 0
        
        if verbose:
            print(f"Epoch {epoch + 1}/{config.epochs_to_train}")
        
        for batch_idx, (word_ids, pos_ids, neg_ids) in enumerate(dataset):
            # Training step using optimized function from training_utils
            loss, grads = train_step(
                model, optimizer, word_ids, pos_ids, neg_ids,
                normclip=config.normclip,
                norm_cap=config.norm_cap,
                lower_sig=config.lower_sig,
                upper_sig=config.upper_sig,
                wout=config.wout
            )
            
            epoch_loss += loss
            num_batches += 1
            
            # TensorBoard logging
            if summary_writer and batch_idx % 100 == 0:
                global_step = epoch * 1000 + batch_idx
                with summary_writer.as_default():
                    tf.summary.scalar("batch_loss", loss, step=global_step)
                    tf.summary.scalar(
                        "learning_rate",
                        optimizer.learning_rate,
                        step=global_step
                    )
                    # Detailed metrics every 500 steps
                    if batch_idx % 500 == 0:
                        log_training_metrics(
                            model, grads, step=global_step,
                            summary_writer=summary_writer
                        )
            
            # Print progress
            if verbose and batch_idx % 100 == 0 and batch_idx > 0:
                avg_loss = epoch_loss / num_batches
                print(
                    f"  Batch {batch_idx}: loss = {loss:.6f}, avg = {avg_loss:.6f}"
                )
        
        # Epoch summary
        avg_loss = epoch_loss / max(1, num_batches)
        epoch_time = time.time() - epoch_start
        
        # TensorBoard epoch logging
        if summary_writer:
            with summary_writer.as_default():
                tf.summary.scalar("epoch_loss", avg_loss, step=epoch)
                tf.summary.scalar("epoch_time", epoch_time, step=epoch)
        
        if verbose:
            print(
                f"  Epoch {epoch + 1} complete: avg_loss = {avg_loss:.6f}, "
                f"time = {epoch_time:.1f}s"
            )
            # Log some model statistics
            print(
                f"  Mean norm: {tf.reduce_mean(tf.norm(model.mus, axis=-1)):.4f}"
            )
            print(
                f"  Mean variance: {tf.reduce_mean(tf.exp(model.logsigmas)):.4f}"
            )
            print()
        
        # Save best model
        if avg_loss < best_loss:
            best_loss = avg_loss
            model_path = os.path.join(output_dir, "best_model")
            model.save_weights(model_path)
            if verbose:
                print(f"  New best model saved: {best_loss:.6f}")
    
    # Save final model
    final_model_path = os.path.join(output_dir, "final_model")
    model.save_weights(final_model_path)
    
    # Save config
    import json
    config_path = os.path.join(output_dir, "config.json")
    with open(config_path, 'w', encoding="utf-8") as f:
        json.dump(config.__dict__, f, indent=2)
    
    # Cleanup TensorBoard
    if summary_writer:
        summary_writer.flush()
        summary_writer.close()
    
    if verbose:
        print(f"Training complete! Models saved to {output_dir}")
        print(f"Best loss: {best_loss:.6f}")
        if use_tensorboard:
            print(
                "View training metrics: tensorboard --logdir "
                f"{os.path.join(output_dir, 'tensorboard')}"
            )
    
    return model
