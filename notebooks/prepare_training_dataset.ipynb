{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ce6ba",
   "metadata": {},
   "source": [
    "# Word2GM Training Data Pipeline\n",
    "\n",
    "**Pipeline: Corpus file → TFRecord training artifacts (triplets and vocabulary)**\n",
    "\n",
    "Use this notebook to prepare a Google 5gram corpora for Word2GM skip-gram training.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Input**: Preprocessed corpus file (e.g., `2019.txt`) in `/vast` NVMe storage\n",
    "2. **Processing**: TensorFlow-native filtering, vocabulary building, and triplet generation\n",
    "3. **Output**: TFRecord artifacts in organized subdirectories (e.g., `2019_artifacts/`)\n",
    "\n",
    "### **Artifact Storage**\n",
    "The pipeline creates year-specific subdirectories alongside the original text corpora:\n",
    "<pre>\n",
    "/vast/edk202/NLP_corpora/.../data/\n",
    "├── 2018.txt\n",
    "├── 2019.txt\n",
    "├── 2020.txt\n",
    "├── 2018_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "├── 2019_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "└── 2020_artifacts/\n",
    "    ├── triplets.tfrecord.gz\n",
    "    └── vocab.tfrecord.gz\n",
    "</pre>\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df61c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "TensorFlow version: 2.19.0\n",
      "Setup complete; all modules loaded successfully\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup project path\n",
    "project_root = Path('/scratch/edk202/word2gm-fast')\n",
    "os.chdir(project_root)\n",
    "src_path = project_root / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import TensorFlow silently\n",
    "from word2gm_fast.utils.tf_silence import import_tensorflow_silently\n",
    "tf = import_tensorflow_silently()\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Import core dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Import Word2GM modules\n",
    "from word2gm_fast.dataprep.pipeline import batch_prepare_training_data\n",
    "\n",
    "print(\"Setup complete; all modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15f209a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER RESOURCE ALLOCATION SUMMARY\n",
      "==================================================\n",
      "Hostname: gr004.hpc.nyu.edu\n",
      "CPU cores: 2 physical, 48 logical (hyperthreading)\n",
      "Job-allocated CPUs: 14 (SLURM)\n",
      "Job-allocated memory: 125.0 GB\n",
      "GPU: 1 accessible\n",
      "  GPU 0: /dev/nvidia0\n",
      "\n",
      "STORAGE QUOTAS AND USAGE\n",
      "==================================================\n",
      "Filesystem   Allocation      Used         Percent \n",
      "--------------------------------------------------\n",
      "/home        50.0GB/30.7K    15.57GB      31.13%  \n",
      "/scratch     5.0TB/1.0M      323.29GB     6.31%   \n",
      "/archive     2.0TB/20.5K     1262.12GB    61.63%  \n",
      "/vast        2TB/5.0M        1.37TB       69.0%   \n",
      "==================================================\n",
      "\n",
      "Resource summary complete\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER RESOURCE ALLOCATION SUMMARY\n",
      "==================================================\n",
      "Hostname: gr004.hpc.nyu.edu\n",
      "CPU cores: 2 physical, 48 logical (hyperthreading)\n",
      "Job-allocated CPUs: 14 (SLURM)\n",
      "Job-allocated memory: 125.0 GB\n",
      "GPU: 1 accessible\n",
      "  GPU 0: /dev/nvidia0\n",
      "\n",
      "STORAGE QUOTAS AND USAGE\n",
      "==================================================\n",
      "Filesystem   Allocation      Used         Percent \n",
      "--------------------------------------------------\n",
      "/home        50.0GB/30.7K    15.57GB      31.13%  \n",
      "/scratch     5.0TB/1.0M      323.29GB     6.31%   \n",
      "/archive     2.0TB/20.5K     1262.12GB    61.63%  \n",
      "/vast        2TB/5.0M        1.37TB       69.0%   \n",
      "==================================================\n",
      "\n",
      "Resource summary complete\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'socket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Hostname\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m hostname = \u001b[43msocket\u001b[49m.gethostname()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHostname: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhostname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# CPU Information\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'socket' is not defined"
     ]
    }
   ],
   "source": [
    "# === CLUSTER RESOURCE MONITORING ===\n",
    "import socket\n",
    "import subprocess\n",
    "import glob\n",
    "import psutil\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(\"CLUSTER RESOURCE ALLOCATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Hostname\n",
    "hostname = socket.gethostname()\n",
    "print(f\"Hostname: {hostname}\")\n",
    "\n",
    "# CPU Information\n",
    "logical_cores = psutil.cpu_count(logical=True)\n",
    "try:\n",
    "    # Detect physical cores via thread siblings\n",
    "    physical_cores = 0\n",
    "    seen_cores = set()\n",
    "    \n",
    "    cpu_paths = glob.glob('/sys/devices/system/cpu/cpu[0-9]*')\n",
    "    cpu_numbers = sorted([int(path.split('cpu')[-1]) for path in cpu_paths])\n",
    "    \n",
    "    for cpu_num in cpu_numbers:\n",
    "        if cpu_num in seen_cores:\n",
    "            continue\n",
    "        siblings_path = f'/sys/devices/system/cpu/cpu{cpu_num}/topology/thread_siblings_list'\n",
    "        try:\n",
    "            with open(siblings_path, 'r') as f:\n",
    "                siblings = f.read().strip()\n",
    "            if ',' in siblings:\n",
    "                sibling_list = [int(x) for x in siblings.split(',')]\n",
    "            elif '-' in siblings:\n",
    "                start, end = siblings.split('-')\n",
    "                sibling_list = list(range(int(start), int(end) + 1))\n",
    "            else:\n",
    "                sibling_list = [int(siblings)]\n",
    "            seen_cores.update(sibling_list)\n",
    "            physical_cores += 1\n",
    "        except:\n",
    "            physical_cores += 1\n",
    "            seen_cores.add(cpu_num)\n",
    "    \n",
    "    if physical_cores > 0 and physical_cores != logical_cores:\n",
    "        print(f\"CPU cores: {physical_cores} physical, {logical_cores} logical (hyperthreading)\")\n",
    "    else:\n",
    "        print(f\"CPU cores: {logical_cores} logical\")\n",
    "except:\n",
    "    print(f\"CPU cores: {logical_cores} logical\")\n",
    "\n",
    "# Memory Information\n",
    "memory = psutil.virtual_memory()\n",
    "total_memory_gb = memory.total / (1024**3)\n",
    "available_memory_gb = memory.available / (1024**3)\n",
    "\n",
    "# Check for SLURM memory limits\n",
    "slurm_memory = None\n",
    "try:\n",
    "    slurm_mem_per_node = os.environ.get('SLURM_MEM_PER_NODE')\n",
    "    if slurm_mem_per_node:\n",
    "        slurm_memory = int(slurm_mem_per_node) / 1024  # MB to GB\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if slurm_memory and slurm_memory < total_memory_gb:\n",
    "    print(f\"Job-allocated memory: {slurm_memory:.1f} GB\")\n",
    "else:\n",
    "    print(f\"Memory: {total_memory_gb:.1f} GB total\")\n",
    "\n",
    "# Verify GPU\n",
    "accessible_gpus = []\n",
    "for gpu_id in range(4):\n",
    "    device_path = f\"/dev/nvidia{gpu_id}\"\n",
    "    if os.path.exists(device_path):\n",
    "        try:\n",
    "            with open(device_path, 'rb') as f:\n",
    "                pass\n",
    "            accessible_gpus.append(gpu_id)\n",
    "        except (PermissionError, OSError):\n",
    "            continue\n",
    "\n",
    "if accessible_gpus:\n",
    "    print(f\"GPU: {len(accessible_gpus)} accessible\")\n",
    "    for gpu_id in accessible_gpus:\n",
    "        print(f\"  GPU {gpu_id}: /dev/nvidia{gpu_id}\")\n",
    "else:\n",
    "    print(\"GPU: Not accessible\")\n",
    "\n",
    "print(\"\\nSTORAGE QUOTAS AND USAGE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get quota information\n",
    "try:\n",
    "    cmd = ['ssh', '-o', 'ConnectTimeout=3', '-o', 'BatchMode=yes', \n",
    "           '-o', 'StrictHostKeyChecking=no', 'log-1.hpc.nyu.edu', 'myquota']\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=8)\n",
    "    \n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        \n",
    "        # Clean ANSI color codes from output\n",
    "        ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "        \n",
    "        # Parse quota output\n",
    "        quota_data = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = ansi_escape.sub('', line).strip()  # Remove color codes\n",
    "            if line.startswith('/'):\n",
    "                # Split by whitespace, but handle the complex format\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    filesystem = parts[0]\n",
    "                    allocation = parts[3]\n",
    "                    usage_info = parts[4]\n",
    "                    \n",
    "                    # Extract usage and percentage from usage_info\n",
    "                    usage_parts = usage_info.split('/')\n",
    "                    if len(usage_parts) >= 1:\n",
    "                        space_part = usage_parts[0]\n",
    "                        # Extract usage and percentage using regex\n",
    "                        match = re.match(r'([0-9.]+[KMGT]?B)\\(([0-9.]+)%\\)', space_part)\n",
    "                        if match:\n",
    "                            used = match.group(1)\n",
    "                            percent = match.group(2) + '%'\n",
    "                        else:\n",
    "                            # Try simpler pattern\n",
    "                            if '(' in space_part and ')' in space_part:\n",
    "                                used = space_part.split('(')[0]\n",
    "                                percent_match = re.search(r'\\(([0-9.]+)%\\)', space_part)\n",
    "                                percent = percent_match.group(1) + '%' if percent_match else \"N/A\"\n",
    "                            else:\n",
    "                                used = space_part\n",
    "                                percent = \"N/A\"\n",
    "                    else:\n",
    "                        used = \"N/A\"\n",
    "                        percent = \"N/A\"\n",
    "                    \n",
    "                    quota_data.append({\n",
    "                        'filesystem': filesystem,\n",
    "                        'allocation': allocation,\n",
    "                        'used': used,\n",
    "                        'percent': percent\n",
    "                    })\n",
    "        \n",
    "        if quota_data:\n",
    "            # Print formatted table\n",
    "            print(f\"{'Filesystem':<12} {'Allocation':<15} {'Used':<12} {'Percent':<8}\")\n",
    "            print(\"-\" * 50)\n",
    "            for item in quota_data:\n",
    "                print(f\"{item['filesystem']:<12} {item['allocation']:<15} {item['used']:<12} {item['percent']:<8}\")\n",
    "        else:\n",
    "            print(\"No quota information found\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Quota check failed: {str(e)}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nResource summary complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301858",
   "metadata": {},
   "source": [
    "## Prepare one or more corpora in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e2724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "PARALLEL BATCH PROCESSING\n",
      "=================================================================\n",
      "Processing 2 years\n",
      "Using 2 parallel workers\n",
      "Estimated speedup: 2.0x\n",
      "=================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m corpus_dir = \u001b[33m\"\u001b[39m\u001b[33m/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Process years with optional parallel processing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mbatch_prepare_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcorpus_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43myear_range\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1951-1952\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_summary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/word2gm-fast/src/word2gm_fast/dataprep/pipeline.py:506\u001b[39m, in \u001b[36mbatch_prepare_training_data\u001b[39m\u001b[34m(corpus_dir, years, year_range, compress, show_progress, show_summary, max_workers, use_multiprocessing)\u001b[39m\n\u001b[32m    500\u001b[39m future_to_year = {\n\u001b[32m    501\u001b[39m     executor.submit(_process_single_year, args): args[\u001b[32m0\u001b[39m] \n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m worker_args\n\u001b[32m    503\u001b[39m }\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# Collect results as they complete\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_to_year\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43myear\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_to_year\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompleted_count\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/concurrent/futures/_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n\u001b[32m    246\u001b[39m     finished = waiter.finished_futures\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "\n",
    "# Process years with optional parallel processing\n",
    "results = batch_prepare_training_data(\n",
    "    corpus_dir=corpus_dir,\n",
    "    year_range=\"1951-1952\",\n",
    "    compress=False,\n",
    "    show_progress=True,\n",
    "    show_summary=True,\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "492b8492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: /usr/bin/nvidia-smi: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! /usr/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6114dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/usr/bin/nvidia-smi': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls -l /usr/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27b412a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU Diagnostics\n",
      "========================================\n",
      "CUDA_VISIBLE_DEVICES: 0\n",
      "\n",
      "TensorFlow GPU devices:\n",
      "  No GPU devices detected by TensorFlow\n",
      "\n",
      "GPU computation test with device placement verification:\n",
      "\n",
      "  Testing device: /GPU:0\n",
      "    Requested: /GPU:0\n",
      "    Actually used: /job:localhost/replica:0/task:0/device:CPU:0\n",
      "    Result: [1. 3. 3. 7.]\n",
      "    ⚠️  Device fallback occurred! Requested /GPU:0 but used /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "  Testing device: /GPU:1\n",
      "    Requested: /GPU:1\n",
      "    Actually used: /job:localhost/replica:0/task:0/device:CPU:0\n",
      "    Result: [1. 3. 3. 7.]\n",
      "    ⚠️  Device fallback occurred! Requested /GPU:1 but used /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "  Testing device: /GPU:5\n",
      "    Requested: /GPU:5\n",
      "    Actually used: /job:localhost/replica:0/task:0/device:CPU:0\n",
      "    Result: [1. 3. 3. 7.]\n",
      "    ⚠️  Device fallback occurred! Requested /GPU:5 but used /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "  Testing device: /CPU:0\n",
      "    Requested: /CPU:0\n",
      "    Actually used: /job:localhost/replica:0/task:0/device:CPU:0\n",
      "    Result: [1. 3. 3. 7.]\n",
      "    ⚠️  Device fallback occurred! Requested /CPU:0 but used /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "📋 TensorFlow Soft Device Placement:\n",
      "  - TensorFlow uses 'soft placement' by default\n",
      "  - If requested device doesn't exist, it falls back to available device\n",
      "  - This is why /GPU:5 'succeeds' but actually runs on CPU or /GPU:0\n",
      "  - Use tf.debugging.set_log_device_placement(True) to see actual placement\n",
      "\n",
      "Checking Word2GM pipeline GPU usage...\n",
      "  ⚠️  Pipeline may not be using GPU - check implementation\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# GPU Diagnostics - Check TensorFlow GPU configuration\n",
    "print(\"TensorFlow GPU Diagnostics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check environment variables\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
    "\n",
    "# Check TensorFlow GPU detection\n",
    "print(f\"\\nTensorFlow GPU devices:\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "        # Check memory info\n",
    "        try:\n",
    "            details = tf.config.experimental.get_device_details(gpu)\n",
    "            print(f\"    Details: {details}\")\n",
    "        except:\n",
    "            print(f\"    Details: Not available\")\n",
    "else:\n",
    "    print(\"  No GPU devices detected by TensorFlow\")\n",
    "\n",
    "# Enhanced GPU computation test - shows actual device placement\n",
    "print(f\"\\nGPU computation test with device placement verification:\")\n",
    "\n",
    "def test_device_placement(device_name):\n",
    "    \"\"\"Test computation on specified device and show where it actually ran.\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n  Testing device: {device_name}\")\n",
    "        \n",
    "        # Enable logging to see actual device placement\n",
    "        tf.debugging.set_log_device_placement(True)\n",
    "        \n",
    "        with tf.device(device_name):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            result = c.numpy()\n",
    "            \n",
    "            # Check where the operation actually ran\n",
    "            actual_device = c.device\n",
    "            \n",
    "        tf.debugging.set_log_device_placement(False)\n",
    "        \n",
    "        print(f\"    Requested: {device_name}\")\n",
    "        print(f\"    Actually used: {actual_device}\")\n",
    "        print(f\"    Result: {result.flatten()}\")\n",
    "        \n",
    "        # Check if requested device matches actual device\n",
    "        if device_name.lower() in actual_device.lower():\n",
    "            print(f\"    ✅ Computation ran on requested device\")\n",
    "        else:\n",
    "            print(f\"    ⚠️  Device fallback occurred! Requested {device_name} but used {actual_device}\")\n",
    "            \n",
    "        return True, actual_device\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ❌ Computation failed: {e}\")\n",
    "        return False, str(e)\n",
    "\n",
    "# Test various device configurations\n",
    "test_devices = ['/GPU:0', '/GPU:1', '/GPU:5', '/CPU:0']\n",
    "\n",
    "for device in test_devices:\n",
    "    test_device_placement(device)\n",
    "\n",
    "# Show TensorFlow's soft placement behavior\n",
    "print(f\"\\n📋 TensorFlow Soft Device Placement:\")\n",
    "print(f\"  - TensorFlow uses 'soft placement' by default\")\n",
    "print(f\"  - If requested device doesn't exist, it falls back to available device\")\n",
    "print(f\"  - This is why /GPU:5 'succeeds' but actually runs on CPU or /GPU:0\")\n",
    "print(f\"  - Use tf.debugging.set_log_device_placement(True) to see actual placement\")\n",
    "\n",
    "# Check if pipeline functions are using GPU\n",
    "print(f\"\\nChecking Word2GM pipeline GPU usage...\")\n",
    "try:\n",
    "    # Check if the pipeline functions have device placement\n",
    "    import inspect\n",
    "    from word2gm_fast.dataprep import pipeline\n",
    "    \n",
    "    # Look for GPU-related code in the pipeline\n",
    "    source = inspect.getsource(pipeline.batch_prepare_training_data)\n",
    "    if 'GPU' in source or 'device' in source:\n",
    "        print(\"  ✅ Pipeline appears to have GPU support\")\n",
    "    else:\n",
    "        print(\"  ⚠️  Pipeline may not be using GPU - check implementation\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Could not inspect pipeline: {e}\")\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4733c1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TensorFlow Soft Device Placement Demonstration\n",
      "=======================================================\n",
      "Testing various GPU IDs:\n",
      "------------------------------\n",
      "  ❌ Failed: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\n",
      "  ❌ Failed: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:1'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\n",
      "  ❌ Failed: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:2'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\n",
      "  ❌ Failed: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:5'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\n",
      "  ❌ Failed: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:99'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\n",
      "\n",
      "💡 Key Points:\n",
      "• TensorFlow's default 'soft placement' allows fallback to available devices\n",
      "• Operations 'succeed' even if the requested GPU doesn't exist\n",
      "• Always check the actual device used (tensor.device) to verify placement\n",
      "• Use tf.debugging.set_log_device_placement(True) for detailed logging\n",
      "\n",
      "🛠️  To force strict device placement (fail if device unavailable):\n",
      "   tf.config.set_soft_device_placement(False)\n",
      "   # This will make /GPU:5 actually fail if GPU 5 doesn't exist\n"
     ]
    }
   ],
   "source": [
    "# Demonstration: Why GPU tests \"succeed\" even with invalid GPU IDs\n",
    "print(\"🔍 TensorFlow Soft Device Placement Demonstration\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "def quick_gpu_test(gpu_id):\n",
    "    \"\"\"Quick test to show where computation actually runs.\"\"\"\n",
    "    device_name = f'/GPU:{gpu_id}'\n",
    "    \n",
    "    try:\n",
    "        with tf.device(device_name):\n",
    "            # Simple computation\n",
    "            x = tf.constant([1.0, 2.0])\n",
    "            y = tf.square(x)\n",
    "            result = y.numpy()\n",
    "            \n",
    "            # Check actual device\n",
    "            actual_device = y.device\n",
    "            \n",
    "        print(f\"GPU {gpu_id}: Requested={device_name}, Actually used={actual_device}\")\n",
    "        \n",
    "        # Show if fallback occurred\n",
    "        if f\"GPU:{gpu_id}\" in actual_device:\n",
    "            return \"✅ Used requested GPU\"\n",
    "        else:\n",
    "            return \"⚠️  FALLBACK occurred\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"❌ Failed: {e}\"\n",
    "\n",
    "print(\"Testing various GPU IDs:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for gpu_id in [0, 1, 2, 5, 99]:\n",
    "    status = quick_gpu_test(gpu_id)\n",
    "    print(f\"  {status}\")\n",
    "\n",
    "print(\"\\n💡 Key Points:\")\n",
    "print(\"• TensorFlow's default 'soft placement' allows fallback to available devices\")\n",
    "print(\"• Operations 'succeed' even if the requested GPU doesn't exist\")\n",
    "print(\"• Always check the actual device used (tensor.device) to verify placement\")\n",
    "print(\"• Use tf.debugging.set_log_device_placement(True) for detailed logging\")\n",
    "\n",
    "print(\"\\n🛠️  To force strict device placement (fail if device unavailable):\")\n",
    "print(\"   tf.config.set_soft_device_placement(False)\")\n",
    "print(\"   # This will make /GPU:5 actually fail if GPU 5 doesn't exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
