{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ce6ba",
   "metadata": {},
   "source": [
    "# Word2GM Training Data Pipeline\n",
    "\n",
    "**Pipeline: Corpus file → TFRecord training artifacts (triplets and vocabulary)**\n",
    "\n",
    "Use this notebook to prepare a Google 5gram corpora for Word2GM skip-gram training.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Input**: Preprocessed corpus file (e.g., `2019.txt`) in `/vast` NVMe storage\n",
    "2. **Processing**: TensorFlow-native filtering, vocabulary building, and triplet generation\n",
    "3. **Output**: TFRecord artifacts in organized subdirectories (e.g., `2019_artifacts/`)\n",
    "\n",
    "### **Artifact Storage**\n",
    "The pipeline creates year-specific subdirectories alongside the original text corpora:\n",
    "<pre>\n",
    "/vast/edk202/NLP_corpora/.../data/\n",
    "├── 2018.txt\n",
    "├── 2019.txt\n",
    "├── 2020.txt\n",
    "├── 2018_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "├── 2019_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "└── 2020_artifacts/\n",
    "    ├── triplets.tfrecord.gz\n",
    "    └── vocab.tfrecord.gz\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49f604",
   "metadata": {},
   "source": [
    "## Set Up for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89181d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>Autoreload enabled</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Project root: /scratch/edk202/word2gm-fast\n",
       "TensorFlow version: 2.19.0\n",
       "Device mode: CPU-only</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Data preprocessing environment ready!</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set project root directory and add `src` to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import the notebook setup utilities\n",
    "from word2gm_fast.utils.notebook_setup import setup_data_preprocessing_notebook, enable_autoreload\n",
    "\n",
    "# Enable autoreload for development\n",
    "enable_autoreload()\n",
    "\n",
    "# Set up environment (CPU-only for data preprocessing)\n",
    "env = setup_data_preprocessing_notebook(project_root=PROJECT_ROOT)\n",
    "\n",
    "# Extract commonly used modules for convenience\n",
    "tf = env['tensorflow']\n",
    "np = env['numpy']\n",
    "pd = env['pandas']\n",
    "batch_prepare_training_data = env['batch_prepare_training_data']\n",
    "print_resource_summary = env['print_resource_summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72a7c6",
   "metadata": {},
   "source": [
    "## Print Resource Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43483cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>SYSTEM RESOURCE SUMMARY\n",
       "============================================================\n",
       "Hostname: cm016.hpc.nyu.edu\n",
       "\n",
       "Job Allocation:\n",
       "   CPUs: 14\n",
       "   Memory: 125.0 GB\n",
       "   Requested partitions: short,cs,cm,cpu_a100_2,cpu_a100_1,cpu_gpu\n",
       "   Running on: SSH failed: Host key verification failed.\n",
       "   Job ID: 63299322\n",
       "   Node list: cm016\n",
       "\n",
       "GPU Information:\n",
       "   Error: NVML Shared Library Not Found\n",
       "\n",
       "TensorFlow GPU Detection:\n",
       "   TensorFlow detects 0 GPU(s)\n",
       "   Built with CUDA: True\n",
       "============================================================</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301858",
   "metadata": {},
   "source": [
    "## Prepare Corpora\n",
    "\n",
    "Here, we run the data-preparation pipeline from start to finish — reading preprocessed ngram corpora, generating all valid triplets, extracting the vocabulary, and saving the triplets and vocabulary as `tfrecord` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14da0c0",
   "metadata": {},
   "source": [
    "### Options for Data Preparation\n",
    "\n",
    "You can control which years are processed and how the batch preparation runs by adjusting the arguments to `batch_prepare_training_data`:\n",
    "\n",
    "**Ways to specify years:**\n",
    "- `year_range=\"2010\"` — Process a single year (e.g., only 2010).\n",
    "- `year_range=\"2010,2012,2015\"` — Process a comma-separated list of years.\n",
    "- `year_range=\"2010-2015\"` — Process a range of years, inclusive (2010 through 2015).\n",
    "- `year_range=\"2010,2012-2014,2016\"` — Combine individual years and ranges (2010, 2012, 2013, 2014, 2016).\n",
    "\n",
    "**Other options:**\n",
    "- `compress` — If `True`, output TFRecords are gzip-compressed. If `False`, output is uncompressed.\n",
    "- `show_progress` — If `True`, display a progress bar for each year.\n",
    "- `show_summary` — If `True`, print a summary of the processed data for each year.\n",
    "- `use_multiprocessing` — If `True`, process years in parallel using multiple CPU cores (recommended for large datasets).\n",
    "\n",
    "See the function docstring or source for more advanced options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e2724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "PARALLEL BATCH PROCESSING\n",
      "=================================================================\n",
      "Processing 100 years\n",
      "Using 14 parallel workers\n",
      "Estimated speedup: 14.0x\n",
      "=================================================================\n",
      "1825 complete (25/100): 1,187,396 triplets, 23,176 vocab, 712.2s\n",
      "1820 complete (26/100): 1,640,775 triplets, 24,563 vocab, 908.9s\n",
      "1822 complete (27/100): 1,583,366 triplets, 25,169 vocab, 908.9s\n",
      "1824 complete (28/100): 1,567,353 triplets, 24,419 vocab, 892.2s\n",
      "1830 complete (29/100): 1,503,816 triplets, 25,450 vocab, 870.7s\n",
      "1829 complete (30/100): 2,057,033 triplets, 27,319 vocab, 1179.7s\n",
      "1837 complete (31/100): 1,235,211 triplets, 25,101 vocab, 703.6s\n",
      "1835 complete (32/100): 1,433,895 triplets, 25,335 vocab, 821.7s\n",
      "1831 complete (33/100): 1,993,305 triplets, 27,548 vocab, 1084.8s\n",
      "1833 complete (34/100): 1,729,847 triplets, 26,916 vocab, 1003.6s\n",
      "1838 complete (35/100): 1,502,966 triplets, 26,324 vocab, 833.6s\n",
      "1832 complete (36/100): 2,185,572 triplets, 27,982 vocab, 1206.4s\n",
      "1834 complete (37/100): 2,415,856 triplets, 28,853 vocab, 1318.6s\n",
      "1841 complete (38/100): 1,698,066 triplets, 26,666 vocab, 974.8s\n",
      "1836 complete (39/100): 2,299,374 triplets, 29,166 vocab, 1279.7s\n",
      "1840 complete (40/100): 2,031,941 triplets, 27,640 vocab, 1154.7s\n",
      "1839 complete (41/100): 2,443,839 triplets, 30,160 vocab, 1375.0s\n",
      "1843 complete (42/100): 2,206,785 triplets, 29,418 vocab, 1260.5s\n",
      "1844 complete (43/100): 2,152,821 triplets, 29,014 vocab, 1218.1s\n",
      "1846 complete (44/100): 2,274,030 triplets, 29,618 vocab, 1257.9s\n",
      "1848 complete (45/100): 2,192,095 triplets, 29,711 vocab, 1211.3s\n",
      "1845 complete (46/100): 2,279,473 triplets, 29,980 vocab, 1303.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804 complete (1/100): 128,037 triplets, 9,769 vocab, 97.1s\n",
      "1801 complete (2/100): 157,360 triplets, 11,000 vocab, 105.8s\n",
      "1802 complete (3/100): 92,701 triplets, 8,487 vocab, 110.0s\n",
      "1805 complete (4/100): 190,798 triplets, 11,609 vocab, 135.5s\n",
      "1808 complete (5/100): 317,657 triplets, 13,193 vocab, 198.6s\n",
      "1803 complete (6/100): 349,757 triplets, 14,473 vocab, 208.7s\n",
      "1807 complete (7/100): 369,858 triplets, 14,626 vocab, 218.4s\n",
      "1814 complete (8/100): 301,143 triplets, 14,965 vocab, 221.1s\n",
      "1806 complete (9/100): 403,392 triplets, 14,124 vocab, 245.6s\n",
      "1813 complete (10/100): 508,850 triplets, 17,428 vocab, 306.5s\n",
      "1816 complete (11/100): 390,078 triplets, 16,144 vocab, 235.6s\n",
      "1811 complete (12/100): 691,955 triplets, 17,916 vocab, 411.9s\n",
      "1812 complete (13/100): 710,549 triplets, 18,423 vocab, 420.4s\n",
      "1817 complete (14/100): 524,951 triplets, 17,803 vocab, 320.6s\n",
      "1809 complete (15/100): 734,668 triplets, 19,226 vocab, 433.1s\n",
      "1815 complete (16/100): 641,679 triplets, 17,905 vocab, 383.7s\n",
      "1818 complete (17/100): 916,709 triplets, 20,694 vocab, 534.5s\n",
      "1810 complete (18/100): 1,184,737 triplets, 20,429 vocab, 692.0s\n",
      "1823 complete (19/100): 919,481 triplets, 21,623 vocab, 524.5s\n",
      "1819 complete (20/100): 1,019,953 triplets, 22,842 vocab, 582.6s\n",
      "1827 complete (21/100): 777,326 triplets, 20,385 vocab, 447.5s\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "\n",
    "# Process years with multiprocessing (CPU-only mode configured in cell 2)\n",
    "results = batch_prepare_training_data(\n",
    "    corpus_dir=corpus_dir,\n",
    "    year_range=\"1801-1900\",\n",
    "    compress=False,\n",
    "    show_progress=True,\n",
    "    show_summary=True,\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8464b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
