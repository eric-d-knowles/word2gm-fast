{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ce6ba",
   "metadata": {},
   "source": [
    "# Word2GM Training Data Pipeline\n",
    "\n",
    "**Pipeline: Corpus file → TFRecord training artifacts (triplets and vocabulary)**\n",
    "\n",
    "Use this notebook to prepare a Google 5gram corpora for Word2GM skip-gram training.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Input**: Preprocessed corpus file (e.g., `2019.txt`) in `/vast` NVMe storage\n",
    "2. **Processing**: TensorFlow-native filtering, vocabulary building, and triplet generation\n",
    "3. **Output**: TFRecord artifacts in organized subdirectories (e.g., `2019_artifacts/`)\n",
    "\n",
    "### **Artifact Storage**\n",
    "The pipeline creates year-specific subdirectories alongside the original text corpora:\n",
    "<pre>\n",
    "/vast/edk202/NLP_corpora/.../data/\n",
    "├── 2018.txt\n",
    "├── 2019.txt\n",
    "├── 2020.txt\n",
    "├── 2018_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "├── 2019_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "└── 2020_artifacts/\n",
    "    ├── triplets.tfrecord.gz\n",
    "    └── vocab.tfrecord.gz\n",
    "</pre>\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df61c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Running in CPU-only mode (optimal for data preprocessing + multiprocessing)\n",
      "Setup complete; all modules loaded successfully\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup project path\n",
    "project_root = Path('/scratch/edk202/word2gm-fast')\n",
    "os.chdir(project_root)\n",
    "src_path = project_root / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Force CPU-only mode for the entire notebook to avoid GPU/multiprocessing conflicts\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Import TensorFlow silently (will use CPU-only due to environment variable above)\n",
    "from word2gm_fast.utils.tf_silence import import_tensorflow_silently\n",
    "tf = import_tensorflow_silently(force_cpu=True, gpu_memory_growth=False)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"Running in CPU-only mode (optimal for data preprocessing + multiprocessing)\")\n",
    "\n",
    "# Import core dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Import Word2GM modules\n",
    "from word2gm_fast.dataprep.pipeline import batch_prepare_training_data\n",
    "\n",
    "print(\"Setup complete; all modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f209a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM RESOURCE SUMMARY\n",
      "================================================\n",
      "Hostname: cm015.hpc.nyu.edu\n",
      "Job-allocated CPUs: 28\n",
      "Job-allocated memory: 125.0 GB\n",
      "Current partition: short,cs,cm,cpu_a100_2\n",
      "\n",
      "GPU Detection:\n",
      "  No CUDA GPUs detected\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# System Resource Summary\n",
    "import socket\n",
    "import pynvml\n",
    "\n",
    "print(\"SYSTEM RESOURCE SUMMARY\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "# Hostname\n",
    "hostname = socket.gethostname()\n",
    "print(f\"Hostname: {hostname}\")\n",
    "\n",
    "# Job-allocated CPUs (from SLURM)\n",
    "cpus_allocated = int(os.environ.get('SLURM_CPUS_PER_TASK', psutil.cpu_count()))\n",
    "print(f\"Job-allocated CPUs: {cpus_allocated}\")\n",
    "\n",
    "# Job-allocated memory (from SLURM) \n",
    "mem_per_node_mb = os.environ.get('SLURM_MEM_PER_NODE')\n",
    "if mem_per_node_mb:\n",
    "    mem_gb = int(mem_per_node_mb) / 1024\n",
    "    print(f\"Job-allocated memory: {mem_gb:.1f} GB\")\n",
    "\n",
    "# Current partition (actual, not requested)\n",
    "current_partition = os.environ.get('SLURM_JOB_PARTITION', 'unknown')\n",
    "print(f\"Current partition: {current_partition}\")\n",
    "\n",
    "# GPU Detection (CUDA only)\n",
    "print(f\"\\nGPU Detection:\")\n",
    "try:\n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "    print(f\"  CUDA GPUs detected: {device_count}\")\n",
    "    \n",
    "    for i in range(device_count):\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "        name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')\n",
    "        memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        memory_gb = memory_info.total / (1024**3)\n",
    "        print(f\"    GPU {i}: {name} ({memory_gb:.1f} GB)\")\n",
    "        \n",
    "except Exception:\n",
    "    print(\"  No CUDA GPUs detected\")\n",
    "\n",
    "print(\"=\" * 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301858",
   "metadata": {},
   "source": [
    "## Prepare one or more corpora in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e2724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "Processing year 1700 (1/1)\n",
      "=================================================================\n",
      "Starting Word2GM data preparation pipeline\n",
      "Corpus: 1700.txt (0.031 MB)\n",
      "Output: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1700_artifacts\n",
      "\n",
      "Step 1/3: Loading and filtering corpus...\n",
      "   Corpus filtered in 0.051s\n",
      "Step 2/3: Building vocabulary...\n",
      "   Vocabulary built: 325 words in 0.113s\n",
      "Step 3/3: Generating triplets and saving to TFRecord...\n",
      "\n",
      "=================================================================\n",
      "Batch processing complete!\n",
      "=================================================================\n",
      "Successful: 1 years\n",
      "   Total triplets: 381\n",
      "   Average vocab size: 325\n",
      "   Average time per year: 0.4s\n",
      "   Overall triplets/second: 976\n",
      "\n",
      "=================================================================\n",
      "Batch processing complete!\n",
      "=================================================================\n",
      "Successful: 1 years\n",
      "   Total triplets: 381\n",
      "   Average vocab size: 325\n",
      "   Average time per year: 0.4s\n",
      "   Overall triplets/second: 976\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "\n",
    "# Process years with multiprocessing (CPU-only mode configured in cell 2)\n",
    "results = batch_prepare_training_data(\n",
    "    corpus_dir=corpus_dir,\n",
    "    year_range=\"1600-1700\",\n",
    "    compress=False,\n",
    "    show_progress=True,\n",
    "    show_summary=True,\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11cf207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
