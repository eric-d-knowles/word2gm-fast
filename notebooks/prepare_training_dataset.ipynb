{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ce6ba",
   "metadata": {},
   "source": [
    "# Word2GM Training Data Pipeline\n",
    "\n",
    "**Pipeline: Corpus file → TFRecord training artifacts (triplets and vocabulary)**\n",
    "\n",
    "Use this notebook to prepare a Google 5gram corpora for Word2GM skip-gram training.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Input**: Preprocessed corpus file (e.g., `2019.txt`) in `/vast` NVMe storage\n",
    "2. **Processing**: TensorFlow-native filtering, vocabulary building, and triplet generation\n",
    "3. **Output**: TFRecord artifacts in organized subdirectories (e.g., `2019_artifacts/`)\n",
    "\n",
    "### **Artifact Storage**\n",
    "The pipeline creates year-specific subdirectories alongside the original text corpora:\n",
    "<pre>\n",
    "/vast/edk202/NLP_corpora/.../data/\n",
    "├── 2018.txt\n",
    "├── 2019.txt\n",
    "├── 2020.txt\n",
    "├── 2018_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "├── 2019_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "└── 2020_artifacts/\n",
    "    ├── triplets.tfrecord.gz\n",
    "    └── vocab.tfrecord.gz\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49f604",
   "metadata": {},
   "source": [
    "## Set Up for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89181d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Autoreload enabled</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Project root: /scratch/edk202/word2gm-fast\n",
       "TensorFlow version: 2.19.0\n",
       "Device mode: CPU-only</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Data preprocessing environment ready!</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow logging has been aggressively suppressed\n"
     ]
    }
   ],
   "source": [
    "# Set project root directory and add `src` to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Completely suppress TensorFlow logging before any imports\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=DEBUG, 1=INFO, 2=WARNING, 3=ERROR\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress all other unwanted logs\n",
    "for logger_name in ['tensorflow', 'absl', 'h5py']:\n",
    "    logging.getLogger(logger_name).setLevel(logging.ERROR)\n",
    "    logging.getLogger(logger_name).propagate = False\n",
    "\n",
    "# Redirect stdout and stderr for TensorFlow\n",
    "import io\n",
    "import contextlib\n",
    "tf_stderr = io.StringIO()\n",
    "tf_redirect = contextlib.redirect_stderr(tf_stderr)\n",
    "tf_redirect.__enter__()\n",
    "\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import the notebook setup utilities\n",
    "from word2gm_fast.utils.notebook_setup import setup_data_preprocessing_notebook, enable_autoreload\n",
    "\n",
    "# Enable autoreload for development\n",
    "enable_autoreload()\n",
    "\n",
    "# Set up environment (CPU-only for data preprocessing)\n",
    "env = setup_data_preprocessing_notebook(project_root=PROJECT_ROOT)\n",
    "\n",
    "# Extract commonly used modules for convenience\n",
    "tf = env['tensorflow']\n",
    "np = env['numpy']\n",
    "pd = env['pandas']\n",
    "batch_prepare_training_data = env['batch_prepare_training_data']\n",
    "print_resource_summary = env['print_resource_summary']\n",
    "\n",
    "# Add custom filter to completely suppress TensorFlow messages at C level\n",
    "import ctypes\n",
    "libc = ctypes.cdll.LoadLibrary('libc.so.6')\n",
    "try:\n",
    "    # This is a more extreme approach that redirects TF C++ messages to /dev/null\n",
    "    libc.fopen.restype = ctypes.c_void_p\n",
    "    null_fptr = libc.fopen(b'/dev/null', b'w')\n",
    "    libc.stderr = null_fptr\n",
    "except:\n",
    "    # If that doesn't work, we still have our other methods\n",
    "    pass\n",
    "\n",
    "print(\"TensorFlow logging has been aggressively suppressed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72a7c6",
   "metadata": {},
   "source": [
    "## Print Resource Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43483cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>SYSTEM RESOURCE SUMMARY\n",
       "============================================================\n",
       "Hostname: cm013.hpc.nyu.edu\n",
       "\n",
       "Job Allocation:\n",
       "   CPUs: 14\n",
       "   Memory: 125.0 GB\n",
       "   Requested partitions: short,cs,cm,cpu_a100_2,cpu_a100_1,cpu_gpu\n",
       "   Running on: SSH failed: Host key verification failed.\n",
       "   Job ID: 63583584\n",
       "   Node list: cm013\n",
       "\n",
       "GPU Information:\n",
       "   Error: NVML Shared Library Not Found\n",
       "\n",
       "TensorFlow GPU Detection:\n",
       "   TensorFlow detects 0 GPU(s)\n",
       "   Built with CUDA: True\n",
       "============================================================</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301858",
   "metadata": {},
   "source": [
    "## Prepare Corpora\n",
    "\n",
    "Here, we run the data-preparation pipeline from start to finish — reading preprocessed ngram corpora, generating all valid triplets, extracting the vocabulary, and saving the triplets and vocabulary as `tfrecord` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14da0c0",
   "metadata": {},
   "source": [
    "### Options for Data Preparation\n",
    "\n",
    "You can control which years are processed and how the batch preparation runs by adjusting the arguments to `batch_prepare_training_data`:\n",
    "\n",
    "**Ways to specify years:**\n",
    "- `year_range=\"2010\"` — Process a single year (e.g., only 2010).\n",
    "- `year_range=\"2010,2012,2015\"` — Process a comma-separated list of years.\n",
    "- `year_range=\"2010-2015\"` — Process a range of years, inclusive (2010 through 2015).\n",
    "- `year_range=\"2010,2012-2014,2016\"` — Combine individual years and ranges (2010, 2012, 2013, 2014, 2016).\n",
    "\n",
    "**Other options:**\n",
    "- `compress` — If `True`, output TFRecords are gzip-compressed. If `False`, output is uncompressed.\n",
    "- `show_progress` — If `True`, display a progress bar for each year.\n",
    "- `show_summary` — If `True`, print a summary of the processed data for each year.\n",
    "- `use_multiprocessing` — If `True`, process years in parallel using multiple CPU cores (recommended for large datasets).\n",
    "\n",
    "**TensorFlow Logging:**\n",
    "- TensorFlow logging is set to ERROR level to reduce verbose output\n",
    "- The pipeline still works normally, but with cleaner console output\n",
    "- Critical errors will still be displayed if they occur\n",
    "\n",
    "See the function docstring or source for more advanced options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e4e2724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing...\n",
      "(TensorFlow logging has been suppressed - only pipeline results will be shown)\n",
      "Requested 51 years, found 26 corpus files.\n",
      "Skipping 25 missing years (showing first 5): 1601, 1605, 1606, 1607, 1610, ...\n",
      "\n",
      "\n",
      "=================================================================\n",
      "PARALLEL BATCH PROCESSING\n",
      "=================================================================\n",
      "Processing 26 years\n",
      "Using 14 parallel workers\n",
      "Estimated speedup: 14.0x\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1615 failed (1/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1602 failed (2/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1620 failed (3/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1608 failed (4/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1626 failed (5/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1622 failed (6/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1611 failed (7/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1613 failed (8/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1623 failed (9/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1609 failed (10/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1632 failed (11/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1634 failed (12/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1635 failed (13/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1642 failed (14/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1604 failed (15/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1629 failed (16/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1640 failed (17/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1633 failed (18/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1643 failed (19/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1609 failed (10/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1632 failed (11/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1634 failed (12/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1635 failed (13/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1642 failed (14/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1604 failed (15/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1629 failed (16/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1640 failed (17/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1633 failed (18/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1643 failed (19/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1631 failed (20/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1644 failed (21/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1647 failed (22/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1600 failed (23/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1631 failed (20/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1644 failed (21/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1647 failed (22/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1600 failed (23/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1603 failed (24/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1603 failed (24/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1650 failed (25/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1650 failed (25/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "1627 failed (26/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "\n",
      "Parallel processing completed in 3.6s\n",
      "\n",
      "=================================================================\n",
      "Batch processing complete!\n",
      "=================================================================\n",
      "Successful: 0 years\n",
      "Failed: 26 years: 1615, 1602, 1620, 1608, 1626, 1622, 1611, 1613, 1623, 1609, 1632, 1634, 1635, 1642, 1604, 1629, 1640, 1633, 1643, 1631, 1644, 1647, 1600, 1603, 1650, 1627\n",
      "\n",
      "Batch processing complete!\n",
      "Processed 26 years successfully.\n",
      "\n",
      "VOCABULARY STATISTICS EXPLANATION:\n",
      "============================================================\n",
      "Total Vocabulary Size:   The complete number of tokens in the vocabulary file\n",
      "                         (All possible tokens that could appear in triplets)\n",
      "\n",
      "Unique Tokens in Triplets: The actual number of distinct token indices used\n",
      "                           in the generated training triplets\n",
      "\n",
      "Token Coverage %:        Percentage of vocabulary tokens that appear in triplets\n",
      "                         (unique_tokens ÷ total_vocab_size × 100)\n",
      "\n",
      "Unused Tokens:           Vocabulary tokens that never appear in any triplets\n",
      "                         (total_vocab_size - unique_tokens)\n",
      "============================================================\n",
      "\n",
      "DETAILED VOCABULARY STATISTICS BY YEAR:\n",
      "==========================================================================================\n",
      "Year       Vocab Size      Unique Tokens        Coverage %      Unused Tokens  \n",
      "------------------------------------------------------------------------------------------\n",
      "==========================================================================================\n",
      "1627 failed (26/26): cannot import name 'read_triplets_from_tfrecord' from 'word2gm_fast.io.triplets' (/scratch/edk202/word2gm-fast/src/word2gm_fast/io/triplets.py)\n",
      "\n",
      "Parallel processing completed in 3.6s\n",
      "\n",
      "=================================================================\n",
      "Batch processing complete!\n",
      "=================================================================\n",
      "Successful: 0 years\n",
      "Failed: 26 years: 1615, 1602, 1620, 1608, 1626, 1622, 1611, 1613, 1623, 1609, 1632, 1634, 1635, 1642, 1604, 1629, 1640, 1633, 1643, 1631, 1644, 1647, 1600, 1603, 1650, 1627\n",
      "\n",
      "Batch processing complete!\n",
      "Processed 26 years successfully.\n",
      "\n",
      "VOCABULARY STATISTICS EXPLANATION:\n",
      "============================================================\n",
      "Total Vocabulary Size:   The complete number of tokens in the vocabulary file\n",
      "                         (All possible tokens that could appear in triplets)\n",
      "\n",
      "Unique Tokens in Triplets: The actual number of distinct token indices used\n",
      "                           in the generated training triplets\n",
      "\n",
      "Token Coverage %:        Percentage of vocabulary tokens that appear in triplets\n",
      "                         (unique_tokens ÷ total_vocab_size × 100)\n",
      "\n",
      "Unused Tokens:           Vocabulary tokens that never appear in any triplets\n",
      "                         (total_vocab_size - unique_tokens)\n",
      "============================================================\n",
      "\n",
      "DETAILED VOCABULARY STATISTICS BY YEAR:\n",
      "==========================================================================================\n",
      "Year       Vocab Size      Unique Tokens        Coverage %      Unused Tokens  \n",
      "------------------------------------------------------------------------------------------\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "\n",
    "downsample_threshold = 1  # Typical values: 1e-3 (conservative), 1e-4 (moderate), 1e-5 (aggressive)\n",
    "\n",
    "# Further suppress all TensorFlow logging for this operation\n",
    "import os\n",
    "import io\n",
    "import contextlib\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Make absolutely sure TF is at ERROR level only\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Silence all TF messages\n",
    "tf.get_logger().setLevel('ERROR')  # Only errors from TF logger\n",
    "logging.root.setLevel(logging.ERROR)  # Only errors from root logger\n",
    "\n",
    "# Temporarily redirect stdout and stderr during processing\n",
    "original_stdout = sys.stdout\n",
    "original_stderr = sys.stderr\n",
    "process_stdout = io.StringIO()\n",
    "process_stderr = io.StringIO()\n",
    "\n",
    "print(\"Starting batch processing...\")\n",
    "print(\"(TensorFlow logging has been suppressed - only pipeline results will be shown)\")\n",
    "\n",
    "try:\n",
    "    # Redirect stderr to capture TF noise\n",
    "    sys.stderr = process_stderr\n",
    "    \n",
    "    # Process year(s)\n",
    "    results = batch_prepare_training_data(\n",
    "        corpus_dir=corpus_dir,\n",
    "        year_range=\"1600-1650\",\n",
    "        compress=False,\n",
    "        show_progress=True,\n",
    "        show_summary=True,\n",
    "        use_multiprocessing=True,\n",
    "        downsample_threshold=downsample_threshold\n",
    "    )\n",
    "    \n",
    "    # Print only the important progress messages\n",
    "    progress_output = process_stderr.getvalue()\n",
    "    filtered_lines = []\n",
    "    for line in progress_output.split('\\n'):\n",
    "        # Only keep lines with the year and triplet/vocab information\n",
    "        if any(x in line for x in ['complete', 'Successful:', 'Total triplets:', 'Average vocab']):\n",
    "            filtered_lines.append(line)\n",
    "    \n",
    "    if filtered_lines:\n",
    "        print(\"\\nPROGRESS SUMMARY (TF noise filtered out):\")\n",
    "        print(\"\\n\".join(filtered_lines))\n",
    "    \n",
    "finally:\n",
    "    # Restore stdout and stderr\n",
    "    sys.stdout = original_stdout\n",
    "    sys.stderr = original_stderr\n",
    "\n",
    "print(\"\\nBatch processing complete!\")\n",
    "print(f\"Processed {len(results)} years successfully.\")\n",
    "\n",
    "# Display a clearer summary of vocabulary stats\n",
    "print(\"\\nVOCABULARY STATISTICS EXPLANATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Total Vocabulary Size:   The complete number of tokens in the vocabulary file\")\n",
    "print(\"                         (All possible tokens that could appear in triplets)\")\n",
    "print(\"\")\n",
    "print(\"Unique Tokens in Triplets: The actual number of distinct token indices used\")\n",
    "print(\"                           in the generated training triplets\")\n",
    "print(\"\")\n",
    "print(\"Token Coverage %:        Percentage of vocabulary tokens that appear in triplets\")\n",
    "print(\"                         (unique_tokens ÷ total_vocab_size × 100)\")\n",
    "print(\"\")\n",
    "print(\"Unused Tokens:           Vocabulary tokens that never appear in any triplets\")\n",
    "print(\"                         (total_vocab_size - unique_tokens)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Print a summary table for all processed years\n",
    "if len(results) > 0:\n",
    "    print(\"\\nDETAILED VOCABULARY STATISTICS BY YEAR:\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"{'Year':<10} {'Vocab Size':<15} {'Unique Tokens':<20} {'Coverage %':<15} {'Unused Tokens':<15}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for year, data in sorted(results.items()):\n",
    "        if 'error' not in data:\n",
    "            vocab_size = data.get('vocab_size', 0)\n",
    "            unique_tokens = data.get('unique_token_count', 0)\n",
    "            coverage = data.get('unique_token_percentage', 0)\n",
    "            unused = data.get('unused_token_count', 0)\n",
    "            \n",
    "            print(f\"{year:<10} {vocab_size:<15,d} {unique_tokens:<20,d} {coverage:<15.1f} {unused:<15,d}\")\n",
    "    \n",
    "    print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c81355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all vocab.txt files from the directory tree\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# First, let's find all vocab.txt files to see what will be deleted\n",
    "print(\"Searching for vocab.txt files...\")\n",
    "result = subprocess.run([\n",
    "    'find', corpus_dir, '-name', 'vocab.txt', '-type', 'f'\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    files = result.stdout.strip().split('\\n') if result.stdout.strip() else []\n",
    "    if files and files[0]:  # Check if we actually found files\n",
    "        print(f\"Found {len(files)} vocab.txt files:\")\n",
    "        for file in files:\n",
    "            print(f\"  {file}\")\n",
    "        \n",
    "        # Ask for confirmation before deleting\n",
    "        print(f\"\\nTo delete all {len(files)} vocab.txt files, run:\")\n",
    "        print(f\"find {corpus_dir} -name 'vocab.txt' -type f -delete\")\n",
    "        print(\"\\nOr if you want to see what's being deleted:\")\n",
    "        print(f\"find {corpus_dir} -name 'vocab.txt' -type f -print -delete\")\n",
    "        \n",
    "        # Uncomment the line below to actually delete the files\n",
    "        # subprocess.run(['find', corpus_dir, '-name', 'vocab.txt', '-type', 'f', '-delete'])\n",
    "        # print(\"Files deleted!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No vocab.txt files found in the directory tree.\")\n",
    "else:\n",
    "    print(f\"Error searching for files: {result.stderr}\")\n",
    "    \n",
    "print(f\"\\nNote: This searches recursively starting from: {corpus_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
