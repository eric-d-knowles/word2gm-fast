{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ce6ba",
   "metadata": {},
   "source": [
    "# Word2GM Training Data Pipeline\n",
    "\n",
    "**Pipeline: Corpus file ‚Üí TFRecord training artifacts (triplets and vocabulary)**\n",
    "\n",
    "Use this notebook to prepare a Google 5gram corpora for Word2GM skip-gram training.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Input**: Preprocessed corpus file (e.g., `2019.txt`) in `/vast` NVMe storage\n",
    "2. **Processing**: TensorFlow-native filtering, vocabulary building, and triplet generation\n",
    "3. **Output**: TFRecord artifacts in organized subdirectories (e.g., `2019_artifacts/`)\n",
    "\n",
    "### **Artifact Storage**\n",
    "The pipeline creates year-specific subdirectories alongside the original text corpora:\n",
    "<pre>\n",
    "/vast/edk202/NLP_corpora/.../data/\n",
    "‚îú‚îÄ‚îÄ 2018.txt\n",
    "‚îú‚îÄ‚îÄ 2019.txt\n",
    "‚îú‚îÄ‚îÄ 2020.txt\n",
    "‚îú‚îÄ‚îÄ 2018_artifacts/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ triplets.tfrecord.gz\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ vocab.tfrecord.gz\n",
    "‚îú‚îÄ‚îÄ 2019_artifacts/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ triplets.tfrecord.gz\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ vocab.tfrecord.gz\n",
    "‚îî‚îÄ‚îÄ 2020_artifacts/\n",
    "    ‚îú‚îÄ‚îÄ triplets.tfrecord.gz\n",
    "    ‚îî‚îÄ‚îÄ vocab.tfrecord.gz\n",
    "</pre>\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df61c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "TensorFlow version: 2.19.0\n",
      "Setup complete; all modules loaded successfully\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup project path\n",
    "project_root = Path('/scratch/edk202/word2gm-fast')\n",
    "os.chdir(project_root)\n",
    "src_path = project_root / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import TensorFlow silently\n",
    "from word2gm_fast.utils.tf_silence import import_tensorflow_silently\n",
    "tf = import_tensorflow_silently()\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Import core dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Import Word2GM modules\n",
    "from word2gm_fast.dataprep.pipeline import batch_prepare_training_data\n",
    "\n",
    "print(\"Setup complete; all modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15f209a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER RESOURCE ALLOCATION SUMMARY\n",
      "==================================================\n",
      "Hostname: gr004.hpc.nyu.edu\n",
      "CPU cores: 2 physical, 48 logical (hyperthreading)\n",
      "Job-allocated CPUs: 14 (SLURM)\n",
      "Job-allocated memory: 125.0 GB\n",
      "GPU: 1 accessible\n",
      "  GPU 0: /dev/nvidia0\n",
      "\n",
      "STORAGE QUOTAS AND USAGE\n",
      "==================================================\n",
      "Filesystem   Allocation      Used         Percent \n",
      "--------------------------------------------------\n",
      "/home        50.0GB/30.7K    15.57GB      31.13%  \n",
      "/scratch     5.0TB/1.0M      323.29GB     6.31%   \n",
      "/archive     2.0TB/20.5K     1262.12GB    61.63%  \n",
      "/vast        2TB/5.0M        1.37TB       69.0%   \n",
      "==================================================\n",
      "\n",
      "Resource summary complete\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER RESOURCE ALLOCATION SUMMARY\n",
      "==================================================\n",
      "Hostname: gr004.hpc.nyu.edu\n",
      "CPU cores: 2 physical, 48 logical (hyperthreading)\n",
      "Job-allocated CPUs: 14 (SLURM)\n",
      "Job-allocated memory: 125.0 GB\n",
      "GPU: 1 accessible\n",
      "  GPU 0: /dev/nvidia0\n",
      "\n",
      "STORAGE QUOTAS AND USAGE\n",
      "==================================================\n",
      "Filesystem   Allocation      Used         Percent \n",
      "--------------------------------------------------\n",
      "/home        50.0GB/30.7K    15.57GB      31.13%  \n",
      "/scratch     5.0TB/1.0M      323.29GB     6.31%   \n",
      "/archive     2.0TB/20.5K     1262.12GB    61.63%  \n",
      "/vast        2TB/5.0M        1.37TB       69.0%   \n",
      "==================================================\n",
      "\n",
      "Resource summary complete\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'socket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Hostname\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m hostname = \u001b[43msocket\u001b[49m.gethostname()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHostname: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhostname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# CPU Information\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'socket' is not defined"
     ]
    }
   ],
   "source": [
    "# === CLUSTER RESOURCE MONITORING ===\n",
    "import socket\n",
    "import subprocess\n",
    "import glob\n",
    "import psutil\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(\"CLUSTER RESOURCE ALLOCATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Hostname\n",
    "hostname = socket.gethostname()\n",
    "print(f\"Hostname: {hostname}\")\n",
    "\n",
    "# CPU Information\n",
    "logical_cores = psutil.cpu_count(logical=True)\n",
    "try:\n",
    "    # Detect physical cores via thread siblings\n",
    "    physical_cores = 0\n",
    "    seen_cores = set()\n",
    "    \n",
    "    cpu_paths = glob.glob('/sys/devices/system/cpu/cpu[0-9]*')\n",
    "    cpu_numbers = sorted([int(path.split('cpu')[-1]) for path in cpu_paths])\n",
    "    \n",
    "    for cpu_num in cpu_numbers:\n",
    "        if cpu_num in seen_cores:\n",
    "            continue\n",
    "        siblings_path = f'/sys/devices/system/cpu/cpu{cpu_num}/topology/thread_siblings_list'\n",
    "        try:\n",
    "            with open(siblings_path, 'r') as f:\n",
    "                siblings = f.read().strip()\n",
    "            if ',' in siblings:\n",
    "                sibling_list = [int(x) for x in siblings.split(',')]\n",
    "            elif '-' in siblings:\n",
    "                start, end = siblings.split('-')\n",
    "                sibling_list = list(range(int(start), int(end) + 1))\n",
    "            else:\n",
    "                sibling_list = [int(siblings)]\n",
    "            seen_cores.update(sibling_list)\n",
    "            physical_cores += 1\n",
    "        except:\n",
    "            physical_cores += 1\n",
    "            seen_cores.add(cpu_num)\n",
    "    \n",
    "    if physical_cores > 0 and physical_cores != logical_cores:\n",
    "        print(f\"CPU cores: {physical_cores} physical, {logical_cores} logical (hyperthreading)\")\n",
    "    else:\n",
    "        print(f\"CPU cores: {logical_cores} logical\")\n",
    "except:\n",
    "    print(f\"CPU cores: {logical_cores} logical\")\n",
    "\n",
    "# Memory Information\n",
    "memory = psutil.virtual_memory()\n",
    "total_memory_gb = memory.total / (1024**3)\n",
    "available_memory_gb = memory.available / (1024**3)\n",
    "\n",
    "# Check for SLURM memory limits\n",
    "slurm_memory = None\n",
    "try:\n",
    "    slurm_mem_per_node = os.environ.get('SLURM_MEM_PER_NODE')\n",
    "    if slurm_mem_per_node:\n",
    "        slurm_memory = int(slurm_mem_per_node) / 1024  # MB to GB\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if slurm_memory and slurm_memory < total_memory_gb:\n",
    "    print(f\"Job-allocated memory: {slurm_memory:.1f} GB\")\n",
    "else:\n",
    "    print(f\"Memory: {total_memory_gb:.1f} GB total\")\n",
    "\n",
    "# Verify GPU\n",
    "accessible_gpus = []\n",
    "for gpu_id in range(4):\n",
    "    device_path = f\"/dev/nvidia{gpu_id}\"\n",
    "    if os.path.exists(device_path):\n",
    "        try:\n",
    "            with open(device_path, 'rb') as f:\n",
    "                pass\n",
    "            accessible_gpus.append(gpu_id)\n",
    "        except (PermissionError, OSError):\n",
    "            continue\n",
    "\n",
    "if accessible_gpus:\n",
    "    print(f\"GPU: {len(accessible_gpus)} accessible\")\n",
    "    for gpu_id in accessible_gpus:\n",
    "        print(f\"  GPU {gpu_id}: /dev/nvidia{gpu_id}\")\n",
    "else:\n",
    "    print(\"GPU: Not accessible\")\n",
    "\n",
    "print(\"\\nSTORAGE QUOTAS AND USAGE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get quota information\n",
    "try:\n",
    "    cmd = ['ssh', '-o', 'ConnectTimeout=3', '-o', 'BatchMode=yes', \n",
    "           '-o', 'StrictHostKeyChecking=no', 'log-1.hpc.nyu.edu', 'myquota']\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=8)\n",
    "    \n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        \n",
    "        # Clean ANSI color codes from output\n",
    "        ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "        \n",
    "        # Parse quota output\n",
    "        quota_data = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = ansi_escape.sub('', line).strip()  # Remove color codes\n",
    "            if line.startswith('/'):\n",
    "                # Split by whitespace, but handle the complex format\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    filesystem = parts[0]\n",
    "                    allocation = parts[3]\n",
    "                    usage_info = parts[4]\n",
    "                    \n",
    "                    # Extract usage and percentage from usage_info\n",
    "                    usage_parts = usage_info.split('/')\n",
    "                    if len(usage_parts) >= 1:\n",
    "                        space_part = usage_parts[0]\n",
    "                        # Extract usage and percentage using regex\n",
    "                        match = re.match(r'([0-9.]+[KMGT]?B)\\(([0-9.]+)%\\)', space_part)\n",
    "                        if match:\n",
    "                            used = match.group(1)\n",
    "                            percent = match.group(2) + '%'\n",
    "                        else:\n",
    "                            # Try simpler pattern\n",
    "                            if '(' in space_part and ')' in space_part:\n",
    "                                used = space_part.split('(')[0]\n",
    "                                percent_match = re.search(r'\\(([0-9.]+)%\\)', space_part)\n",
    "                                percent = percent_match.group(1) + '%' if percent_match else \"N/A\"\n",
    "                            else:\n",
    "                                used = space_part\n",
    "                                percent = \"N/A\"\n",
    "                    else:\n",
    "                        used = \"N/A\"\n",
    "                        percent = \"N/A\"\n",
    "                    \n",
    "                    quota_data.append({\n",
    "                        'filesystem': filesystem,\n",
    "                        'allocation': allocation,\n",
    "                        'used': used,\n",
    "                        'percent': percent\n",
    "                    })\n",
    "        \n",
    "        if quota_data:\n",
    "            # Print formatted table\n",
    "            print(f\"{'Filesystem':<12} {'Allocation':<15} {'Used':<12} {'Percent':<8}\")\n",
    "            print(\"-\" * 50)\n",
    "            for item in quota_data:\n",
    "                print(f\"{item['filesystem']:<12} {item['allocation']:<15} {item['used']:<12} {item['percent']:<8}\")\n",
    "        else:\n",
    "            print(\"No quota information found\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Quota check failed: {str(e)}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nResource summary complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301858",
   "metadata": {},
   "source": [
    "## Prepare one or more corpora in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e2724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "PARALLEL BATCH PROCESSING\n",
      "=================================================================\n",
      "Processing 2 years\n",
      "Using 2 parallel workers\n",
      "Estimated speedup: 2.0x\n",
      "=================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m corpus_dir = \u001b[33m\"\u001b[39m\u001b[33m/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Process years with optional parallel processing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mbatch_prepare_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcorpus_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43myear_range\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1951-1952\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_summary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/word2gm-fast/src/word2gm_fast/dataprep/pipeline.py:506\u001b[39m, in \u001b[36mbatch_prepare_training_data\u001b[39m\u001b[34m(corpus_dir, years, year_range, compress, show_progress, show_summary, max_workers, use_multiprocessing)\u001b[39m\n\u001b[32m    500\u001b[39m future_to_year = {\n\u001b[32m    501\u001b[39m     executor.submit(_process_single_year, args): args[\u001b[32m0\u001b[39m] \n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m worker_args\n\u001b[32m    503\u001b[39m }\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# Collect results as they complete\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_to_year\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43myear\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_to_year\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompleted_count\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/concurrent/futures/_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n\u001b[32m    246\u001b[39m     finished = waiter.finished_futures\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "\n",
    "# Process years with optional parallel processing\n",
    "results = batch_prepare_training_data(\n",
    "    corpus_dir=corpus_dir,\n",
    "    year_range=\"1951-1952\",\n",
    "    compress=False,\n",
    "    show_progress=True,\n",
    "    show_summary=True,\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "492b8492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: /usr/bin/nvidia-smi: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! /usr/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6114dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/usr/bin/nvidia-smi': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls -l /usr/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27b412a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU Diagnostics\n",
      "========================================\n",
      "CUDA_VISIBLE_DEVICES: 0\n",
      "\n",
      "TensorFlow GPU devices:\n",
      "  No GPU devices detected by TensorFlow\n",
      "\n",
      "GPU computation test with device placement verification:\n",
      "\n",
      "  Testing device: /GPU:0\n",
      "    Requested: /GPU:0\n",
      "    Actually used: /job:localhost/replica:0/task:0/device:CPU:0\n",
      "    Result: [1. 3. 3. 7.]\n",
      "    ‚ö†Ô∏è  Device fallback occurred! Requested /GPU:0 but used /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "  Testing device: /GPU:1\n",
      "    Requested: /GPU:1\n",
      "    Actually used: /job:localhost/replica:0/task:0/device:CPU:0\n",
      "    Result: [1. 3. 3. 7.]\n",
      "    ‚ö†Ô∏è  Device fallback occurred! Requested /GPU:1 but used /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "  Testing device: /GPU:5\n",
      "    Requested: /GPU:5\n",
      "    Actually used: /job:localhost/replica:0/task:0/device:CPU:0\n",
      "    Result: [1. 3. 3. 7.]\n",
      "    ‚ö†Ô∏è  Device fallback occurred! Requested /GPU:5 but used /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "  Testing device: /CPU:0\n",
      "    Requested: /CPU:0\n",
      "    Actually used: /job:localhost/replica:0/task:0/device:CPU:0\n",
      "    Result: [1. 3. 3. 7.]\n",
      "    ‚ö†Ô∏è  Device fallback occurred! Requested /CPU:0 but used /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "üìã TensorFlow Soft Device Placement:\n",
      "  - TensorFlow uses 'soft placement' by default\n",
      "  - If requested device doesn't exist, it falls back to available device\n",
      "  - This is why /GPU:5 'succeeds' but actually runs on CPU or /GPU:0\n",
      "  - Use tf.debugging.set_log_device_placement(True) to see actual placement\n",
      "\n",
      "Checking Word2GM pipeline GPU usage...\n",
      "  ‚ö†Ô∏è  Pipeline may not be using GPU - check implementation\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# GPU Diagnostics - Check TensorFlow GPU configuration\n",
    "print(\"TensorFlow GPU Diagnostics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check environment variables\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
    "\n",
    "# Check TensorFlow GPU detection\n",
    "print(f\"\\nTensorFlow GPU devices:\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "        # Check memory info\n",
    "        try:\n",
    "            details = tf.config.experimental.get_device_details(gpu)\n",
    "            print(f\"    Details: {details}\")\n",
    "        except:\n",
    "            print(f\"    Details: Not available\")\n",
    "else:\n",
    "    print(\"  No GPU devices detected by TensorFlow\")\n",
    "\n",
    "# Enhanced GPU computation test - shows actual device placement\n",
    "print(f\"\\nGPU computation test with device placement verification:\")\n",
    "\n",
    "def test_device_placement(device_name):\n",
    "    \"\"\"Test computation on specified device and show where it actually ran.\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n  Testing device: {device_name}\")\n",
    "        \n",
    "        # Enable logging to see actual device placement\n",
    "        tf.debugging.set_log_device_placement(True)\n",
    "        \n",
    "        with tf.device(device_name):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            result = c.numpy()\n",
    "            \n",
    "            # Check where the operation actually ran\n",
    "            actual_device = c.device\n",
    "            \n",
    "        tf.debugging.set_log_device_placement(False)\n",
    "        \n",
    "        print(f\"    Requested: {device_name}\")\n",
    "        print(f\"    Actually used: {actual_device}\")\n",
    "        print(f\"    Result: {result.flatten()}\")\n",
    "        \n",
    "        # Check if requested device matches actual device\n",
    "        if device_name.lower() in actual_device.lower():\n",
    "            print(f\"    ‚úÖ Computation ran on requested device\")\n",
    "        else:\n",
    "            print(f\"    ‚ö†Ô∏è  Device fallback occurred! Requested {device_name} but used {actual_device}\")\n",
    "            \n",
    "        return True, actual_device\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå Computation failed: {e}\")\n",
    "        return False, str(e)\n",
    "\n",
    "# Test various device configurations\n",
    "test_devices = ['/GPU:0', '/GPU:1', '/GPU:5', '/CPU:0']\n",
    "\n",
    "for device in test_devices:\n",
    "    test_device_placement(device)\n",
    "\n",
    "# Show TensorFlow's soft placement behavior\n",
    "print(f\"\\nüìã TensorFlow Soft Device Placement:\")\n",
    "print(f\"  - TensorFlow uses 'soft placement' by default\")\n",
    "print(f\"  - If requested device doesn't exist, it falls back to available device\")\n",
    "print(f\"  - This is why /GPU:5 'succeeds' but actually runs on CPU or /GPU:0\")\n",
    "print(f\"  - Use tf.debugging.set_log_device_placement(True) to see actual placement\")\n",
    "\n",
    "# Check if pipeline functions are using GPU\n",
    "print(f\"\\nChecking Word2GM pipeline GPU usage...\")\n",
    "try:\n",
    "    # Check if the pipeline functions have device placement\n",
    "    import inspect\n",
    "    from word2gm_fast.dataprep import pipeline\n",
    "    \n",
    "    # Look for GPU-related code in the pipeline\n",
    "    source = inspect.getsource(pipeline.batch_prepare_training_data)\n",
    "    if 'GPU' in source or 'device' in source:\n",
    "        print(\"  ‚úÖ Pipeline appears to have GPU support\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  Pipeline may not be using GPU - check implementation\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Could not inspect pipeline: {e}\")\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4733c1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TensorFlow Soft Device Placement Demonstration\n",
      "=======================================================\n",
      "Testing various GPU IDs:\n",
      "------------------------------\n",
      "  ‚ùå Failed: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\n",
      "  ‚ùå Failed: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:1'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\n",
      "  ‚ùå Failed: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:2'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\n",
      "  ‚ùå Failed: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:5'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\n",
      "  ‚ùå Failed: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:99'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\n",
      "\n",
      "üí° Key Points:\n",
      "‚Ä¢ TensorFlow's default 'soft placement' allows fallback to available devices\n",
      "‚Ä¢ Operations 'succeed' even if the requested GPU doesn't exist\n",
      "‚Ä¢ Always check the actual device used (tensor.device) to verify placement\n",
      "‚Ä¢ Use tf.debugging.set_log_device_placement(True) for detailed logging\n",
      "\n",
      "üõ†Ô∏è  To force strict device placement (fail if device unavailable):\n",
      "   tf.config.set_soft_device_placement(False)\n",
      "   # This will make /GPU:5 actually fail if GPU 5 doesn't exist\n"
     ]
    }
   ],
   "source": [
    "# Demonstration: Why GPU tests \"succeed\" even with invalid GPU IDs\n",
    "print(\"üîç TensorFlow Soft Device Placement Demonstration\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "def quick_gpu_test(gpu_id):\n",
    "    \"\"\"Quick test to show where computation actually runs.\"\"\"\n",
    "    device_name = f'/GPU:{gpu_id}'\n",
    "    \n",
    "    try:\n",
    "        with tf.device(device_name):\n",
    "            # Simple computation\n",
    "            x = tf.constant([1.0, 2.0])\n",
    "            y = tf.square(x)\n",
    "            result = y.numpy()\n",
    "            \n",
    "            # Check actual device\n",
    "            actual_device = y.device\n",
    "            \n",
    "        print(f\"GPU {gpu_id}: Requested={device_name}, Actually used={actual_device}\")\n",
    "        \n",
    "        # Show if fallback occurred\n",
    "        if f\"GPU:{gpu_id}\" in actual_device:\n",
    "            return \"‚úÖ Used requested GPU\"\n",
    "        else:\n",
    "            return \"‚ö†Ô∏è  FALLBACK occurred\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Failed: {e}\"\n",
    "\n",
    "print(\"Testing various GPU IDs:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for gpu_id in [0, 1, 2, 5, 99]:\n",
    "    status = quick_gpu_test(gpu_id)\n",
    "    print(f\"  {status}\")\n",
    "\n",
    "print(\"\\nüí° Key Points:\")\n",
    "print(\"‚Ä¢ TensorFlow's default 'soft placement' allows fallback to available devices\")\n",
    "print(\"‚Ä¢ Operations 'succeed' even if the requested GPU doesn't exist\")\n",
    "print(\"‚Ä¢ Always check the actual device used (tensor.device) to verify placement\")\n",
    "print(\"‚Ä¢ Use tf.debugging.set_log_device_placement(True) for detailed logging\")\n",
    "\n",
    "print(\"\\nüõ†Ô∏è  To force strict device placement (fail if device unavailable):\")\n",
    "print(\"   tf.config.set_soft_device_placement(False)\")\n",
    "print(\"   # This will make /GPU:5 actually fail if GPU 5 doesn't exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
