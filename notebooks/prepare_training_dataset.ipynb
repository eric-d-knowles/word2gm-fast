{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ce6ba",
   "metadata": {},
   "source": [
    "# Word2GM Training Data Pipeline\n",
    "\n",
    "**Pipeline: Corpus file → TFRecord training artifacts (triplets and vocabulary)**\n",
    "\n",
    "Use this notebook to prepare a Google 5gram corpora for Word2GM skip-gram training.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Input**: Preprocessed corpus file (e.g., `2019.txt`) in `/vast` NVMe storage\n",
    "2. **Processing**: TensorFlow-native filtering, vocabulary building, and triplet generation\n",
    "3. **Output**: TFRecord artifacts in organized subdirectories (e.g., `2019_artifacts/`)\n",
    "\n",
    "### **Artifact Storage**\n",
    "The pipeline creates year-specific subdirectories alongside the original text corpora:\n",
    "<pre>\n",
    "/vast/edk202/NLP_corpora/.../data/\n",
    "├── 2018.txt\n",
    "├── 2019.txt\n",
    "├── 2020.txt\n",
    "├── 2018_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "├── 2019_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "└── 2020_artifacts/\n",
    "    ├── triplets.tfrecord.gz\n",
    "    └── vocab.tfrecord.gz\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49f604",
   "metadata": {},
   "source": [
    "## Set Up for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89181d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoreload enabled\n",
      "Data preprocessing environment ready!\n",
      "Project root: /scratch/edk202/word2gm-fast\n",
      "TensorFlow 2.19.0 (CPU-only mode)\n",
      "Data preprocessing environment ready!\n",
      "Project root: /scratch/edk202/word2gm-fast\n",
      "TensorFlow 2.19.0 (CPU-only mode)\n"
     ]
    }
   ],
   "source": [
    "# Set project root directory and add `src` to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import the notebook setup utilities\n",
    "from word2gm_fast.utils.notebook_setup import setup_data_preprocessing_notebook, enable_autoreload\n",
    "\n",
    "# Enable autoreload for development\n",
    "enable_autoreload()\n",
    "\n",
    "# Set up environment (CPU-only for data preprocessing)\n",
    "env = setup_data_preprocessing_notebook(project_root=PROJECT_ROOT)\n",
    "\n",
    "# Extract commonly used modules for convenience\n",
    "tf = env['tensorflow']\n",
    "np = env['numpy']\n",
    "pd = env['pandas']\n",
    "batch_prepare_training_data = env['batch_prepare_training_data']\n",
    "print_resource_summary = env['print_resource_summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72a7c6",
   "metadata": {},
   "source": [
    "## Print Resource Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43483cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM RESOURCE SUMMARY\n",
      "==================================================\n",
      "Hostname: gr009.hpc.nyu.edu\n",
      "\n",
      "Job Allocation:\n",
      "   CPUs: 14\n",
      "   Memory: 125.0 GB\n",
      "   Partition: rtx8000\n",
      "   Job ID: 62853078\n",
      "   Node list: gr009\n",
      "\n",
      "GPU Information:\n",
      "   CUDA GPUs detected: 1\n",
      "   GPU 0: Quadro RTX 8000\n",
      "      Memory: 0.5/45.0 GB (44.5 GB free)\n",
      "      Temperature: 29°C\n",
      "      Utilization: GPU 0%, Memory 0%\n",
      "\n",
      "TensorFlow GPU Detection:\n",
      "   TensorFlow detects 0 GPU(s)\n",
      "   Built with CUDA: True\n",
      "==================================================\n",
      "\n",
      "Job Allocation:\n",
      "   CPUs: 14\n",
      "   Memory: 125.0 GB\n",
      "   Partition: rtx8000\n",
      "   Job ID: 62853078\n",
      "   Node list: gr009\n",
      "\n",
      "GPU Information:\n",
      "   CUDA GPUs detected: 1\n",
      "   GPU 0: Quadro RTX 8000\n",
      "      Memory: 0.5/45.0 GB (44.5 GB free)\n",
      "      Temperature: 29°C\n",
      "      Utilization: GPU 0%, Memory 0%\n",
      "\n",
      "TensorFlow GPU Detection:\n",
      "   TensorFlow detects 0 GPU(s)\n",
      "   Built with CUDA: True\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import subprocess\n",
    "\n",
    "# Show requested partitions (what you submitted to)\n",
    "requested_partitions = os.environ.get('SLURM_JOB_PARTITION', 'unknown')\n",
    "print(f\"Requested partitions: {requested_partitions}\")\n",
    "\n",
    "# Get node name\n",
    "nodename = os.environ.get('SLURMD_NODENAME', socket.gethostname())\n",
    "print(f\"Running on node: {nodename}\")\n",
    "\n",
    "# Get the actual partition by SSH'ing to login node\n",
    "job_id = os.environ.get('SLURM_JOB_ID')\n",
    "if job_id:\n",
    "    try:\n",
    "        # SSH to greene-login and run squeue to get actual partition\n",
    "        ssh_cmd = ['ssh', 'greene-login', 'squeue', '-j', job_id, '-h', '-o', '%P']\n",
    "        result = subprocess.run(ssh_cmd, capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            actual_partition = result.stdout.strip()\n",
    "            print(f\"Actually running on partition: {actual_partition}\")\n",
    "        else:\n",
    "            print(f\"SSH squeue failed: {result.stderr.strip()}\")\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"SSH command timed out\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running SSH squeue: {e}\")\n",
    "else:\n",
    "    print(\"No SLURM_JOB_ID found - not in a SLURM job\")\n",
    "\n",
    "# Show other useful SLURM info\n",
    "print(f\"\\nOther SLURM info:\")\n",
    "print(f\"Job ID: {job_id or 'N/A'}\")\n",
    "print(f\"Node list: {os.environ.get('SLURM_JOB_NODELIST', 'N/A')}\")\n",
    "print(f\"Allocated CPUs: {os.environ.get('SLURM_CPUS_PER_TASK', 'N/A')}\")\n",
    "print(f\"Memory per node: {os.environ.get('SLURM_MEM_PER_NODE', 'N/A')} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301858",
   "metadata": {},
   "source": [
    "## Prepare one or more corpora in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "\n",
    "# Process years with multiprocessing (CPU-only mode configured in cell 2)\n",
    "results = batch_prepare_training_data(\n",
    "    corpus_dir=corpus_dir,\n",
    "    year_range=\"1701-1800\",\n",
    "    compress=False,\n",
    "    show_progress=True,\n",
    "    show_summary=True,\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11cf207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
