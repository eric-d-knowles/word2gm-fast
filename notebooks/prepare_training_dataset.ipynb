{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ce6ba",
   "metadata": {},
   "source": [
    "# Word2GM Training Data Pipeline\n",
    "\n",
    "**Pipeline: Corpus file → TFRecord training artifacts (triplets and vocabulary)**\n",
    "\n",
    "Use this notebook to prepare a Google 5gram corpora for Word2GM skip-gram training.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Input**: Preprocessed corpus file (e.g., `2019.txt`) in `/vast` NVMe storage\n",
    "2. **Processing**: TensorFlow-native filtering, vocabulary building, and triplet generation\n",
    "3. **Output**: TFRecord artifacts in organized subdirectories (e.g., `2019_artifacts/`)\n",
    "\n",
    "### **Artifact Storage**\n",
    "The pipeline creates year-specific subdirectories alongside the original text corpora:\n",
    "<pre>\n",
    "/vast/edk202/NLP_corpora/.../data/\n",
    "├── 2018.txt\n",
    "├── 2019.txt\n",
    "├── 2020.txt\n",
    "├── 2018_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "├── 2019_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "└── 2020_artifacts/\n",
    "    ├── triplets.tfrecord.gz\n",
    "    └── vocab.tfrecord.gz\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49f604",
   "metadata": {},
   "source": [
    "## Set Up for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89181d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>Autoreload enabled</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Project root: /scratch/edk202/word2gm-fast\n",
       "TensorFlow version: 2.19.0\n",
       "Device mode: CPU-only</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Data preprocessing environment ready!</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set project root directory and add `src` to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import the notebook setup utilities\n",
    "from word2gm_fast.utils.notebook_setup import setup_data_preprocessing_notebook, enable_autoreload\n",
    "\n",
    "# Enable autoreload for development\n",
    "enable_autoreload()\n",
    "\n",
    "# Set up environment (CPU-only for data preprocessing)\n",
    "env = setup_data_preprocessing_notebook(project_root=PROJECT_ROOT)\n",
    "\n",
    "# Extract commonly used modules for convenience\n",
    "tf = env['tensorflow']\n",
    "np = env['numpy']\n",
    "pd = env['pandas']\n",
    "batch_prepare_training_data = env['batch_prepare_training_data']\n",
    "print_resource_summary = env['print_resource_summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72a7c6",
   "metadata": {},
   "source": [
    "## Print Resource Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43483cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>SYSTEM RESOURCE SUMMARY\n",
       "============================================================\n",
       "Hostname: gv009.hpc.nyu.edu\n",
       "\n",
       "Job Allocation:\n",
       "   CPUs: 14\n",
       "   Memory: 125.0 GB\n",
       "   Requested partitions: v100,rtx8000,a100_2,a100_1,h100_1\n",
       "   Running on: SSH failed: Host key verification failed.\n",
       "   Job ID: 63450166\n",
       "   Node list: gv009\n",
       "\n",
       "GPU Information:\n",
       "   CUDA GPUs detected: 1\n",
       "   GPU 0: Tesla V100-PCIE-32GB\n",
       "      Memory: 0.6/32.0 GB (31.4 GB free)\n",
       "      Temperature: 31°C\n",
       "      Utilization: GPU 0%, Memory 0%\n",
       "\n",
       "TensorFlow GPU Detection:\n",
       "   TensorFlow detects 0 GPU(s)\n",
       "   Built with CUDA: True\n",
       "============================================================</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301858",
   "metadata": {},
   "source": [
    "## Prepare Corpora\n",
    "\n",
    "Here, we run the data-preparation pipeline from start to finish — reading preprocessed ngram corpora, generating all valid triplets, extracting the vocabulary, and saving the triplets and vocabulary as `tfrecord` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14da0c0",
   "metadata": {},
   "source": [
    "### Options for Data Preparation\n",
    "\n",
    "You can control which years are processed and how the batch preparation runs by adjusting the arguments to `batch_prepare_training_data`:\n",
    "\n",
    "**Ways to specify years:**\n",
    "- `year_range=\"2010\"` — Process a single year (e.g., only 2010).\n",
    "- `year_range=\"2010,2012,2015\"` — Process a comma-separated list of years.\n",
    "- `year_range=\"2010-2015\"` — Process a range of years, inclusive (2010 through 2015).\n",
    "- `year_range=\"2010,2012-2014,2016\"` — Combine individual years and ranges (2010, 2012, 2013, 2014, 2016).\n",
    "\n",
    "**Other options:**\n",
    "- `compress` — If `True`, output TFRecords are gzip-compressed. If `False`, output is uncompressed.\n",
    "- `show_progress` — If `True`, display a progress bar for each year.\n",
    "- `show_summary` — If `True`, print a summary of the processed data for each year.\n",
    "- `use_multiprocessing` — If `True`, process years in parallel using multiple CPU cores (recommended for large datasets).\n",
    "\n",
    "See the function docstring or source for more advanced options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e2724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested 301 years, found 71 corpus files.\n",
      "Skipping 230 missing years (showing first 5): 1400, 1401, 1402, 1403, 1404, ...\n",
      "\n",
      "\n",
      "=================================================================\n",
      "PARALLEL BATCH PROCESSING\n",
      "=================================================================\n",
      "Processing 71 years\n",
      "Using 14 parallel workers\n",
      "Estimated speedup: 14.0x\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1602 complete (1/71): 0 triplets, 1 vocab, 1.6s\n",
      "1608 complete (2/71): 0 triplets, 4 vocab, 1.6s\n",
      "1579 complete (3/71): 0 triplets, 16 vocab, 1.6s\n",
      "1590 complete (4/71): 0 triplets, 5 vocab, 1.6s\n",
      "1578 complete (5/71): 0 triplets, 12 vocab, 1.6s\n",
      "1583 complete (6/71): 0 triplets, 27 vocab, 1.6s\n",
      "1598 complete (7/71): 12 triplets, 195 vocab, 1.6s\n",
      "1595 complete (8/71): 14 triplets, 223 vocab, 1.7s\n",
      "1604 complete (9/71): 34 triplets, 409 vocab, 1.7s\n",
      "1597 complete (10/71): 27 triplets, 338 vocab, 1.7s\n",
      "1594 complete (11/71): 51 triplets, 527 vocab, 1.8s\n",
      "1600 complete (12/71): 60 triplets, 535 vocab, 1.8s\n",
      "1603 complete (13/71): 46 triplets, 522 vocab, 1.9s\n",
      "1611 complete (14/71): 1 triplets, 14 vocab, 0.3s\n",
      "1622 complete (15/71): 0 triplets, 6 vocab, 0.3s\n",
      "1620 complete (16/71): 1 triplets, 12 vocab, 0.3s\n",
      "1613 complete (17/71): 1 triplets, 27 vocab, 0.3s\n",
      "1615 complete (18/71): 0 triplets, 1 vocab, 0.3s\n",
      "1623 complete (19/71): 0 triplets, 59 vocab, 0.3s\n",
      "1626 complete (20/71): 0 triplets, 3 vocab, 0.3s\n",
      "1609 complete (21/71): 15 triplets, 223 vocab, 0.5s\n",
      "1632 complete (22/71): 0 triplets, 8 vocab, 0.3s\n",
      "1629 complete (23/71): 3 triplets, 113 vocab, 0.4s\n",
      "1635 complete (24/71): 0 triplets, 9 vocab, 0.3s\n",
      "1634 complete (25/71): 0 triplets, 15 vocab, 0.3s\n",
      "1633 complete (26/71): 6 triplets, 128 vocab, 0.4s\n",
      "1640 complete (27/71): 1 triplets, 29 vocab, 0.3s\n",
      "1642 complete (28/71): 0 triplets, 5 vocab, 0.3s\n",
      "1643 complete (29/71): 0 triplets, 20 vocab, 0.3s\n",
      "1631 complete (30/71): 15 triplets, 228 vocab, 0.5s\n",
      "1644 complete (31/71): 0 triplets, 1 vocab, 0.3s\n",
      "1647 complete (32/71): 4 triplets, 163 vocab, 0.5s\n",
      "1655 complete (33/71): 0 triplets, 13 vocab, 0.4s\n",
      "1662 complete (34/71): 0 triplets, 47 vocab, 0.3s\n",
      "1669 complete (35/71): 0 triplets, 23 vocab, 0.4s\n",
      "1652 complete (36/71): 5 triplets, 122 vocab, 0.5s\n",
      "1668 complete (37/71): 2 triplets, 72 vocab, 0.4s\n",
      "1587 complete (38/71): 133 triplets, 850 vocab, 2.7s\n",
      "1661 complete (39/71): 1 triplets, 87 vocab, 0.4s\n",
      "1671 complete (40/71): 23 triplets, 321 vocab, 0.6s\n",
      "1674 complete (41/71): 0 triplets, 8 vocab, 0.4s\n",
      "1660 complete (42/71): 24 triplets, 282 vocab, 0.8s\n",
      "1675 complete (43/71): 11 triplets, 211 vocab, 0.6s\n",
      "1677 complete (44/71): 0 triplets, 20 vocab, 0.6s\n",
      "1682 complete (45/71): 34 triplets, 350 vocab, 0.9s\n",
      "1686 complete (46/71): 1 triplets, 67 vocab, 0.5s\n",
      "1680 complete (47/71): 35 triplets, 361 vocab, 1.1s\n",
      "1687 complete (48/71): 22 triplets, 320 vocab, 0.9s\n",
      "1683 complete (49/71): 30 triplets, 317 vocab, 1.2s\n",
      "1690 complete (50/71): 0 triplets, 30 vocab, 0.4s\n",
      "1673 complete (51/71): 85 triplets, 680 vocab, 2.1s\n",
      "1689 complete (52/71): 16 triplets, 234 vocab, 0.7s\n",
      "1691 complete (53/71): 1 triplets, 58 vocab, 0.6s\n",
      "1693 complete (54/71): 1 triplets, 63 vocab, 0.5s\n",
      "1695 complete (55/71): 0 triplets, 10 vocab, 0.4s\n",
      "1694 complete (56/71): 1 triplets, 80 vocab, 0.5s\n",
      "1692 complete (57/71): 9 triplets, 126 vocab, 0.9s\n",
      "1697 complete (58/71): 0 triplets, 7 vocab, 0.4s\n",
      "1696 complete (59/71): 8 triplets, 151 vocab, 0.5s\n",
      "1679 complete (60/71): 234 triplets, 934 vocab, 2.6s\n",
      "1698 complete (61/71): 0 triplets, 3 vocab, 0.5s\n",
      "1627 complete (62/71): 723 triplets, 1,672 vocab, 3.8s\n",
      "1699 complete (63/71): 11 triplets, 206 vocab, 0.8s\n",
      "1700 complete (64/71): 19 triplets, 325 vocab, 0.7s\n",
      "1650 complete (65/71): 216 triplets, 1,104 vocab, 3.6s\n",
      "1688 complete (66/71): 258 triplets, 1,135 vocab, 2.7s\n",
      "1678 complete (67/71): 942 triplets, 1,731 vocab, 4.0s\n",
      "1681 complete (68/71): 1,550 triplets, 2,314 vocab, 5.0s\n",
      "1684 complete (69/71): 1,953 triplets, 2,673 vocab, 7.3s\n",
      "1667 complete (70/71): 5,241 triplets, 3,999 vocab, 10.6s\n",
      "1685 complete (71/71): 7,154 triplets, 4,963 vocab, 12.7s\n",
      "\n",
      "Parallel processing completed in 16.1s\n",
      "\n",
      "=================================================================\n",
      "Batch processing complete!\n",
      "=================================================================\n",
      "Successful: 71 years\n",
      "   Total triplets: 19,034\n",
      "   Average vocab size: 420\n",
      "   Average time per year: 1.4s\n",
      "   Overall triplets/second: 1,186\n",
      "   Parallel speedup: 6.4x\n",
      "   Parallel efficiency: 45.6%\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "\n",
    "downsample_threshold = 1e-5  # Typical values: 1e-3 (conservative), 1e-4 (moderate), 1e-5 (aggressive)\n",
    "\n",
    "# Process year(s)\n",
    "results = batch_prepare_training_data(\n",
    "    corpus_dir=corpus_dir,\n",
    "    year_range=\"1701-1800\",\n",
    "    compress=False,\n",
    "    show_progress=True,\n",
    "    show_summary=True,\n",
    "    use_multiprocessing=True,\n",
    "    downsample_threshold=downsample_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c81355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
