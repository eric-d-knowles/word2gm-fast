{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ce6ba",
   "metadata": {},
   "source": [
    "# Word2GM Training Data Pipeline\n",
    "\n",
    "**Pipeline: Corpus file → TFRecord training artifacts (triplets and vocabulary)**\n",
    "\n",
    "Use this notebook to prepare a Google 5gram corpora for Word2GM skip-gram training.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Input**: Preprocessed corpus file (e.g., `2019.txt`) in `/vast` NVMe storage\n",
    "2. **Processing**: TensorFlow-native filtering, vocabulary building, and triplet generation\n",
    "3. **Output**: TFRecord artifacts in organized subdirectories (e.g., `2019_artifacts/`)\n",
    "\n",
    "### **Artifact Storage**\n",
    "The pipeline creates year-specific subdirectories alongside the original text corpora:\n",
    "<pre>\n",
    "/vast/edk202/NLP_corpora/.../data/\n",
    "├── 2018.txt\n",
    "├── 2019.txt\n",
    "├── 2020.txt\n",
    "├── 2018_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "├── 2019_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "└── 2020_artifacts/\n",
    "    ├── triplets.tfrecord.gz\n",
    "    └── vocab.tfrecord.gz\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ad9e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoreload enabled; modules will update automatically when files change\n",
      "TensorFlow 2.19.0 imported silently\n",
      "All pipeline modules loaded successfully\n",
      "Ready to process corpus and generate training data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Enable automatic reloading of changed modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print(\"Autoreload enabled; modules will update automatically when files change\")\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/scratch/edk202/word2gm-fast/notebooks')\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Clean TensorFlow import with complete silencing\n",
    "from src.word2gm_fast.utils import import_tensorflow_silently\n",
    "\n",
    "tf = import_tensorflow_silently(deterministic=False)\n",
    "print(f\"TensorFlow {tf.__version__} imported silently\")\n",
    "\n",
    "# Import optimized data pipeline modules\n",
    "from src.word2gm_fast.dataprep.pipeline import batch_prepare_training_data\n",
    "\n",
    "print(\"All pipeline modules loaded successfully\")\n",
    "print(\"Ready to process corpus and generate training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301858",
   "metadata": {},
   "source": [
    "## Prepare one or more corpora in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4e2724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARALLEL BATCH PROCESSING\n",
      "============================================================\n",
      "Processing 28 years\n",
      "Using 14 parallel workers\n",
      "Estimated speedup: 14.0x\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1802 complete (1/28): 92,701 triplets, 8,487 vocab, 70.0s\n",
      "1801 complete (2/28): 157,360 triplets, 11,000 vocab, 100.6s\n",
      "1805 complete (3/28): 190,798 triplets, 11,609 vocab, 113.0s\n",
      "1804 complete (4/28): 128,037 triplets, 9,769 vocab, 119.3s\n",
      "1814 complete (5/28): 301,143 triplets, 14,965 vocab, 182.2s\n",
      "1803 complete (6/28): 349,757 triplets, 14,473 vocab, 204.9s\n",
      "1807 complete (7/28): 369,858 triplets, 14,626 vocab, 216.0s\n",
      "1806 complete (8/28): 403,392 triplets, 14,124 vocab, 223.7s\n",
      "1808 complete (9/28): 317,657 triplets, 13,193 vocab, 224.9s\n",
      "1813 complete (10/28): 508,850 triplets, 17,428 vocab, 323.9s\n",
      "1816 complete (11/28): 390,078 triplets, 16,144 vocab, 242.2s\n",
      "1811 complete (12/28): 691,955 triplets, 17,916 vocab, 403.5s\n",
      "1812 complete (13/28): 710,549 triplets, 18,423 vocab, 432.8s\n",
      "1817 complete (14/28): 524,951 triplets, 17,803 vocab, 325.2s\n",
      "1809 complete (15/28): 734,668 triplets, 19,226 vocab, 443.2s\n",
      "1815 complete (16/28): 641,679 triplets, 17,905 vocab, 375.6s\n",
      "1818 complete (17/28): 916,709 triplets, 20,694 vocab, 529.0s\n",
      "1810 complete (18/28): 1,184,737 triplets, 20,429 vocab, 660.8s\n",
      "1823 complete (19/28): 919,481 triplets, 21,623 vocab, 475.9s\n",
      "1819 complete (20/28): 1,019,953 triplets, 22,842 vocab, 539.0s\n",
      "1827 complete (21/28): 777,326 triplets, 20,385 vocab, 381.8s\n",
      "1821 complete (22/28): 1,333,192 triplets, 23,237 vocab, 651.9s\n",
      "1825 complete (23/28): 1,187,396 triplets, 23,176 vocab, 527.3s\n",
      "1828 complete (24/28): 1,088,739 triplets, 22,406 vocab, 442.4s\n",
      "1826 complete (25/28): 1,006,031 triplets, 22,657 vocab, 484.6s\n",
      "1822 complete (26/28): 1,583,366 triplets, 25,169 vocab, 705.2s\n",
      "1820 complete (27/28): 1,640,775 triplets, 24,563 vocab, 751.4s\n",
      "1824 complete (28/28): 1,567,353 triplets, 24,419 vocab, 660.6s\n",
      "\n",
      "Parallel processing completed in 985.4s\n",
      "\n",
      "============================================================\n",
      "Batch processing complete!\n",
      "============================================================\n",
      "Successful: 28 years\n",
      "   Total triplets: 20,738,491\n",
      "   Average vocab size: 18,167\n",
      "   Average time per year: 386.1s\n",
      "   Overall triplets/second: 21,045\n",
      "   Parallel speedup: 11.0x\n",
      "   Parallel efficiency: 78.4%\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "\n",
    "# Process years with optional parallel processing\n",
    "results = batch_prepare_training_data(\n",
    "    corpus_dir=corpus_dir,\n",
    "    year_range=\"1829-1900\",\n",
    "    compress=False,\n",
    "    show_progress=True,\n",
    "    show_summary=True,\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e1e8f4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
