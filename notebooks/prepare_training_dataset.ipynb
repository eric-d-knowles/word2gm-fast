{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ce6ba",
   "metadata": {},
   "source": [
    "# Word2GM Training Data Pipeline\n",
    "\n",
    "**Pipeline: Corpus file → TFRecord training artifacts (triplets and vocabulary)**\n",
    "\n",
    "Use this notebook to prepare a Google 5gram corpora for Word2GM skip-gram training.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Input**: Preprocessed corpus file (e.g., `2019.txt`) in `/vast` NVMe storage\n",
    "2. **Processing**: TensorFlow-native filtering, vocabulary building, and triplet generation\n",
    "3. **Output**: TFRecord artifacts in organized subdirectories (e.g., `2019_artifacts/`)\n",
    "\n",
    "### **Artifact Storage**\n",
    "The pipeline creates year-specific subdirectories alongside the original text corpora:\n",
    "<pre>\n",
    "/vast/edk202/NLP_corpora/.../data/\n",
    "├── 2018.txt\n",
    "├── 2019.txt\n",
    "├── 2020.txt\n",
    "├── 2018_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "├── 2019_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "└── 2020_artifacts/\n",
    "    ├── triplets.tfrecord.gz\n",
    "    └── vocab.tfrecord.gz\n",
    "</pre>\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0365380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGC-DL-CONTAINER-LICENSE    environment  lib32\t opt   scratch\t    sys\n",
      "bin\t\t\t    etc\t\t lib64\t proc  share\t    tmp\n",
      "boot\t\t\t    ext3\t libx32  root  singularity  usr\n",
      "cuda-keyring_1.1-1_all.deb  home\t media\t run   srv\t    var\n",
      "dev\t\t\t    lib\t\t mnt\t sbin  state\t    vast\n"
     ]
    }
   ],
   "source": [
    "!ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df61c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "🔧 Setting up CUDA environment for GPU support...\n",
      "✅ Found CUDA path: /usr/lib64\n",
      "✅ Found CUDA path: /share/apps/cuda/11.6.2/lib64\n",
      "✅ Found CUDA path: /share/apps/cuda/11.6.2/targets/x86_64-linux/lib\n",
      "✅ Found CUDA path: /usr/local/cuda/lib64\n",
      "✅ Updated LD_LIBRARY_PATH with 4 CUDA paths\n",
      "⚠️  libcuda.so.1 still not accessible: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "TensorFlow version: 2.19.0\n",
      "Setup complete; all modules loaded successfully\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup project path\n",
    "project_root = Path('/scratch/edk202/word2gm-fast')\n",
    "os.chdir(project_root)\n",
    "src_path = project_root / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# 🔧 CRITICAL: Set CUDA library paths BEFORE importing TensorFlow\n",
    "print(\"🔧 Setting up CUDA environment for GPU support...\")\n",
    "\n",
    "# Set CUDA library paths (NYU Greene)\n",
    "cuda_lib_paths = [\n",
    "    '/usr/lib64',  # System NVIDIA driver libraries\n",
    "    '/share/apps/cuda/11.6.2/lib64',  # CUDA toolkit\n",
    "    '/share/apps/cuda/11.6.2/targets/x86_64-linux/lib',  # CUDA target libs\n",
    "    '/usr/local/cuda/lib64'  # Default CUDA location\n",
    "]\n",
    "\n",
    "# Update LD_LIBRARY_PATH\n",
    "current_ld_path = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "new_ld_paths = []\n",
    "\n",
    "for cuda_path in cuda_lib_paths:\n",
    "    if os.path.exists(cuda_path):\n",
    "        new_ld_paths.append(cuda_path)\n",
    "        print(f\"✅ Found CUDA path: {cuda_path}\")\n",
    "\n",
    "if new_ld_paths:\n",
    "    if current_ld_path:\n",
    "        os.environ['LD_LIBRARY_PATH'] = ':'.join(new_ld_paths) + ':' + current_ld_path\n",
    "    else:\n",
    "        os.environ['LD_LIBRARY_PATH'] = ':'.join(new_ld_paths)\n",
    "    print(f\"✅ Updated LD_LIBRARY_PATH with {len(new_ld_paths)} CUDA paths\")\n",
    "\n",
    "# Test if libcuda.so.1 is now accessible\n",
    "try:\n",
    "    import ctypes\n",
    "    ctypes.CDLL('libcuda.so.1')\n",
    "    print(\"✅ libcuda.so.1 successfully loaded!\")\n",
    "    cuda_available = True\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  libcuda.so.1 still not accessible: {e}\")\n",
    "    cuda_available = False\n",
    "\n",
    "# Import TensorFlow silently\n",
    "from word2gm_fast.utils.tf_silence import import_tensorflow_silently\n",
    "tf = import_tensorflow_silently()\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Import core dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Import Word2GM modules\n",
    "from word2gm_fast.dataprep.pipeline import batch_prepare_training_data\n",
    "\n",
    "print(\"Setup complete; all modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f209a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER RESOURCE ALLOCATION SUMMARY\n",
      "==================================================\n",
      "Hostname: gr004.hpc.nyu.edu\n",
      "CPU cores: 48 logical\n",
      "Job-allocated memory: 125.0 GB\n",
      "GPU: 1 accessible\n",
      "  GPU 0: /dev/nvidia0\n",
      "\n",
      "STORAGE QUOTAS AND USAGE\n",
      "==================================================\n",
      "Filesystem   Allocation      Used         Percent \n",
      "--------------------------------------------------\n",
      "/home        50.0GB/30.7K    15.56GB      31.12%  \n",
      "/scratch     5.0TB/1.0M      323.29GB     6.31%   \n",
      "/archive     2.0TB/20.5K     1262.12GB    61.63%  \n",
      "/vast        2TB/5.0M        1.37TB       69.0%   \n",
      "==================================================\n",
      "\n",
      "Resource summary complete\n",
      "Filesystem   Allocation      Used         Percent \n",
      "--------------------------------------------------\n",
      "/home        50.0GB/30.7K    15.56GB      31.12%  \n",
      "/scratch     5.0TB/1.0M      323.29GB     6.31%   \n",
      "/archive     2.0TB/20.5K     1262.12GB    61.63%  \n",
      "/vast        2TB/5.0M        1.37TB       69.0%   \n",
      "==================================================\n",
      "\n",
      "Resource summary complete\n"
     ]
    }
   ],
   "source": [
    "# === CLUSTER RESOURCE MONITORING ===\n",
    "import socket\n",
    "import subprocess\n",
    "import glob\n",
    "import psutil\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(\"CLUSTER RESOURCE ALLOCATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Hostname\n",
    "hostname = socket.gethostname()\n",
    "print(f\"Hostname: {hostname}\")\n",
    "\n",
    "# CPU Information\n",
    "logical_cores = psutil.cpu_count(logical=True)\n",
    "try:\n",
    "    # Detect physical cores via thread siblings\n",
    "    physical_cores = 0\n",
    "    seen_cores = set()\n",
    "    \n",
    "    cpu_paths = glob.glob('/sys/devices/system/cpu/cpu[0-9]*')\n",
    "    cpu_numbers = sorted([int(path.split('cpu')[-1]) for path in cpu_paths])\n",
    "    \n",
    "    for cpu_num in cpu_numbers:\n",
    "        if cpu_num in seen_cores:\n",
    "            continue\n",
    "        siblings_path = f'/sys/devices/system/cpu/cpu{cpu_num}/topology/thread_siblings_list'\n",
    "        try:\n",
    "            with open(siblings_path, 'r') as f:\n",
    "                siblings = f.read().strip()\n",
    "            if ',' in siblings:\n",
    "                sibling_list = [int(x) for x in siblings.split(',')]\n",
    "            elif '-' in siblings:\n",
    "                start, end = siblings.split('-')\n",
    "                sibling_list = list(range(int(start), int(end) + 1))\n",
    "            else:\n",
    "                sibling_list = [int(siblings)]\n",
    "            seen_cores.update(sibling_list)\n",
    "            physical_cores += 1\n",
    "        except:\n",
    "            physical_cores += 1\n",
    "            seen_cores.add(cpu_num)\n",
    "    \n",
    "    if physical_cores > 0 and physical_cores != logical_cores:\n",
    "        print(f\"CPU cores: {physical_cores} physical, {logical_cores} logical (hyperthreading)\")\n",
    "    else:\n",
    "        print(f\"CPU cores: {logical_cores} logical\")\n",
    "except:\n",
    "    print(f\"CPU cores: {logical_cores} logical\")\n",
    "\n",
    "# Memory Information\n",
    "memory = psutil.virtual_memory()\n",
    "total_memory_gb = memory.total / (1024**3)\n",
    "available_memory_gb = memory.available / (1024**3)\n",
    "\n",
    "# Check for SLURM memory limits\n",
    "slurm_memory = None\n",
    "try:\n",
    "    slurm_mem_per_node = os.environ.get('SLURM_MEM_PER_NODE')\n",
    "    if slurm_mem_per_node:\n",
    "        slurm_memory = int(slurm_mem_per_node) / 1024  # MB to GB\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if slurm_memory and slurm_memory < total_memory_gb:\n",
    "    print(f\"Job-allocated memory: {slurm_memory:.1f} GB\")\n",
    "else:\n",
    "    print(f\"Memory: {total_memory_gb:.1f} GB total\")\n",
    "\n",
    "# Verify GPU\n",
    "accessible_gpus = []\n",
    "for gpu_id in range(4):\n",
    "    device_path = f\"/dev/nvidia{gpu_id}\"\n",
    "    if os.path.exists(device_path):\n",
    "        try:\n",
    "            with open(device_path, 'rb') as f:\n",
    "                pass\n",
    "            accessible_gpus.append(gpu_id)\n",
    "        except (PermissionError, OSError):\n",
    "            continue\n",
    "\n",
    "if accessible_gpus:\n",
    "    print(f\"GPU: {len(accessible_gpus)} accessible\")\n",
    "    for gpu_id in accessible_gpus:\n",
    "        print(f\"  GPU {gpu_id}: /dev/nvidia{gpu_id}\")\n",
    "else:\n",
    "    print(\"GPU: Not accessible\")\n",
    "\n",
    "print(\"\\nSTORAGE QUOTAS AND USAGE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get quota information\n",
    "try:\n",
    "    cmd = ['ssh', '-o', 'ConnectTimeout=3', '-o', 'BatchMode=yes', \n",
    "           '-o', 'StrictHostKeyChecking=no', 'log-1.hpc.nyu.edu', 'myquota']\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=8)\n",
    "    \n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        \n",
    "        # Clean ANSI color codes from output\n",
    "        ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "        \n",
    "        # Parse quota output\n",
    "        quota_data = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = ansi_escape.sub('', line).strip()  # Remove color codes\n",
    "            if line.startswith('/'):\n",
    "                # Split by whitespace, but handle the complex format\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    filesystem = parts[0]\n",
    "                    allocation = parts[3]\n",
    "                    usage_info = parts[4]\n",
    "                    \n",
    "                    # Extract usage and percentage from usage_info\n",
    "                    usage_parts = usage_info.split('/')\n",
    "                    if len(usage_parts) >= 1:\n",
    "                        space_part = usage_parts[0]\n",
    "                        # Extract usage and percentage using regex\n",
    "                        match = re.match(r'([0-9.]+[KMGT]?B)\\(([0-9.]+)%\\)', space_part)\n",
    "                        if match:\n",
    "                            used = match.group(1)\n",
    "                            percent = match.group(2) + '%'\n",
    "                        else:\n",
    "                            # Try simpler pattern\n",
    "                            if '(' in space_part and ')' in space_part:\n",
    "                                used = space_part.split('(')[0]\n",
    "                                percent_match = re.search(r'\\(([0-9.]+)%\\)', space_part)\n",
    "                                percent = percent_match.group(1) + '%' if percent_match else \"N/A\"\n",
    "                            else:\n",
    "                                used = space_part\n",
    "                                percent = \"N/A\"\n",
    "                    else:\n",
    "                        used = \"N/A\"\n",
    "                        percent = \"N/A\"\n",
    "                    \n",
    "                    quota_data.append({\n",
    "                        'filesystem': filesystem,\n",
    "                        'allocation': allocation,\n",
    "                        'used': used,\n",
    "                        'percent': percent\n",
    "                    })\n",
    "        \n",
    "        if quota_data:\n",
    "            # Print formatted table\n",
    "            print(f\"{'Filesystem':<12} {'Allocation':<15} {'Used':<12} {'Percent':<8}\")\n",
    "            print(\"-\" * 50)\n",
    "            for item in quota_data:\n",
    "                print(f\"{item['filesystem']:<12} {item['allocation']:<15} {item['used']:<12} {item['percent']:<8}\")\n",
    "        else:\n",
    "            print(\"No quota information found\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Quota check failed: {str(e)}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nResource summary complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301858",
   "metadata": {},
   "source": [
    "## Prepare one or more corpora in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "\n",
    "# Process years with optional parallel processing\n",
    "results = batch_prepare_training_data(\n",
    "    corpus_dir=corpus_dir,\n",
    "    year_range=\"1951-1952\",\n",
    "    compress=False,\n",
    "    show_progress=True,\n",
    "    show_summary=True,\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b412a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU Diagnostics\n",
      "========================================\n",
      "CUDA_VISIBLE_DEVICES: 0\n",
      "\n",
      "TensorFlow GPU devices:\n",
      "  No GPU devices detected by TensorFlow\n",
      "\n",
      "GPU computation test:\n",
      "  ✅ GPU computation successful: [1. 3. 3. 7.]\n",
      "\n",
      "Checking Word2GM pipeline GPU usage...\n",
      "  ⚠️  Pipeline may not be using GPU - check implementation\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# GPU Diagnostics - Check TensorFlow GPU configuration\n",
    "print(\"TensorFlow GPU Diagnostics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check environment variables\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
    "\n",
    "# Check TensorFlow GPU detection\n",
    "print(f\"\\nTensorFlow GPU devices:\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "        # Check memory info\n",
    "        try:\n",
    "            details = tf.config.experimental.get_device_details(gpu)\n",
    "            print(f\"    Details: {details}\")\n",
    "        except:\n",
    "            print(f\"    Details: Not available\")\n",
    "else:\n",
    "    print(\"  No GPU devices detected by TensorFlow\")\n",
    "\n",
    "# Test GPU computation\n",
    "print(f\"\\nGPU computation test:\")\n",
    "try:\n",
    "    with tf.device('/GPU:0'):\n",
    "        a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "        b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "        result = c.numpy()\n",
    "    print(f\"  ✅ GPU computation successful: {result.flatten()}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ GPU computation failed: {e}\")\n",
    "    print(f\"  Falling back to CPU...\")\n",
    "\n",
    "# Check if pipeline functions are using GPU\n",
    "print(f\"\\nChecking Word2GM pipeline GPU usage...\")\n",
    "try:\n",
    "    # Check if the pipeline functions have device placement\n",
    "    import inspect\n",
    "    from word2gm_fast.dataprep import pipeline\n",
    "    \n",
    "    # Look for GPU-related code in the pipeline\n",
    "    source = inspect.getsource(pipeline.batch_prepare_training_data)\n",
    "    if 'GPU' in source or 'device' in source:\n",
    "        print(\"  ✅ Pipeline appears to have GPU support\")\n",
    "    else:\n",
    "        print(\"  ⚠️  Pipeline may not be using GPU - check implementation\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Could not inspect pipeline: {e}\")\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4733c1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 SYSTEMATIC GPU DIAGNOSIS\n",
      "============================================================\n",
      "1. HARDWARE-LEVEL GPU CHECK:\n",
      "------------------------------\n",
      "   ✅ GPU 0: /dev/nvidia0 - ACCESSIBLE\n",
      "   ❌ GPU 1: /dev/nvidia1 - PERMISSION ERROR: [Errno 1] Operation not permitted: '/dev/nvidia1'\n",
      "   ❌ GPU 2: /dev/nvidia2 - PERMISSION ERROR: [Errno 1] Operation not permitted: '/dev/nvidia2'\n",
      "   ❌ GPU 3: /dev/nvidia3 - PERMISSION ERROR: [Errno 1] Operation not permitted: '/dev/nvidia3'\n",
      "   Summary: 1 GPU(s) accessible at hardware level\n",
      "\n",
      "2. ENVIRONMENT VARIABLES:\n",
      "------------------------------\n",
      "   CUDA_VISIBLE_DEVICES: 0\n",
      "   CUDA_DEVICE_ORDER: NOT SET\n",
      "   NVIDIA_VISIBLE_DEVICES: all\n",
      "\n",
      "3. TENSORFLOW GPU SUPPORT:\n",
      "------------------------------\n",
      "   TensorFlow version: 2.19.0\n",
      "   Built with CUDA: True\n",
      "   GPU support available: False\n",
      "\n",
      "4. TENSORFLOW PHYSICAL DEVICES:\n",
      "------------------------------\n",
      "   All devices: 1\n",
      "     PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "   GPU devices: 0\n",
      "\n",
      "5. CUDA LIBRARY CHECK:\n",
      "------------------------------\n",
      "   nvcc available: 12.6\n",
      "   ❌ libcuda.so.1 failed to load: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\n",
      "6. MODULE ENVIRONMENT (NYU Greene):\n",
      "------------------------------\n",
      "   Module check failed: [Errno 2] No such file or directory: 'module'\n",
      "\n",
      "7. RECOMMENDED FIXES:\n",
      "------------------------------\n",
      "   🎯 DIAGNOSIS: Hardware GPU accessible, but TensorFlow can't see it\n",
      "   📋 LIKELY CAUSES:\n",
      "      - CUDA/cuDNN version mismatch with TensorFlow\n",
      "      - TensorFlow not built with GPU support\n",
      "      - Missing CUDA modules on cluster\n",
      "   🔧 RECOMMENDED ACTIONS:\n",
      "      1. Load CUDA module: module load cuda/11.8\n",
      "      2. Check TensorFlow-GPU installation\n",
      "      3. Verify CUDA/cuDNN compatibility\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔧 SYSTEMATIC GPU DIAGNOSIS\n",
    "print(\"🔧 SYSTEMATIC GPU DIAGNOSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Hardware-level GPU check\n",
    "print(\"1. HARDWARE-LEVEL GPU CHECK:\")\n",
    "print(\"-\" * 30)\n",
    "import os\n",
    "accessible_gpus = []\n",
    "for gpu_id in range(4):\n",
    "    device_path = f\"/dev/nvidia{gpu_id}\"\n",
    "    if os.path.exists(device_path):\n",
    "        try:\n",
    "            with open(device_path, 'rb') as f:\n",
    "                pass\n",
    "            accessible_gpus.append(gpu_id)\n",
    "            print(f\"   ✅ GPU {gpu_id}: {device_path} - ACCESSIBLE\")\n",
    "        except (PermissionError, OSError) as e:\n",
    "            print(f\"   ❌ GPU {gpu_id}: {device_path} - PERMISSION ERROR: {e}\")\n",
    "    else:\n",
    "        print(f\"   ❌ GPU {gpu_id}: {device_path} - NOT FOUND\")\n",
    "\n",
    "print(f\"   Summary: {len(accessible_gpus)} GPU(s) accessible at hardware level\")\n",
    "\n",
    "# Step 2: Environment variables\n",
    "print(\"\\n2. ENVIRONMENT VARIABLES:\")\n",
    "print(\"-\" * 30)\n",
    "env_vars = ['CUDA_VISIBLE_DEVICES', 'CUDA_DEVICE_ORDER', 'NVIDIA_VISIBLE_DEVICES']\n",
    "for var in env_vars:\n",
    "    value = os.environ.get(var, 'NOT SET')\n",
    "    print(f\"   {var}: {value}\")\n",
    "\n",
    "# Step 3: TensorFlow GPU support\n",
    "print(\"\\n3. TENSORFLOW GPU SUPPORT:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"   TensorFlow version: {tf.__version__}\")\n",
    "print(f\"   Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "print(f\"   GPU support available: {tf.test.is_gpu_available()}\")\n",
    "\n",
    "# Step 4: Physical devices detected by TensorFlow\n",
    "print(\"\\n4. TENSORFLOW PHYSICAL DEVICES:\")\n",
    "print(\"-\" * 30)\n",
    "all_devices = tf.config.list_physical_devices()\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f\"   All devices: {len(all_devices)}\")\n",
    "for device in all_devices:\n",
    "    print(f\"     {device}\")\n",
    "print(f\"   GPU devices: {len(gpu_devices)}\")\n",
    "for device in gpu_devices:\n",
    "    print(f\"     {device}\")\n",
    "\n",
    "# Step 5: CUDA library check\n",
    "print(\"\\n5. CUDA LIBRARY CHECK:\")\n",
    "print(\"-\" * 30)\n",
    "try:\n",
    "    # Try to get CUDA version info\n",
    "    import subprocess\n",
    "    result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"   nvcc available: {result.stdout.split('release')[1].split(',')[0].strip()}\")\n",
    "    else:\n",
    "        print(f\"   nvcc not available or error: {result.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"   nvcc check failed: {e}\")\n",
    "\n",
    "# Check for libcuda\n",
    "import ctypes\n",
    "try:\n",
    "    libcuda = ctypes.CDLL('libcuda.so.1')\n",
    "    print(f\"   ✅ libcuda.so.1 loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ libcuda.so.1 failed to load: {e}\")\n",
    "\n",
    "# Step 6: Module environment (HPC specific)\n",
    "print(\"\\n6. MODULE ENVIRONMENT (NYU Greene):\")\n",
    "print(\"-\" * 30)\n",
    "try:\n",
    "    result = subprocess.run(['module', 'list'], capture_output=True, text=True, timeout=5)\n",
    "    if 'cuda' in result.stderr.lower():\n",
    "        print(f\"   ✅ CUDA module appears to be loaded\")\n",
    "        # Extract CUDA modules\n",
    "        cuda_lines = [line for line in result.stderr.split('\\n') if 'cuda' in line.lower()]\n",
    "        for line in cuda_lines:\n",
    "            print(f\"     {line.strip()}\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  No CUDA module detected in module list\")\n",
    "        print(f\"   Available modules with 'cuda': \")\n",
    "        avail_result = subprocess.run(['module', 'avail', 'cuda'], capture_output=True, text=True, timeout=5)\n",
    "        print(f\"     {avail_result.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Module check failed: {e}\")\n",
    "\n",
    "# Step 7: Recommended fixes\n",
    "print(\"\\n7. RECOMMENDED FIXES:\")\n",
    "print(\"-\" * 30)\n",
    "if len(accessible_gpus) > 0 and len(gpu_devices) == 0:\n",
    "    print(\"   🎯 DIAGNOSIS: Hardware GPU accessible, but TensorFlow can't see it\")\n",
    "    print(\"   📋 LIKELY CAUSES:\")\n",
    "    print(\"      - CUDA/cuDNN version mismatch with TensorFlow\")\n",
    "    print(\"      - TensorFlow not built with GPU support\")\n",
    "    print(\"      - Missing CUDA modules on cluster\")\n",
    "    print(\"   🔧 RECOMMENDED ACTIONS:\")\n",
    "    print(\"      1. Load CUDA module: module load cuda/11.8\")\n",
    "    print(\"      2. Check TensorFlow-GPU installation\")\n",
    "    print(\"      3. Verify CUDA/cuDNN compatibility\")\n",
    "elif len(accessible_gpus) == 0:\n",
    "    print(\"   🎯 DIAGNOSIS: No GPU accessible at hardware level\")\n",
    "    print(\"   🔧 RECOMMENDED ACTIONS:\")\n",
    "    print(\"      1. Request GPU node: salloc --gres=gpu:1\")\n",
    "    print(\"      2. Check job allocation\")\n",
    "else:\n",
    "    print(\"   ✅ GPU appears to be properly configured\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317dc7d",
   "metadata": {},
   "source": [
    "## 🔧 GPU Fix Required: Restart Kernel\n",
    "\n",
    "**DIAGNOSIS**: CUDA runtime libraries weren't loaded when TensorFlow was imported.\n",
    "\n",
    "**SOLUTION**: \n",
    "1. ✅ CUDA module loaded: `module load cuda/11.6.2`  \n",
    "2. 🔄 **RESTART KERNEL** (Kernel → Restart Kernel)\n",
    "3. 🚀 Re-run setup cells with CUDA properly available\n",
    "\n",
    "After restart, TensorFlow should detect GPU 0 successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "638ed673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TESTING ACTUAL DEVICE PLACEMENT\n",
      "==================================================\n",
      "Testing with TensorFlow device placement logging:\n",
      "\n",
      "1. Requesting /GPU:0:\n",
      "   Requested: /GPU:0\n",
      "   Actually used: /job:localhost/replica:0/task:0/device:CPU:0\n",
      "   ❌ CPU FALLBACK (soft placement)\n",
      "\n",
      "2. Explicitly requesting /CPU:0:\n",
      "   Requested: /CPU:0\n",
      "   Actually used: /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "🎯 CONCLUSION:\n",
      "   TensorFlow CANNOT see any GPU devices\n",
      "   All '/GPU:0' requests are falling back to CPU\n",
      "   The 'successful' GPU computation was actually CPU computation\n",
      "   This confirms the GPU configuration issue\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔍 DEFINITIVE GPU vs CPU TEST\n",
    "print(\"🔍 TESTING ACTUAL DEVICE PLACEMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with device logging enabled to see where computation REALLY runs\n",
    "print(\"Testing with TensorFlow device placement logging:\")\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "try:\n",
    "    print(\"\\n1. Requesting /GPU:0:\")\n",
    "    with tf.device('/GPU:0'):\n",
    "        a = tf.constant([[1.0, 2.0]])\n",
    "        b = tf.constant([[3.0], [4.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "        actual_device = c.device\n",
    "        result = c.numpy()\n",
    "    \n",
    "    print(f\"   Requested: /GPU:0\")\n",
    "    print(f\"   Actually used: {actual_device}\")\n",
    "    if \"GPU\" in actual_device:\n",
    "        print(\"   ✅ TRUE GPU computation\")\n",
    "    else:\n",
    "        print(\"   ❌ CPU FALLBACK (soft placement)\")\n",
    "\n",
    "    print(\"\\n2. Explicitly requesting /CPU:0:\")\n",
    "    with tf.device('/CPU:0'):\n",
    "        a = tf.constant([[1.0, 2.0]])\n",
    "        b = tf.constant([[3.0], [4.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "        actual_device = c.device\n",
    "        result = c.numpy()\n",
    "    \n",
    "    print(f\"   Requested: /CPU:0\")\n",
    "    print(f\"   Actually used: {actual_device}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Error during device testing: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Turn off device logging\n",
    "    tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "print(\"\\n🎯 CONCLUSION:\")\n",
    "if len(tf.config.list_physical_devices('GPU')) == 0:\n",
    "    print(\"   TensorFlow CANNOT see any GPU devices\")\n",
    "    print(\"   All '/GPU:0' requests are falling back to CPU\")\n",
    "    print(\"   The 'successful' GPU computation was actually CPU computation\")\n",
    "    print(\"   This confirms the GPU configuration issue\")\n",
    "else:\n",
    "    print(\"   TensorFlow can see GPU devices - GPU computation should work\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
