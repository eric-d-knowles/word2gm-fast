{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ce6ba",
   "metadata": {},
   "source": [
    "# Word2GM Training Data Pipeline\n",
    "\n",
    "**Pipeline: Corpus file → TFRecord training artifacts (triplets and vocabulary)**\n",
    "\n",
    "Use this notebook to prepare a Google 5gram corpora for Word2GM skip-gram training.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Input**: Preprocessed corpus file (e.g., `2019.txt`) in `/vast` NVMe storage\n",
    "2. **Processing**: TensorFlow-native filtering, vocabulary building, and triplet generation\n",
    "3. **Output**: TFRecord artifacts in organized subdirectories (e.g., `2019_artifacts/`)\n",
    "\n",
    "### **Artifact Storage**\n",
    "The pipeline creates year-specific subdirectories alongside the original text corpora:\n",
    "<pre>\n",
    "/vast/edk202/NLP_corpora/.../data/\n",
    "├── 2018.txt\n",
    "├── 2019.txt\n",
    "├── 2020.txt\n",
    "├── 2018_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "├── 2019_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "└── 2020_artifacts/\n",
    "    ├── triplets.tfrecord.gz\n",
    "    └── vocab.tfrecord.gz\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49f604",
   "metadata": {},
   "source": [
    "## Set Up for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89181d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>Autoreload enabled</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Project root: /scratch/edk202/word2gm-fast\n",
       "TensorFlow version: 2.19.0\n",
       "Device mode: CPU-only</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Data preprocessing environment ready!</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set project root directory and add `src` to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import the notebook setup utilities\n",
    "from word2gm_fast.utils.notebook_setup import setup_data_preprocessing_notebook, enable_autoreload\n",
    "\n",
    "# Enable autoreload for development\n",
    "enable_autoreload()\n",
    "\n",
    "# Set up environment (CPU-only for data preprocessing)\n",
    "env = setup_data_preprocessing_notebook(project_root=PROJECT_ROOT)\n",
    "\n",
    "# Extract commonly used modules for convenience\n",
    "tf = env['tensorflow']\n",
    "np = env['numpy']\n",
    "pd = env['pandas']\n",
    "batch_prepare_training_data = env['batch_prepare_training_data']\n",
    "print_resource_summary = env['print_resource_summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72a7c6",
   "metadata": {},
   "source": [
    "## Print Resource Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43483cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>SYSTEM RESOURCE SUMMARY\n",
       "============================================================\n",
       "Hostname: cm048.hpc.nyu.edu\n",
       "\n",
       "Job Allocation:\n",
       "   CPUs: 48\n",
       "   Memory: 125.0 GB\n",
       "   Requested partitions: short\n",
       "   Running on: SSH failed: Host key verification failed.\n",
       "   Job ID: 63410489\n",
       "   Node list: cm048\n",
       "\n",
       "GPU Information:\n",
       "   Error: NVML Shared Library Not Found\n",
       "\n",
       "TensorFlow GPU Detection:\n",
       "   TensorFlow detects 0 GPU(s)\n",
       "   Built with CUDA: True\n",
       "============================================================</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301858",
   "metadata": {},
   "source": [
    "## Prepare Corpora\n",
    "\n",
    "Here, we run the data-preparation pipeline from start to finish — reading preprocessed ngram corpora, generating all valid triplets, extracting the vocabulary, and saving the triplets and vocabulary as `tfrecord` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14da0c0",
   "metadata": {},
   "source": [
    "### Options for Data Preparation\n",
    "\n",
    "You can control which years are processed and how the batch preparation runs by adjusting the arguments to `batch_prepare_training_data`:\n",
    "\n",
    "**Ways to specify years:**\n",
    "- `year_range=\"2010\"` — Process a single year (e.g., only 2010).\n",
    "- `year_range=\"2010,2012,2015\"` — Process a comma-separated list of years.\n",
    "- `year_range=\"2010-2015\"` — Process a range of years, inclusive (2010 through 2015).\n",
    "- `year_range=\"2010,2012-2014,2016\"` — Combine individual years and ranges (2010, 2012, 2013, 2014, 2016).\n",
    "\n",
    "**Other options:**\n",
    "- `compress` — If `True`, output TFRecords are gzip-compressed. If `False`, output is uncompressed.\n",
    "- `show_progress` — If `True`, display a progress bar for each year.\n",
    "- `show_summary` — If `True`, print a summary of the processed data for each year.\n",
    "- `use_multiprocessing` — If `True`, process years in parallel using multiple CPU cores (recommended for large datasets).\n",
    "\n",
    "See the function docstring or source for more advanced options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e2724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested 100 years, found 100 corpus files.\n",
      "\n",
      "=================================================================\n",
      "PARALLEL BATCH PROCESSING\n",
      "=================================================================\n",
      "Processing 100 years\n",
      "Using 48 parallel workers\n",
      "Estimated speedup: 48.0x\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804 complete (1/100): 32,051 triplets, 9,769 vocab, 79.7s\n",
      "1802 complete (2/100): 21,569 triplets, 8,487 vocab, 137.9s\n",
      "1808 complete (3/100): 83,265 triplets, 13,193 vocab, 222.3s\n",
      "1805 complete (4/100): 49,730 triplets, 11,609 vocab, 242.8s\n",
      "1801 complete (5/100): 41,106 triplets, 11,000 vocab, 244.9s\n",
      "1814 complete (6/100): 84,587 triplets, 14,965 vocab, 261.1s\n",
      "1803 complete (7/100): 96,772 triplets, 14,473 vocab, 349.8s\n",
      "1817 complete (8/100): 155,413 triplets, 17,803 vocab, 367.2s\n",
      "1807 complete (9/100): 100,828 triplets, 14,626 vocab, 392.3s\n",
      "1816 complete (10/100): 112,225 triplets, 16,144 vocab, 399.8s\n",
      "1806 complete (11/100): 106,612 triplets, 14,124 vocab, 456.8s\n",
      "1813 complete (12/100): 143,347 triplets, 17,428 vocab, 485.4s\n",
      "1812 complete (13/100): 199,177 triplets, 18,423 vocab, 526.2s\n",
      "1809 complete (14/100): 212,783 triplets, 19,226 vocab, 537.2s\n",
      "1815 complete (15/100): 184,543 triplets, 17,905 vocab, 571.2s\n",
      "1811 complete (16/100): 196,589 triplets, 17,916 vocab, 695.1s\n",
      "1827 complete (17/100): 230,575 triplets, 20,385 vocab, 773.9s\n",
      "1823 complete (18/100): 281,554 triplets, 21,623 vocab, 780.5s\n",
      "1828 complete (19/100): 327,616 triplets, 22,406 vocab, 846.2s\n",
      "1818 complete (20/100): 273,148 triplets, 20,694 vocab, 864.9s\n",
      "1826 complete (21/100): 307,840 triplets, 22,657 vocab, 881.2s\n",
      "1819 complete (22/100): 329,830 triplets, 22,842 vocab, 911.1s\n",
      "1821 complete (23/100): 421,826 triplets, 23,237 vocab, 939.5s\n",
      "1810 complete (24/100): 343,340 triplets, 20,429 vocab, 953.5s\n",
      "1835 complete (25/100): 440,123 triplets, 25,335 vocab, 1050.0s\n",
      "1825 complete (26/100): 369,648 triplets, 23,176 vocab, 1096.0s\n",
      "1837 complete (27/100): 376,450 triplets, 25,101 vocab, 1108.6s\n",
      "1841 complete (28/100): 515,738 triplets, 26,666 vocab, 1229.3s\n",
      "1838 complete (29/100): 471,996 triplets, 26,324 vocab, 1285.1s\n",
      "1820 complete (30/100): 510,730 triplets, 24,563 vocab, 1310.3s\n",
      "1830 complete (31/100): 474,687 triplets, 25,450 vocab, 1315.8s\n",
      "1824 complete (32/100): 487,896 triplets, 24,419 vocab, 1343.5s\n",
      "1833 complete (33/100): 543,217 triplets, 26,916 vocab, 1378.3s\n",
      "1822 complete (34/100): 508,497 triplets, 25,169 vocab, 1429.8s\n",
      "1831 complete (35/100): 657,875 triplets, 27,548 vocab, 1478.3s\n",
      "1840 complete (36/100): 620,934 triplets, 27,640 vocab, 1734.5s\n",
      "1829 complete (37/100): 680,433 triplets, 27,319 vocab, 1791.1s\n",
      "1844 complete (38/100): 676,532 triplets, 29,014 vocab, 1793.0s\n",
      "1839 complete (39/100): 801,222 triplets, 30,160 vocab, 1823.3s\n",
      "1848 complete (40/100): 697,994 triplets, 29,711 vocab, 1839.2s\n",
      "1845 complete (41/100): 731,526 triplets, 29,980 vocab, 1851.1s\n",
      "1832 complete (42/100): 709,932 triplets, 27,982 vocab, 1854.3s\n",
      "1834 complete (43/100): 798,636 triplets, 28,853 vocab, 1881.3s\n",
      "1843 complete (44/100): 717,690 triplets, 29,418 vocab, 1922.4s\n",
      "1836 complete (45/100): 740,850 triplets, 29,166 vocab, 1932.8s\n",
      "1846 complete (46/100): 741,581 triplets, 29,618 vocab, 1990.2s\n",
      "1847 complete (47/100): 762,281 triplets, 30,204 vocab, 1993.9s\n",
      "1849 complete (48/100): 858,398 triplets, 31,463 vocab, 1991.1s\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "\n",
    "downsample_threshold = 1e-5  # Typical values: 1e-3 (conservative), 1e-4 (moderate), 1e-5 (aggressive)\n",
    "\n",
    "# Process year(s)\n",
    "results = batch_prepare_training_data(\n",
    "    corpus_dir=corpus_dir,\n",
    "    year_range=\"1801-1900\",\n",
    "    compress=False,\n",
    "    show_progress=True,\n",
    "    show_summary=True,\n",
    "    use_multiprocessing=True,\n",
    "    downsample_threshold=downsample_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f02d9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
