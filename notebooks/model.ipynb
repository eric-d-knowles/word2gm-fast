{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's measure the ACTUAL triplet generation time by materializing the dataset\n",
    "print(\"\\n[-- Actual Triplet Generation Timing --]\\n\")\n",
    "\n",
    "# Count total triplets (this forces evaluation) - WARNING: This takes a long time!\n",
    "# Uncomment the lines below to see the real timing (takes ~15 minutes for large files)\n",
    "\n",
    "# start_actual = time.time()\n",
    "# total_triplets = sum(1 for _ in triplets_ds)\n",
    "# duration_actual = time.time() - start_actual\n",
    "# rate_actual = file_size / duration_actual\n",
    "\n",
    "# print(f\"Total triplets generated: {total_triplets:,}\")\n",
    "# print(f\"Actual generation time: {duration_actual:.2f}s\")\n",
    "# print(f\"Actual generation rate: {rate_actual:.2f} MB/s\")\n",
    "# print(f\"Triplets per second: {total_triplets/duration_actual:,.0f}\")\n",
    "\n",
    "# The original timing was just building the computation graph!\n",
    "print(f\"Graph construction time: {duration_triplets:.3f}s\")\n",
    "print(\"Actual processing time: ~15 minutes for large files\")\n",
    "print(\"The 'fast' triplet generation was just building the computation graph!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39454b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:26:45.295294: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-21 01:26:45.311914: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750483605.329218 2870105 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750483605.334406 2870105 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750483605.348141 2870105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750483605.348167 2870105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750483605.348168 2870105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750483605.348170 2870105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-21 01:26:45.352504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "os.chdir('/scratch/edk202/word2gm-fast/notebooks')\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from src.word2gm_fast.dataprep.corpus_to_dataset import make_dataset\n",
    "from src.word2gm_fast.dataprep.index_vocab import make_vocab\n",
    "from src.word2gm_fast.dataprep.dataset_to_triplets import build_skipgram_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0779c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 00:40:52.893344: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-21 00:40:52.907089: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750480852.923017 3902574 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750480852.927811 3902574 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750480852.940672 3902574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480852.940695 3902574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480852.940699 3902574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480852.940701 3902574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-21 00:40:52.944462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "W0000 00:00:1750480852.940672 3902574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480852.940695 3902574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480852.940699 3902574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480852.940701 3902574 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-21 00:40:52.944462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-21 00:40:55.106375: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-06-21 00:40:55.106375: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-06-21 00:40:55.400663: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.297s\n",
      "\n",
      "OK\n",
      "2025-06-21 00:40:55.400663: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.297s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest -b tests.test_corpus_to_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb216b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 00:40:56.420920: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-21 00:40:56.434513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750480856.450534 3902645 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750480856.455334 3902645 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750480856.468235 3902645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480856.468255 3902645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480856.468258 3902645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480856.468261 3902645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-21 00:40:56.472076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "W0000 00:00:1750480856.468235 3902645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480856.468255 3902645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480856.468258 3902645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480856.468261 3902645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-21 00:40:56.472076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-21 00:40:58.628859: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-06-21 00:40:58.628859: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-06-21 00:40:58.766740: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      ".2025-06-21 00:40:58.766740: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.151s\n",
      "\n",
      "OK\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.151s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest -b tests.test_index_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d290ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 00:40:59.807210: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-21 00:40:59.820766: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750480859.836531 3902688 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750480859.841335 3902688 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750480859.854048 3902688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480859.854073 3902688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480859.854075 3902688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480859.854078 3902688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-21 00:40:59.857872: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "W0000 00:00:1750480859.854048 3902688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480859.854073 3902688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480859.854075 3902688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750480859.854078 3902688 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-21 00:40:59.857872: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "test_basic_triplet_generation (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_basic_triplet_generation)\n",
      "Test that triplets are generated with correct structure. ... 2025-06-21 00:41:02.021831: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "test_basic_triplet_generation (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_basic_triplet_generation)\n",
      "Test that triplets are generated with correct structure. ... 2025-06-21 00:41:02.021831: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-06-21 00:41:02.314831: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "ok\n",
      "test_center_word_extraction (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_center_word_extraction)\n",
      "Test that center words are correctly extracted (3rd token). ... 2025-06-21 00:41:02.314831: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "ok\n",
      "test_center_word_extraction (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_center_word_extraction)\n",
      "Test that center words are correctly extracted (3rd token). ... 2025-06-21 00:41:02.430636: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "ok\n",
      "test_context_word_extraction (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_context_word_extraction)\n",
      "Test that positive words come from context positions. ... 2025-06-21 00:41:02.430636: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "ok\n",
      "test_context_word_extraction (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_context_word_extraction)\n",
      "Test that positive words come from context positions. ... ok\n",
      "test_multiple_triplets_per_line (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_multiple_triplets_per_line)\n",
      "Test that multiple triplets are generated per valid line. ... ok\n",
      "test_multiple_triplets_per_line (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_multiple_triplets_per_line)\n",
      "Test that multiple triplets are generated per valid line. ... 2025-06-21 00:41:02.652343: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "ok\n",
      "test_negative_sampling_range (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_negative_sampling_range)\n",
      "Test that negative samples are in the correct range. ... 2025-06-21 00:41:02.652343: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "ok\n",
      "test_negative_sampling_range (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_negative_sampling_range)\n",
      "Test that negative samples are in the correct range. ... ok\n",
      "test_no_unk_centers (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_no_unk_centers)\n",
      "Test that UNK tokens are never used as center words. ... ok\n",
      "test_no_unk_centers (tests.test_dataset_to_triplets.TestDatasetToTriplets.test_no_unk_centers)\n",
      "Test that UNK tokens are never used as center words. ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.859s\n",
      "\n",
      "OK\n",
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.859s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest -v tests.test_dataset_to_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233685ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS FILE:  /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1850.txt \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:26:50.080213: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-06-21 01:28:54.910835: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-06-21 01:28:54.910835: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--    Benchmarks    --]\n",
      "\n",
      "Step                                 Duration             Quantity                 Rate\n",
      "---------------------------------------------------------------------------------------\n",
      "Corpus loading and filtering           0.29 s            120.81 MB          419.37 MB/s\n",
      "Vocabulary creation                  124.59 s            120.81 MB            0.97 MB/s\n",
      "Triplet generation                     0.15 s            120.81 MB          788.08 MB/s\n",
      "\n",
      "[--   Sample Lines   --]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:28:55.449700: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK one another UNK two\n",
      "UNK one day UNK serve\n",
      "UNK one another UNK good\n",
      "UNK one afore long UNK\n",
      "UNK one back UNK UNK\n",
      "\n",
      "[--    Test Words    --]\n",
      "\n",
      "Word                   ID\n",
      "-------------------------\n",
      "UNK                     0\n",
      "man                 18349\n",
      "king                16702\n",
      "nonexistentword         0\n",
      "\n",
      "[--  Sample Triplets  --]\n",
      "\n",
      "Center   Center Word  Positive Pos Word     Negative Neg Word    \n",
      "---------------------------------------------------------------------------\n",
      "1638     article      24205    rank         6700     corrigan    \n",
      "2515     battle       20956    one          13066    gratefully  \n",
      "3239     blood        20956    one          4585     carpentaria \n",
      "833      almost       15475    infinite     10444    evangelicals\n",
      "1658     artorius     982      among        14982    impending   \n",
      "1638     article      24205    rank         6700     corrigan    \n",
      "2515     battle       20956    one          13066    gratefully  \n",
      "3239     blood        20956    one          4585     carpentaria \n",
      "833      almost       15475    infinite     10444    evangelicals\n",
      "1658     artorius     982      among        14982    impending   \n"
     ]
    }
   ],
   "source": [
    "# Corpus file information\n",
    "corpus_file = \"1850.txt\"\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "corpus_path = os.path.join(corpus_dir, corpus_file)\n",
    "file_size = os.path.getsize(corpus_path) / 1024 / 1024\n",
    "print(\"CORPUS FILE: \", corpus_path, \"\\n\")\n",
    "\n",
    "# Load and filter the corpus\n",
    "start = time.time()\n",
    "dataset, _ = make_dataset(corpus_path)\n",
    "duration_load = time.time() - start\n",
    "rate_load = file_size / duration_load\n",
    "dataset = dataset.cache()\n",
    "\n",
    "# Build the vocabulary from the dataset\n",
    "start = time.time()\n",
    "vocab_table = make_vocab(dataset)\n",
    "duration_vocab = time.time() - start\n",
    "rate_vocab = file_size / duration_vocab\n",
    "\n",
    "# Make triplets from the dataset and vocabulary table\n",
    "start = time.time()\n",
    "triplets_ds = build_skipgram_triplets(dataset, vocab_table)\n",
    "duration_triplets = time.time() - start\n",
    "rate_triplets = file_size / duration_triplets\n",
    "\n",
    "# Benchmarking output\n",
    "print(\"[--    Benchmarks    --]\\n\")\n",
    "print(f\"{'Step':<35}{'Duration':>10}{'Quantity':>21}{'Rate':>21}\")\n",
    "print(\"-\" * 87)\n",
    "print(f\"{'Corpus loading and filtering':<35}{duration_load:8,.2f}{'s':>2}{file_size:18,.2f}{'MB':>3}{rate_load:16,.2f}{'MB/s':>5}\")\n",
    "print(f\"{'Vocabulary creation':<35}{duration_vocab:8,.2f}{'s':>2}{file_size:18,.2f}{'MB':>3}{rate_vocab:16,.2f}{'MB/s':>5}\")\n",
    "print(f\"{'Triplet generation':<35}{duration_triplets:8,.2f}{'s':>2}{file_size:18,.2f}{'MB':>3}{rate_triplets:16,.2f}{'MB/s':>5}\")\n",
    "\n",
    "# Create reverse lookup from vocab table once at the beginning\n",
    "vocab_export = vocab_table.export()\n",
    "vocab_keys = vocab_export[0].numpy()\n",
    "vocab_values = vocab_export[1].numpy()\n",
    "index_to_word = {idx: word.decode('utf-8') for word, idx in zip(vocab_keys, vocab_values)}\n",
    "\n",
    "# Show sample lines (use a fresh iterator)\n",
    "print(\"\\n[--   Sample Lines   --]\\n\")\n",
    "sample_lines = list(dataset.shuffle(1000, seed=42).take(5).as_numpy_iterator())\n",
    "for line_bytes in sample_lines:\n",
    "    print(line_bytes.decode(\"utf-8\"))\n",
    "\n",
    "# Test the vocab table with example words\n",
    "print(\"\\n[--    Test Words    --]\\n\")\n",
    "test_words = [\"UNK\", \"man\", \"king\", \"nonexistentword\"]\n",
    "ids = vocab_table.lookup(tf.constant(test_words)).numpy()\n",
    "print(f\"{'Word':<18} {'ID':>6}\")\n",
    "print(\"-\" * 25)\n",
    "for word, idx in zip(test_words, ids):\n",
    "    print(f\"{word:<18} {idx:>6}\")\n",
    "\n",
    "# Show sample triplets\n",
    "print(\"\\n[--  Sample Triplets  --]\\n\")\n",
    "print(f\"{'Center':<8} {'Center Word':<12} {'Positive':<8} {'Pos Word':<12} {'Negative':<8} {'Neg Word':<12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Get sample triplets (use a fresh iterator with seed for reproducibility)\n",
    "sample_triplets = list(triplets_ds.shuffle(1000, seed=123).take(5).as_numpy_iterator())\n",
    "for triplet in sample_triplets:\n",
    "    center, positive, negative = triplet\n",
    "    center_word = index_to_word.get(center, f\"ID_{center}\")\n",
    "    pos_word = index_to_word.get(positive, f\"ID_{positive}\")\n",
    "    neg_word = index_to_word.get(negative, f\"ID_{negative}\")\n",
    "    print(f\"{center:<8} {center_word:<12} {positive:<8} {pos_word:<12} {negative:<8} {neg_word:<12}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed5079f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-- TF-Native vs Python Comparison --]\n",
      "\n",
      "Testing Python-heavy approach...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:28:56.684578: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python-heavy (1000 lines): 0.845s, 1858 triplets\n",
      "Testing TF-native approach...\n",
      "TF-native (same count): 0.527s, 1858 triplets\n",
      "\n",
      "Speedup from TF-native: 1.6x faster\n",
      "\n",
      "[-- Where TF-Native Really Matters --]\n",
      "\n",
      "1. BATCHED PROCESSING:\n",
      "   - TF datasets can be batched, prefetched, and parallelized\n",
      "   - Python iteration is sequential and single-threaded\n",
      "\n",
      "2. MEMORY EFFICIENCY:\n",
      "   - TF datasets stream data without loading everything into memory\n",
      "   - Python approach above loads all triplets into a list\n",
      "\n",
      "3. INTEGRATION WITH TRAINING:\n",
      "   - TF datasets integrate seamlessly with tf.keras.Model.fit()\n",
      "   - No need to convert between Python objects and tensors\n",
      "\n",
      "4. GRAPH COMPILATION:\n",
      "   - With tf.function or XLA, TF ops can be compiled for speed\n",
      "   - Python logic cannot be compiled\n",
      "\n",
      "5. MEMORY USAGE DEMO:\n",
      "   Memory before: 1147.7 MB\n",
      "   Memory after Python triplets: 1147.7 MB\n",
      "   Memory increase: 0.0 MB\n",
      "   TF dataset memory: minimal (lazy evaluation)\n",
      "\n",
      "6. SCALABILITY:\n",
      "   Python approach: 1858 triplets = 0.0 MB\n",
      "   For 14M triplets: ~0.0 GB!\n",
      "   TF approach: streams data, constant memory usage\n",
      "TF-native (same count): 0.527s, 1858 triplets\n",
      "\n",
      "Speedup from TF-native: 1.6x faster\n",
      "\n",
      "[-- Where TF-Native Really Matters --]\n",
      "\n",
      "1. BATCHED PROCESSING:\n",
      "   - TF datasets can be batched, prefetched, and parallelized\n",
      "   - Python iteration is sequential and single-threaded\n",
      "\n",
      "2. MEMORY EFFICIENCY:\n",
      "   - TF datasets stream data without loading everything into memory\n",
      "   - Python approach above loads all triplets into a list\n",
      "\n",
      "3. INTEGRATION WITH TRAINING:\n",
      "   - TF datasets integrate seamlessly with tf.keras.Model.fit()\n",
      "   - No need to convert between Python objects and tensors\n",
      "\n",
      "4. GRAPH COMPILATION:\n",
      "   - With tf.function or XLA, TF ops can be compiled for speed\n",
      "   - Python logic cannot be compiled\n",
      "\n",
      "5. MEMORY USAGE DEMO:\n",
      "   Memory before: 1147.7 MB\n",
      "   Memory after Python triplets: 1147.7 MB\n",
      "   Memory increase: 0.0 MB\n",
      "   TF dataset memory: minimal (lazy evaluation)\n",
      "\n",
      "6. SCALABILITY:\n",
      "   Python approach: 1858 triplets = 0.0 MB\n",
      "   For 14M triplets: ~0.0 GB!\n",
      "   TF approach: streams data, constant memory usage\n"
     ]
    }
   ],
   "source": [
    "# Let's compare TF-native vs Python-based approaches to see where optimizations matter\n",
    "print(\"\\n[-- TF-Native vs Python Comparison --]\\n\")\n",
    "\n",
    "# First, let's try a more Python-heavy approach for triplet generation\n",
    "def python_heavy_triplets(dataset, vocab_table):\n",
    "    \"\"\"Generate triplets using more Python logic (less TF-friendly)\"\"\"\n",
    "    import random\n",
    "    random.seed(123)  # For reproducibility\n",
    "    \n",
    "    triplets = []\n",
    "    vocab_size_int = int(vocab_table.size().numpy())\n",
    "    \n",
    "    for line_tensor in dataset:\n",
    "        line = line_tensor.numpy().decode('utf-8')\n",
    "        tokens = line.strip().split()\n",
    "        \n",
    "        if len(tokens) != 5:\n",
    "            continue\n",
    "            \n",
    "        # Look up token IDs one by one (less efficient)\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            token_id = vocab_table.lookup(tf.constant([token])).numpy()[0]\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        center_id = token_ids[2]\n",
    "        context_ids = [token_ids[0], token_ids[1], token_ids[3], token_ids[4]]\n",
    "        \n",
    "        # Skip if center is UNK\n",
    "        if center_id == 0:\n",
    "            continue\n",
    "            \n",
    "        # Generate triplets for each valid context\n",
    "        for context_id in context_ids:\n",
    "            if context_id != 0:  # Skip UNK context\n",
    "                negative_id = random.randint(1, vocab_size_int - 1)\n",
    "                triplets.append((center_id, context_id, negative_id))\n",
    "    \n",
    "    return triplets\n",
    "\n",
    "# Time the Python-heavy approach\n",
    "print(\"Testing Python-heavy approach...\")\n",
    "start_python = time.time()\n",
    "python_triplets = python_heavy_triplets(dataset.take(1000), vocab_table)  # Just 1000 lines for comparison\n",
    "duration_python = time.time() - start_python\n",
    "print(f\"Python-heavy (1000 lines): {duration_python:.3f}s, {len(python_triplets)} triplets\")\n",
    "\n",
    "# Time the TF-native approach on same subset\n",
    "print(\"Testing TF-native approach...\")\n",
    "start_tf = time.time()\n",
    "tf_triplets = list(triplets_ds.take(len(python_triplets)))\n",
    "duration_tf = time.time() - start_tf\n",
    "print(f\"TF-native (same count): {duration_tf:.3f}s, {len(tf_triplets)} triplets\")\n",
    "\n",
    "print(f\"\\nSpeedup from TF-native: {duration_python/duration_tf:.1f}x faster\")\n",
    "\n",
    "# But the REAL benefits of TF-native code show up in different scenarios:\n",
    "print(\"\\n[-- Where TF-Native Really Matters --]\\n\")\n",
    "\n",
    "print(\"1. BATCHED PROCESSING:\")\n",
    "print(\"   - TF datasets can be batched, prefetched, and parallelized\")\n",
    "print(\"   - Python iteration is sequential and single-threaded\")\n",
    "\n",
    "print(\"\\n2. MEMORY EFFICIENCY:\")\n",
    "print(\"   - TF datasets stream data without loading everything into memory\")\n",
    "print(\"   - Python approach above loads all triplets into a list\")\n",
    "\n",
    "print(\"\\n3. INTEGRATION WITH TRAINING:\")\n",
    "print(\"   - TF datasets integrate seamlessly with tf.keras.Model.fit()\")\n",
    "print(\"   - No need to convert between Python objects and tensors\")\n",
    "\n",
    "print(\"\\n4. GRAPH COMPILATION:\")\n",
    "print(\"   - With tf.function or XLA, TF ops can be compiled for speed\")\n",
    "print(\"   - Python logic cannot be compiled\")\n",
    "\n",
    "# Demonstrate memory efficiency difference\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "print(f\"\\n5. MEMORY USAGE DEMO:\")\n",
    "print(f\"   Memory before: {memory_before:.1f} MB\")\n",
    "\n",
    "# The Python approach loaded all triplets into memory\n",
    "memory_after_python = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"   Memory after Python triplets: {memory_after_python:.1f} MB\")\n",
    "print(f\"   Memory increase: {memory_after_python - memory_before:.1f} MB\")\n",
    "\n",
    "# The TF dataset doesn't materialize until consumed\n",
    "print(f\"   TF dataset memory: minimal (lazy evaluation)\")\n",
    "\n",
    "print(f\"\\n6. SCALABILITY:\")\n",
    "print(f\"   Python approach: {len(python_triplets)} triplets = {memory_after_python - memory_before:.1f} MB\")\n",
    "print(f\"   For 14M triplets: ~{(memory_after_python - memory_before) * 14000000 / len(python_triplets) / 1024:.1f} GB!\")\n",
    "print(f\"   TF approach: streams data, constant memory usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4642522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-- Dataset Caching Investigation --]\n",
      "\n",
      "Creating fresh dataset without caching...\n",
      "Fresh dataset creation: 0.062s\n",
      "\n",
      "Caching the dataset...\n",
      "Cache operation: 0.001s\n",
      "\n",
      "First iteration through cached dataset (should materialize)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:30:46.991351: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-06-21 01:30:47.163967: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First iteration (1000 items): 0.241s, count: 1000\n",
      "\n",
      "Second iteration through same cached dataset...\n",
      "Second iteration (1000 items): 0.172s, count: 1000\n",
      "\n",
      "Caching speedup: 1.4x faster on second pass\n",
      "\n",
      "Memory before full dataset cache: 1148.9 MB\n",
      "Forcing full dataset into cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 01:32:43.002837: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full cache materialization: 115.84s\n",
      "Total items cached: 2,038,127\n",
      "Memory after full cache: 1639.0 MB\n",
      "Memory increase: 490.1 MB\n",
      "\n",
      "Testing cached performance...\n",
      "Iteration over fully cached dataset: 113.477s\n",
      "Items: 2,038,127\n",
      "\n",
      "[-- Caching Conclusions --]\n",
      "\n",
      "1. .cache() call itself: 0.001s (just creates cache object)\n",
      "2. First iteration: 0.241s (materializes data into cache)\n",
      "3. Subsequent iterations: 113.477s (reads from cache)\n",
      "4. Memory overhead: 490.1 MB for 2,038,127 items\n",
      "5. Cache speedup: 1x faster\n",
      "\n",
      "So caching does NOT materialize immediately - it's lazy until first use!\n",
      "Iteration over fully cached dataset: 113.477s\n",
      "Items: 2,038,127\n",
      "\n",
      "[-- Caching Conclusions --]\n",
      "\n",
      "1. .cache() call itself: 0.001s (just creates cache object)\n",
      "2. First iteration: 0.241s (materializes data into cache)\n",
      "3. Subsequent iterations: 113.477s (reads from cache)\n",
      "4. Memory overhead: 490.1 MB for 2,038,127 items\n",
      "5. Cache speedup: 1x faster\n",
      "\n",
      "So caching does NOT materialize immediately - it's lazy until first use!\n"
     ]
    }
   ],
   "source": [
    "# Let's test whether caching a dataset materializes it\n",
    "print(\"\\n[-- Dataset Caching Investigation --]\\n\")\n",
    "\n",
    "# Create a fresh dataset without caching\n",
    "print(\"Creating fresh dataset without caching...\")\n",
    "start_fresh = time.time()\n",
    "fresh_dataset, _ = make_dataset(corpus_path)\n",
    "duration_fresh = time.time() - start_fresh\n",
    "print(f\"Fresh dataset creation: {duration_fresh:.3f}s\")\n",
    "\n",
    "# Now cache it - does this materialize the data?\n",
    "print(\"\\nCaching the dataset...\")\n",
    "start_cache = time.time()\n",
    "cached_dataset = fresh_dataset.cache()\n",
    "duration_cache = time.time() - start_cache\n",
    "print(f\"Cache operation: {duration_cache:.3f}s\")\n",
    "\n",
    "# Test: does iterating through cached dataset take time the first time?\n",
    "print(\"\\nFirst iteration through cached dataset (should materialize)...\")\n",
    "start_first = time.time()\n",
    "first_count = sum(1 for _ in cached_dataset.take(1000))\n",
    "duration_first = time.time() - start_first\n",
    "print(f\"First iteration (1000 items): {duration_first:.3f}s, count: {first_count}\")\n",
    "\n",
    "# Test: does iterating through cached dataset again go faster?\n",
    "print(\"\\nSecond iteration through same cached dataset...\")\n",
    "start_second = time.time()\n",
    "second_count = sum(1 for _ in cached_dataset.take(1000))\n",
    "duration_second = time.time() - start_second\n",
    "print(f\"Second iteration (1000 items): {duration_second:.3f}s, count: {second_count}\")\n",
    "\n",
    "print(f\"\\nCaching speedup: {duration_first/duration_second:.1f}x faster on second pass\")\n",
    "\n",
    "# Check memory usage during caching\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_before_full_cache = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "print(f\"\\nMemory before full dataset cache: {memory_before_full_cache:.1f} MB\")\n",
    "\n",
    "# Force full materialization into cache\n",
    "print(\"Forcing full dataset into cache...\")\n",
    "start_full_cache = time.time()\n",
    "full_count = sum(1 for _ in cached_dataset)\n",
    "duration_full_cache = time.time() - start_full_cache\n",
    "\n",
    "memory_after_full_cache = process.memory_info().rss / 1024 / 1024\n",
    "memory_increase = memory_after_full_cache - memory_before_full_cache\n",
    "\n",
    "print(f\"Full cache materialization: {duration_full_cache:.2f}s\")\n",
    "print(f\"Total items cached: {full_count:,}\")\n",
    "print(f\"Memory after full cache: {memory_after_full_cache:.1f} MB\")\n",
    "print(f\"Memory increase: {memory_increase:.1f} MB\")\n",
    "\n",
    "# Now test if subsequent iterations are really fast\n",
    "print(\"\\nTesting cached performance...\")\n",
    "start_cached = time.time()\n",
    "cached_count = sum(1 for _ in cached_dataset)\n",
    "duration_cached = time.time() - start_cached\n",
    "print(f\"Iteration over fully cached dataset: {duration_cached:.3f}s\")\n",
    "print(f\"Items: {cached_count:,}\")\n",
    "\n",
    "print(f\"\\n[-- Caching Conclusions --]\\n\")\n",
    "print(f\"1. .cache() call itself: {duration_cache:.3f}s (just creates cache object)\")\n",
    "print(f\"2. First iteration: {duration_first:.3f}s (materializes data into cache)\")\n",
    "print(f\"3. Subsequent iterations: {duration_cached:.3f}s (reads from cache)\")\n",
    "print(f\"4. Memory overhead: {memory_increase:.1f} MB for {full_count:,} items\")\n",
    "print(f\"5. Cache speedup: {duration_full_cache/duration_cached:.0f}x faster\")\n",
    "\n",
    "print(\"\\nSo caching does NOT materialize immediately - it's lazy until first use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
