{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39454b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow 2.19.0 imported silently (non-deterministic mode)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/scratch/edk202/word2gm-fast/notebooks')\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Clean TensorFlow import with complete silencing (non-deterministic by default)\n",
    "from src.word2gm_fast.utils import (\n",
    "    import_tensorflow_silently, \n",
    "    log_tf_to_file, \n",
    "    run_silent_subprocess\n",
    ")\n",
    "\n",
    "tf = import_tensorflow_silently(deterministic=False)\n",
    "print(f\"✅ TensorFlow {tf.__version__} imported silently (non-deterministic mode)\")\n",
    "\n",
    "# Import data pipeline modules\n",
    "from src.word2gm_fast.dataprep.corpus_to_dataset import make_dataset\n",
    "from src.word2gm_fast.dataprep.index_vocab import make_vocab\n",
    "from src.word2gm_fast.dataprep.dataset_to_triplets import build_skipgram_triplets\n",
    "from src.word2gm_fast.dataprep.tfrecord_io import (\n",
    "    save_pipeline_artifacts,\n",
    "    load_pipeline_artifacts,\n",
    "    write_vocab_to_tfrecord,\n",
    "    load_vocab_from_tfrecord\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0779c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ test_corpus_to_dataset: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Run tests with TensorFlow silencing\n",
    "result = run_silent_subprocess(\n",
    "    ['python', '-m', 'unittest', '-b', 'tests.test_corpus_to_dataset'], \n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ test_corpus_to_dataset: PASSED\")\n",
    "else:\n",
    "    print(\"❌ test_corpus_to_dataset: FAILED\")\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb216b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ test_index_vocab: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Run tests with TensorFlow silencing\n",
    "result = run_silent_subprocess(\n",
    "    ['python', '-m', 'unittest', '-b', 'tests.test_index_vocab'], \n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ test_index_vocab: PASSED\")\n",
    "else:\n",
    "    print(\"❌ test_index_vocab: FAILED\")\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499f264d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ test_dataset_to_triplets: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Run tests with TensorFlow silencing (non-deterministic mode)\n",
    "result = run_silent_subprocess(\n",
    "    ['python', '-m', 'unittest', '-b', 'tests.test_dataset_to_triplets'], \n",
    "    deterministic=False,\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ test_dataset_to_triplets: PASSED\")\n",
    "else:\n",
    "    print(\"❌ test_dataset_to_triplets: FAILED\")\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233685ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS FILE:  /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1800.txt \n",
      "\n",
      "[--    Benchmarks    --]\n",
      "\n",
      "Step                                 Duration             Quantity                 Rate\n",
      "---------------------------------------------------------------------------------------\n",
      "Corpus loading and filtering (Lazy)    0.42 s             31.49 MB           75.19 MB/s\n",
      "Vocabulary creation                   30.08 s             31.49 MB            1.05 MB/s\n",
      "Triplet generation (Lazy)              0.17 s             31.49 MB          182.72 MB/s\n",
      "\n",
      "[--   Sample Lines   --]\n",
      "\n",
      "UNK one chance UNK UNK\n",
      "UNK one pair front UNK\n",
      "UNK one brave action UNK\n",
      "UNK one another UNK since\n",
      "UNK one could imagine UNK\n",
      "\n",
      "[--    Test Words    --]\n",
      "\n",
      "Word                   ID\n",
      "-------------------------\n",
      "UNK                     0\n",
      "man                 11276\n",
      "king                10355\n",
      "nonexistentword         0\n",
      "\n",
      "[--  Sample Triplets  --]\n",
      "\n",
      "Center   Center Word  Positive Pos Word     Negative Neg Word    \n",
      "---------------------------------------------------------------------------\n",
      "[--    Benchmarks    --]\n",
      "\n",
      "Step                                 Duration             Quantity                 Rate\n",
      "---------------------------------------------------------------------------------------\n",
      "Corpus loading and filtering (Lazy)    0.42 s             31.49 MB           75.19 MB/s\n",
      "Vocabulary creation                   30.08 s             31.49 MB            1.05 MB/s\n",
      "Triplet generation (Lazy)              0.17 s             31.49 MB          182.72 MB/s\n",
      "\n",
      "[--   Sample Lines   --]\n",
      "\n",
      "UNK one chance UNK UNK\n",
      "UNK one pair front UNK\n",
      "UNK one brave action UNK\n",
      "UNK one another UNK since\n",
      "UNK one could imagine UNK\n",
      "\n",
      "[--    Test Words    --]\n",
      "\n",
      "Word                   ID\n",
      "-------------------------\n",
      "UNK                     0\n",
      "man                 11276\n",
      "king                10355\n",
      "nonexistentword         0\n",
      "\n",
      "[--  Sample Triplets  --]\n",
      "\n",
      "Center   Center Word  Positive Pos Word     Negative Neg Word    \n",
      "---------------------------------------------------------------------------\n",
      "4150     could        5273     discover     9793     intended    \n",
      "4271     creature     11024    love         5353     disown      \n",
      "4631     dead         2095     body         8962     hounslow    \n",
      "1033     arm          12800    one          6190     enervate    \n",
      "4150     could        5437     distinctly   18552    toad        \n",
      "4150     could        5273     discover     9793     intended    \n",
      "4271     creature     11024    love         5353     disown      \n",
      "4631     dead         2095     body         8962     hounslow    \n",
      "1033     arm          12800    one          6190     enervate    \n",
      "4150     could        5437     distinctly   18552    toad        \n"
     ]
    }
   ],
   "source": [
    "# Corpus file information\n",
    "corpus_file = \"1800.txt\"\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "corpus_path = os.path.join(corpus_dir, corpus_file)\n",
    "file_size = os.path.getsize(corpus_path) / 1024 / 1024\n",
    "print(\"CORPUS FILE: \", corpus_path, \"\\n\")\n",
    "\n",
    "# Load and filter the corpus\n",
    "start = time.time()\n",
    "dataset, _ = make_dataset(corpus_path)\n",
    "duration_load = time.time() - start\n",
    "rate_load = file_size / duration_load\n",
    "\n",
    "# Cache the dataset\n",
    "dataset = dataset.cache()\n",
    "\n",
    "# Build the vocabulary from the dataset\n",
    "start = time.time()\n",
    "vocab_table = make_vocab(dataset)\n",
    "duration_vocab = time.time() - start\n",
    "rate_vocab = file_size / duration_vocab\n",
    "\n",
    "# Make triplets from the dataset and vocabulary table\n",
    "start = time.time()\n",
    "triplets_ds = build_skipgram_triplets(dataset, vocab_table)\n",
    "duration_triplets = time.time() - start\n",
    "rate_triplets = file_size / duration_triplets\n",
    "\n",
    "# Benchmarking output\n",
    "print(\"[--    Benchmarks    --]\\n\")\n",
    "print(f\"{'Step':<35}{'Duration':>10}{'Quantity':>21}{'Rate':>21}\")\n",
    "print(\"-\" * 87)\n",
    "print(f\"{'Corpus loading and filtering (Lazy)':<35}{duration_load:8,.2f}{'s':>2}{file_size:18,.2f}{'MB':>3}{rate_load:16,.2f}{'MB/s':>5}\")\n",
    "print(f\"{'Vocabulary creation':<35}{duration_vocab:8,.2f}{'s':>2}{file_size:18,.2f}{'MB':>3}{rate_vocab:16,.2f}{'MB/s':>5}\")\n",
    "print(f\"{'Triplet generation (Lazy)':<35}{duration_triplets:8,.2f}{'s':>2}{file_size:18,.2f}{'MB':>3}{rate_triplets:16,.2f}{'MB/s':>5}\")\n",
    "\n",
    "# Create reverse lookup from vocab table once at the beginning\n",
    "vocab_export = vocab_table.export()\n",
    "vocab_keys = vocab_export[0].numpy()\n",
    "vocab_values = vocab_export[1].numpy()\n",
    "index_to_word = {idx: word.decode('utf-8') for word, idx in zip(vocab_keys, vocab_values)}\n",
    "\n",
    "# Show sample lines\n",
    "print(\"\\n[--   Sample Lines   --]\\n\")\n",
    "sample_lines = list(dataset.shuffle(1000, seed=42).take(5).as_numpy_iterator())\n",
    "for line_bytes in sample_lines:\n",
    "    print(line_bytes.decode(\"utf-8\"))\n",
    "\n",
    "# Test the vocab table with example words\n",
    "print(\"\\n[--    Test Words    --]\\n\")\n",
    "test_words = [\"UNK\", \"man\", \"king\", \"nonexistentword\"]\n",
    "ids = vocab_table.lookup(tf.constant(test_words)).numpy()\n",
    "print(f\"{'Word':<18} {'ID':>6}\")\n",
    "print(\"-\" * 25)\n",
    "for word, idx in zip(test_words, ids):\n",
    "    print(f\"{word:<18} {idx:>6}\")\n",
    "\n",
    "# Show sample triplets\n",
    "print(\"\\n[--  Sample Triplets  --]\\n\")\n",
    "print(f\"{'Center':<8} {'Center Word':<12} {'Positive':<8} {'Pos Word':<12} {'Negative':<8} {'Neg Word':<12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "sample_triplets = list(triplets_ds.shuffle(1000, seed=123).take(5).as_numpy_iterator())\n",
    "for triplet in sample_triplets:\n",
    "    center, positive, negative = triplet\n",
    "    center_word = index_to_word.get(center, f\"ID_{center}\")\n",
    "    pos_word = index_to_word.get(positive, f\"ID_{positive}\")\n",
    "    neg_word = index_to_word.get(negative, f\"ID_{negative}\")\n",
    "    print(f\"{center:<8} {center_word:<12} {positive:<8} {pos_word:<12} {negative:<8} {neg_word:<12}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a60669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-- TFRecord Demo (Fast Pipeline Serialization) --]\n",
      "\n",
      "Saving pipeline artifacts...\n",
      "Saving pipeline artifacts to: ./pipeline_tfrecords\n",
      "Writing vocabulary TFRecord to: ./pipeline_tfrecords/vocab.tfrecord.gz\n",
      "Vocabulary write complete. Words written: 33,668\n",
      "Write time: 0.34 sec\n",
      "Writing TFRecord to: ./pipeline_tfrecords/triplets.tfrecord.gz\n",
      "Vocabulary write complete. Words written: 33,668\n",
      "Write time: 0.34 sec\n",
      "Writing TFRecord to: ./pipeline_tfrecords/triplets.tfrecord.gz\n",
      "Write complete. Triplets written: 3,249,693\n",
      "Write time: 399.83 sec\n",
      "All artifacts saved successfully!\n",
      "Saved to: ./pipeline_tfrecords\n",
      "Write complete. Triplets written: 3,249,693\n",
      "Write time: 399.83 sec\n",
      "All artifacts saved successfully!\n",
      "Saved to: ./pipeline_tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 14:36:31.250432: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate TFRecord functionality (optional - commented out for speed)\n",
    "print(\"\\n[-- TFRecord Demo (Fast Pipeline Serialization) --]\\n\")\n",
    "\n",
    "# Example usage (uncomment to actually save/load):\n",
    "output_dir = \"./pipeline_tfrecords\"\n",
    "\n",
    "# Save all pipeline artifacts to TFRecords (one-time cost)\n",
    "print(\"Saving pipeline artifacts...\")\n",
    "artifacts = save_pipeline_artifacts(\n",
    "    dataset, vocab_table, triplets_ds, \n",
    "    output_dir=output_dir, \n",
    "    compress=True\n",
    ")\n",
    "print(f\"Saved to: {artifacts['output_dir']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66e197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
