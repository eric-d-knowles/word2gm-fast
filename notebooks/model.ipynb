{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39454b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TensorFlow 2.19.0 imported silently (non-deterministic mode)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/scratch/edk202/word2gm-fast/notebooks')\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Clean TensorFlow import with complete silencing (non-deterministic by default)\n",
    "from src.word2gm_fast.utils import (\n",
    "    import_tensorflow_silently, \n",
    "    log_tf_to_file, \n",
    "    run_silent_subprocess\n",
    ")\n",
    "\n",
    "tf = import_tensorflow_silently(deterministic=False)\n",
    "print(f\"‚úÖ TensorFlow {tf.__version__} imported silently (non-deterministic mode)\")\n",
    "\n",
    "# Import data pipeline modules\n",
    "from src.word2gm_fast.dataprep.corpus_to_dataset import make_dataset\n",
    "from src.word2gm_fast.dataprep.index_vocab import make_vocab\n",
    "from src.word2gm_fast.dataprep.dataset_to_triplets import build_skipgram_triplets\n",
    "from src.word2gm_fast.dataprep.tfrecord_io import (\n",
    "    save_pipeline_artifacts,\n",
    "    load_pipeline_artifacts,\n",
    "    write_triplets_to_tfrecord,\n",
    "    load_triplets_from_tfrecord,\n",
    "    write_vocab_to_tfrecord,\n",
    "    load_vocab_from_tfrecord\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0779c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ test_corpus_to_dataset: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Run tests with TensorFlow silencing\n",
    "result = run_silent_subprocess(\n",
    "    ['python', '-m', 'unittest', '-b', 'tests.test_corpus_to_dataset'], \n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ test_corpus_to_dataset: PASSED\")\n",
    "else:\n",
    "    print(\"‚ùå test_corpus_to_dataset: FAILED\")\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb216b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ test_index_vocab: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Run tests with TensorFlow silencing\n",
    "result = run_silent_subprocess(\n",
    "    ['python', '-m', 'unittest', '-b', 'tests.test_index_vocab'], \n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ test_index_vocab: PASSED\")\n",
    "else:\n",
    "    print(\"‚ùå test_index_vocab: FAILED\")\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499f264d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ test_dataset_to_triplets: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Run tests with TensorFlow silencing (non-deterministic mode)\n",
    "result = run_silent_subprocess(\n",
    "    ['python', '-m', 'unittest', '-b', 'tests.test_dataset_to_triplets'], \n",
    "    deterministic=False,\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ test_dataset_to_triplets: PASSED\")\n",
    "else:\n",
    "    print(\"‚ùå test_dataset_to_triplets: FAILED\")\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233685ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS FILE:  /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1800.txt \n",
      "\n",
      "[--    Benchmarks    --]\n",
      "\n",
      "Step                                 Duration             Quantity                 Rate\n",
      "---------------------------------------------------------------------------------------\n",
      "Corpus loading and filtering (Lazy)    0.13 s             31.49 MB          240.73 MB/s\n",
      "Vocabulary creation                   18.71 s             31.49 MB            1.68 MB/s\n",
      "Triplet generation (Lazy)              0.16 s             31.49 MB          199.70 MB/s\n",
      "\n",
      "[--   Sample Lines   --]\n",
      "\n",
      "UNK one chance UNK UNK\n",
      "UNK one pair front UNK\n",
      "UNK one brave action UNK\n",
      "UNK one another UNK since\n",
      "UNK one could imagine UNK\n",
      "\n",
      "[--    Test Words    --]\n",
      "\n",
      "Word                   ID\n",
      "-------------------------\n",
      "UNK                     0\n",
      "man                 11276\n",
      "king                10355\n",
      "nonexistentword         0\n",
      "\n",
      "[--  Sample Triplets  --]\n",
      "\n",
      "Center   Center Word  Positive Pos Word     Negative Neg Word    \n",
      "---------------------------------------------------------------------------\n",
      "[--    Benchmarks    --]\n",
      "\n",
      "Step                                 Duration             Quantity                 Rate\n",
      "---------------------------------------------------------------------------------------\n",
      "Corpus loading and filtering (Lazy)    0.13 s             31.49 MB          240.73 MB/s\n",
      "Vocabulary creation                   18.71 s             31.49 MB            1.68 MB/s\n",
      "Triplet generation (Lazy)              0.16 s             31.49 MB          199.70 MB/s\n",
      "\n",
      "[--   Sample Lines   --]\n",
      "\n",
      "UNK one chance UNK UNK\n",
      "UNK one pair front UNK\n",
      "UNK one brave action UNK\n",
      "UNK one another UNK since\n",
      "UNK one could imagine UNK\n",
      "\n",
      "[--    Test Words    --]\n",
      "\n",
      "Word                   ID\n",
      "-------------------------\n",
      "UNK                     0\n",
      "man                 11276\n",
      "king                10355\n",
      "nonexistentword         0\n",
      "\n",
      "[--  Sample Triplets  --]\n",
      "\n",
      "Center   Center Word  Positive Pos Word     Negative Neg Word    \n",
      "---------------------------------------------------------------------------\n",
      "4150     could        5273     discover     9793     intended    \n",
      "4271     creature     11024    love         5353     disown      \n",
      "4631     dead         2095     body         8962     hounslow    \n",
      "1033     arm          12800    one          6190     enervate    \n",
      "4150     could        5437     distinctly   18552    toad        \n",
      "4150     could        5273     discover     9793     intended    \n",
      "4271     creature     11024    love         5353     disown      \n",
      "4631     dead         2095     body         8962     hounslow    \n",
      "1033     arm          12800    one          6190     enervate    \n",
      "4150     could        5437     distinctly   18552    toad        \n"
     ]
    }
   ],
   "source": [
    "# Corpus file information\n",
    "corpus_file = \"1800.txt\"\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "corpus_path = os.path.join(corpus_dir, corpus_file)\n",
    "file_size = os.path.getsize(corpus_path) / 1024 / 1024\n",
    "print(\"CORPUS FILE: \", corpus_path, \"\\n\")\n",
    "\n",
    "# Load and filter the corpus\n",
    "start = time.time()\n",
    "dataset, _ = make_dataset(corpus_path)\n",
    "duration_load = time.time() - start\n",
    "rate_load = file_size / duration_load\n",
    "\n",
    "# Cache the dataset\n",
    "dataset = dataset.cache()\n",
    "\n",
    "# Build the vocabulary from the dataset\n",
    "start = time.time()\n",
    "vocab_table = make_vocab(dataset)\n",
    "duration_vocab = time.time() - start\n",
    "rate_vocab = file_size / duration_vocab\n",
    "\n",
    "# Make triplets from the dataset and vocabulary table\n",
    "start = time.time()\n",
    "triplets_ds = build_skipgram_triplets(dataset, vocab_table)\n",
    "duration_triplets = time.time() - start\n",
    "rate_triplets = file_size / duration_triplets\n",
    "\n",
    "# Benchmarking output\n",
    "print(\"[--    Benchmarks    --]\\n\")\n",
    "print(f\"{'Step':<35}{'Duration':>10}{'Quantity':>21}{'Rate':>21}\")\n",
    "print(\"-\" * 87)\n",
    "print(f\"{'Corpus loading and filtering (Lazy)':<35}{duration_load:8,.2f}{'s':>2}{file_size:18,.2f}{'MB':>3}{rate_load:16,.2f}{'MB/s':>5}\")\n",
    "print(f\"{'Vocabulary creation':<35}{duration_vocab:8,.2f}{'s':>2}{file_size:18,.2f}{'MB':>3}{rate_vocab:16,.2f}{'MB/s':>5}\")\n",
    "print(f\"{'Triplet generation (Lazy)':<35}{duration_triplets:8,.2f}{'s':>2}{file_size:18,.2f}{'MB':>3}{rate_triplets:16,.2f}{'MB/s':>5}\")\n",
    "\n",
    "# Create reverse lookup from vocab table once at the beginning\n",
    "vocab_export = vocab_table.export()\n",
    "vocab_keys = vocab_export[0].numpy()\n",
    "vocab_values = vocab_export[1].numpy()\n",
    "index_to_word = {idx: word.decode('utf-8') for word, idx in zip(vocab_keys, vocab_values)}\n",
    "\n",
    "# Show sample lines\n",
    "print(\"\\n[--   Sample Lines   --]\\n\")\n",
    "sample_lines = list(dataset.shuffle(1000, seed=42).take(5).as_numpy_iterator())\n",
    "for line_bytes in sample_lines:\n",
    "    print(line_bytes.decode(\"utf-8\"))\n",
    "\n",
    "# Test the vocab table with example words\n",
    "print(\"\\n[--    Test Words    --]\\n\")\n",
    "test_words = [\"UNK\", \"man\", \"king\", \"nonexistentword\"]\n",
    "ids = vocab_table.lookup(tf.constant(test_words)).numpy()\n",
    "print(f\"{'Word':<18} {'ID':>6}\")\n",
    "print(\"-\" * 25)\n",
    "for word, idx in zip(test_words, ids):\n",
    "    print(f\"{word:<18} {idx:>6}\")\n",
    "\n",
    "# Show sample triplets\n",
    "print(\"\\n[--  Sample Triplets  --]\\n\")\n",
    "print(f\"{'Center':<8} {'Center Word':<12} {'Positive':<8} {'Pos Word':<12} {'Negative':<8} {'Neg Word':<12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "sample_triplets = list(triplets_ds.shuffle(1000, seed=123).take(5).as_numpy_iterator())\n",
    "for triplet in sample_triplets:\n",
    "    center, positive, negative = triplet\n",
    "    center_word = index_to_word.get(center, f\"ID_{center}\")\n",
    "    pos_word = index_to_word.get(positive, f\"ID_{positive}\")\n",
    "    neg_word = index_to_word.get(negative, f\"ID_{negative}\")\n",
    "    print(f\"{center:<8} {center_word:<12} {positive:<8} {pos_word:<12} {negative:<8} {neg_word:<12}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a60669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-- TFRecord Demo (Fast Pipeline Serialization) --]\n",
      "\n",
      "Saving pipeline artifacts...\n",
      "Saving pipeline artifacts to: ./pipeline_tfrecords\n",
      "Writing vocabulary TFRecord to: ./pipeline_tfrecords/vocab.tfrecord.gz\n",
      "Vocabulary write complete. Words written: 20,685\n",
      "Write time: 0.21 sec\n",
      "Writing TFRecord to: ./pipeline_tfrecords/triplets.tfrecord.gz\n",
      "Vocabulary write complete. Words written: 20,685\n",
      "Write time: 0.21 sec\n",
      "Writing TFRecord to: ./pipeline_tfrecords/triplets.tfrecord.gz\n",
      "Write complete. Triplets written: 794,296\n",
      "Write time: 202.15 sec\n",
      "All artifacts saved successfully!\n",
      "Saved to: ./pipeline_tfrecords\n",
      "Write complete. Triplets written: 794,296\n",
      "Write time: 202.15 sec\n",
      "All artifacts saved successfully!\n",
      "Saved to: ./pipeline_tfrecords\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate TFRecord functionality\n",
    "print(\"\\n[-- TFRecord Demo (Fast Pipeline Serialization) --]\\n\")\n",
    "\n",
    "# Example usage:\n",
    "output_dir = \"./pipeline_tfrecords\"\n",
    "\n",
    "# Save all pipeline artifacts to TFRecords (one-time cost)\n",
    "print(\"Saving pipeline artifacts...\")\n",
    "artifacts = save_pipeline_artifacts(\n",
    "    dataset, vocab_table, triplets_ds, \n",
    "    output_dir=output_dir, \n",
    "    compress=True\n",
    ")\n",
    "print(f\"Saved to: {artifacts['output_dir']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a884ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-- TFRecord Loading Speed Demo --]\n",
      "\n",
      "TFRecord files found! Benchmarking load times...\n",
      "\n",
      "Loading vocabulary from TFRecord...\n",
      "Loading vocabulary TFRecord from: ./pipeline_tfrecords/vocab.tfrecord.gz\n",
      "Vocabulary loaded. Size: 20,685 words\n",
      "Load time: 2.70 sec\n",
      "Loading triplets dataset from TFRecord...\n",
      "Loading TFRecord from: ./pipeline_tfrecords/triplets.tfrecord.gz\n",
      "TFRecord loaded and parsed\n",
      "Load time (lazy initialization): 0.066 sec\n",
      "Verifying loaded data...\n",
      "Vocabulary loaded. Size: 20,685 words\n",
      "Load time: 2.70 sec\n",
      "Loading triplets dataset from TFRecord...\n",
      "Loading TFRecord from: ./pipeline_tfrecords/triplets.tfrecord.gz\n",
      "TFRecord loaded and parsed\n",
      "Load time (lazy initialization): 0.066 sec\n",
      "Verifying loaded data...\n",
      "\n",
      "[--  Loading Benchmarks  --]\n",
      "\n",
      "Operation                  Duration               Rate/Notes\n",
      "------------------------------------------------------------\n",
      "Vocabulary loading           2.70 s               ~instant\n",
      "Triplets dataset loading     0.07 s       lazy - very fast\n",
      "Sample verification          0.14 s             5 triplets\n",
      "\n",
      "üí° Compare with original pipeline times:\n",
      "\n",
      "[--  Loading Benchmarks  --]\n",
      "\n",
      "Operation                  Duration               Rate/Notes\n",
      "------------------------------------------------------------\n",
      "Vocabulary loading           2.70 s               ~instant\n",
      "Triplets dataset loading     0.07 s       lazy - very fast\n",
      "Sample verification          0.14 s             5 triplets\n",
      "\n",
      "üí° Compare with original pipeline times:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'duration_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mSample verification\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mverification_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m8,.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33ms\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m5 triplets\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>23\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müí° Compare with original pipeline times:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚Ä¢ Original vocab creation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mduration_vocab\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚Ä¢ Original triplet generation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration_triplets\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚Ä¢ TFRecord vocab loading: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_load_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'duration_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# Demonstrate TFRecord loading speed\n",
    "print(\"\\n[-- TFRecord Loading Speed Demo --]\\n\")\n",
    "\n",
    "output_dir = \"./pipeline_tfrecords\"\n",
    "\n",
    "# Check if TFRecords exist\n",
    "import os\n",
    "vocab_file = os.path.join(output_dir, \"vocab.tfrecord.gz\")\n",
    "triplets_file = os.path.join(output_dir, \"triplets.tfrecord.gz\")\n",
    "\n",
    "if os.path.exists(vocab_file) and os.path.exists(triplets_file):\n",
    "    print(\"TFRecord files found! Benchmarking load times...\\n\")\n",
    "    \n",
    "    # Time vocabulary loading\n",
    "    print(\"Loading vocabulary from TFRecord...\")\n",
    "    start = time.time()\n",
    "    loaded_vocab_table = load_vocab_from_tfrecord(vocab_file)\n",
    "    vocab_load_time = time.time() - start\n",
    "    \n",
    "    # Time triplet dataset loading\n",
    "    print(\"Loading triplets dataset from TFRecord...\")\n",
    "    start = time.time()\n",
    "    loaded_triplets_ds = load_triplets_from_tfrecord(triplets_file)\n",
    "    triplets_load_time = time.time() - start\n",
    "    \n",
    "    # Quick verification by taking a sample\n",
    "    print(\"Verifying loaded data...\")\n",
    "    start = time.time()\n",
    "    sample_triplets = list(loaded_triplets_ds.take(5).as_numpy_iterator())\n",
    "    verification_time = time.time() - start\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n[--  Loading Benchmarks  --]\\n\")\n",
    "    print(f\"{'Operation':<25}{'Duration':>10}{'Rate/Notes':>25}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Vocabulary loading':<25}{vocab_load_time:8,.2f}{'s':>2}{'~instant':>23}\")\n",
    "    print(f\"{'Triplets dataset loading':<25}{triplets_load_time:8,.2f}{'s':>2}{'lazy - very fast':>23}\")\n",
    "    print(f\"{'Sample verification':<25}{verification_time:8,.2f}{'s':>2}{'5 triplets':>23}\")\n",
    "    \n",
    "    # Check if original timing variables are available for comparison\n",
    "    try:\n",
    "        print(f\"\\nüí° Compare with original pipeline times:\")\n",
    "        print(f\"   ‚Ä¢ Original vocab creation: {duration_vocab:.2f}s\")\n",
    "        print(f\"   ‚Ä¢ Original triplet generation: {duration_triplets:.2f}s\")\n",
    "        print(f\"   ‚Ä¢ TFRecord vocab loading: {vocab_load_time:.2f}s\")\n",
    "        print(f\"   ‚Ä¢ TFRecord triplet loading: {triplets_load_time:.2f}s\")\n",
    "        \n",
    "        # Calculate speedup\n",
    "        vocab_speedup = duration_vocab / vocab_load_time if vocab_load_time > 0 else float('inf')\n",
    "        triplet_speedup = duration_triplets / triplets_load_time if triplets_load_time > 0 else float('inf')\n",
    "        \n",
    "        print(f\"\\nüöÄ Speedup factors:\")\n",
    "        print(f\"   ‚Ä¢ Vocabulary: {vocab_speedup:.1f}x faster\")\n",
    "        print(f\"   ‚Ä¢ Triplets: {triplet_speedup:.1f}x faster\")\n",
    "        \n",
    "    except NameError:\n",
    "        print(f\"\\nüí° Original pipeline timing not available.\")\n",
    "        print(f\"   Run the main pipeline cell first for speed comparison.\")\n",
    "        print(f\"   Current TFRecord loading times:\")\n",
    "        print(f\"   ‚Ä¢ Vocab loading: {vocab_load_time:.2f}s\")\n",
    "        print(f\"   ‚Ä¢ Triplet loading: {triplets_load_time:.2f}s\")\n",
    "    \n",
    "    # Verify data integrity\n",
    "    print(f\"\\n‚úÖ Data verification:\")\n",
    "    print(f\"   ‚Ä¢ Loaded vocab size: {loaded_vocab_table.size().numpy():,}\")\n",
    "    try:\n",
    "        print(f\"   ‚Ä¢ Original vocab size: {vocab_table.size().numpy():,}\")\n",
    "    except NameError:\n",
    "        print(f\"   ‚Ä¢ Original vocab size: (not available - run main pipeline first)\")\n",
    "    print(f\"   ‚Ä¢ Sample triplets loaded: {len(sample_triplets)}\")\n",
    "    \n",
    "    # Show sample loaded triplet\n",
    "    if sample_triplets:\n",
    "        triplet = sample_triplets[0]\n",
    "        center, positive, negative = triplet\n",
    "        print(f\"   ‚Ä¢ First loaded triplet: ({center}, {positive}, {negative})\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå TFRecord files not found. Run the save demo first!\")\n",
    "    print(f\"   Looking for: {vocab_file}\")\n",
    "    print(f\"   Looking for: {triplets_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66e197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
