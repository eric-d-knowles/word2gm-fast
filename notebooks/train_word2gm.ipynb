{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c5f9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>Autoreload enabled</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Project root: /scratch/edk202/word2gm-fast\n",
       "TensorFlow version: 2.19.0\n",
       "Device mode: GPU-enabled</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Training environment ready!</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set project root directory and add `src` to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import the notebook setup utilities\n",
    "from word2gm_fast.utils.notebook_setup import setup_training_notebook, enable_autoreload, run_silent_subprocess\n",
    "\n",
    "# Enable autoreload for development\n",
    "enable_autoreload()\n",
    "\n",
    "# Set up environment (deterministic=True for reproducibility)\n",
    "env = setup_training_notebook(project_root=PROJECT_ROOT)\n",
    "\n",
    "# Extract commonly used modules for convenience\n",
    "tf = env['tensorflow']\n",
    "np = env['numpy']\n",
    "pd = env['pandas']\n",
    "print_resource_summary = env['print_resource_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6e18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust diagnostic: print all model and optimizer variable names and shapes ---\n",
    "import importlib\n",
    "import word2gm_fast.training.train_loop as train_loop_mod\n",
    "import word2gm_fast.training.notebook_training as notebook_training_mod\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "importlib.reload(train_loop_mod)\n",
    "importlib.reload(notebook_training_mod)\n",
    "\n",
    "def diagnostic_train_one_epoch(model, optimizer, dataset, summary_writer=None, epoch=0):\n",
    "    print(\"\\n=== DIAGNOSTIC: train_one_epoch called ===\")\n",
    "    print(f\"Model id: {id(model)}  Optimizer id: {id(optimizer)}\")\n",
    "    print(\"Model trainable variables:\")\n",
    "    for v in model.trainable_variables:\n",
    "        print(f\"  {v.name} shape={v.shape} id={id(v)}\")\n",
    "    print(\"Optimizer variables (including slots):\")\n",
    "    for v in optimizer.variables:\n",
    "        print(f\"  {v.name} shape={v.shape} id={id(v)}\")\n",
    "    for batch in dataset.take(1):\n",
    "        w, p, n = batch\n",
    "        print(\"Batch indices (first 10):\", w.numpy()[:10], p.numpy()[:10], n.numpy()[:10])\n",
    "        print(\"Batch max index:\", max(w.numpy().max(), p.numpy().max(), n.numpy().max()))\n",
    "        try:\n",
    "            loss = train_loop_mod.train_step(model, optimizer, w, p, n)\n",
    "            print(\"train_step loss:\", loss.numpy())\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(\"Exception during train_step:\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        break\n",
    "    print(\"=== END DIAGNOSTIC ===\\n\")\n",
    "    return 0.0\n",
    "\n",
    "train_loop_mod.train_one_epoch = diagnostic_train_one_epoch\n",
    "notebook_training_mod.train_one_epoch = diagnostic_train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a9987c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>SYSTEM RESOURCE SUMMARY\n",
       "============================================================\n",
       "Hostname: cm048.hpc.nyu.edu\n",
       "\n",
       "Job Allocation:\n",
       "   CPUs: 14\n",
       "   Memory: 125.0 GB\n",
       "   Requested partitions: short,cs,cm,cpu_a100_2,cpu_a100_1,cpu_gpu\n",
       "   Running on: SSH failed: Host key verification failed.\n",
       "   Job ID: 63043526\n",
       "   Node list: cm048\n",
       "\n",
       "GPU Information:\n",
       "   Error: NVML Shared Library Not Found\n",
       "\n",
       "TensorFlow GPU Detection:\n",
       "   TensorFlow detects 0 GPU(s)\n",
       "   Built with CUDA: True\n",
       "============================================================</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51623d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from word2gm_fast.training.notebook_training import run_notebook_training\n",
    "from word2gm_fast.utils.tfrecord_io import load_pipeline_artifacts\n",
    "\n",
    "# Define paths for your small corpus aartifacts and output\n",
    "artifacts_dir = '/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1750_artifacts'\n",
    "output_dir = '/scratch/edk202/word2gm-fast/output/test_small_corpus'\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d21be501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>Loading pipeline artifacts from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1750_artifacts</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading vocabulary TFRecord from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1750_artifacts/vocab.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading triplet TFRecord from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1750_artifacts/triplets.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Triplet TFRecord loaded and parsed</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>All artifacts loaded successfully!</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the dataset pipeline: cache → shuffle → batch → prefetch\n",
    "\n",
    "# Set the path to your artifacts directory (already defined as artifacts_dir)\n",
    "artifacts = load_pipeline_artifacts(artifacts_dir)\n",
    "\n",
    "# Unpack the loaded artifacts\n",
    "vocab_table = artifacts['vocab_table']\n",
    "triplets_ds = artifacts['triplets_ds']\n",
    "vocab_size = artifacts['vocab_size']\n",
    "\n",
    "# Build the dataset pipeline: cache -> shuffle -> batch -> prefetch\n",
    "triplets_ds = triplets_ds.cache()\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 10\n",
    "triplets_ds = triplets_ds.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "triplets_ds = triplets_ds.batch(BATCH_SIZE)\n",
    "triplets_ds = triplets_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e04ed9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 5143\n",
      "max index in triplets: 5142\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab_size:\", vocab_size)\n",
    "max_idx = -1\n",
    "for batch in triplets_ds.unbatch():\n",
    "    w, p, n = batch\n",
    "    max_idx = max(max_idx, w.numpy(), p.numpy(), n.numpy())\n",
    "print(\"max index in triplets:\", max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_notebook_training(\n",
    "    training_dataset=triplets_ds,\n",
    "    save_path=output_dir,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=30,\n",
    "    num_mixtures=2,\n",
    "    spherical=True,\n",
    "    learning_rate=0.01,\n",
    "    epochs=5,\n",
    "    adagrad=True,\n",
    "    normclip=True,\n",
    "    norm_cap=1.0,\n",
    "    lower_sig=0.01,\n",
    "    upper_sig=2.0,\n",
    "    var_scale=10.0,\n",
    "    loss_epsilon=1e-3,\n",
    "    wout=False,\n",
    "    tensorboard_log_path=os.path.join(output_dir, 'tensorboard'),\n",
    "    monitor_interval=2,\n",
    "    profile=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d9c417c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset pipeline structure (oldest to newest):\n",
      "  [0] TFRecordDatasetV2\n",
      "  [1] _ParallelMapDataset\n",
      "  [2] _ParallelMapDataset\n",
      "  [3] CacheDataset\n",
      "  [4] _ShuffleDataset\n",
      "  [5] _BatchDataset\n",
      "  [6] _PrefetchDataset\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<pre>\n",
       "Starting Word2GM training</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Writing TensorBoard logs to /scratch/edk202/word2gm-fast/output/test_small_corpus/tensorboard</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer slot variables initialized.\n",
      "All optimizer variables and shapes after init:\n",
      "  iteration: shape ()\n",
      "  learning_rate: shape ()\n",
      "  word2gm_model_mus_accumulator: shape (5143, 2, 30)\n",
      "  word2gm_model_logsigmas_accumulator: shape (5143, 2, 1)\n",
      "  word2gm_model_mixture_accumulator: shape (5143, 2)\n",
      "\n",
      "[Diagnostics] Model trainable variables:\n",
      "  mus: shape (5143, 2, 30)\n",
      "  logsigmas: shape (5143, 2, 1)\n",
      "  mixture: shape (5143, 2)\n",
      "\n",
      "[Diagnostics] Optimizer variables:\n",
      "  iteration: shape ()\n",
      "  learning_rate: shape ()\n",
      "  word2gm_model_mus_accumulator: shape (5143, 2, 30)\n",
      "  word2gm_model_logsigmas_accumulator: shape (5143, 2, 1)\n",
      "  word2gm_model_mixture_accumulator: shape (5143, 2)\n",
      "\n",
      "Training model vocab_size: 5143\n",
      "model.mus.shape: (5143, 2, 30)\n",
      "model.logsigmas.shape: (5143, 2, 1)\n",
      "model.mixture.shape: (5143, 2)\n",
      "id(model): 22755893395680\n",
      "id(optimizer): 22755893305824\n",
      "Batch min/max indices: 29 5108 38 5135 7 5140\n",
      "All index and shape checks passed at training start.\n",
      "\n",
      "=== DIAGNOSTIC: train_one_epoch called ===\n",
      "Model id: 22755893395680  Optimizer id: 22755893305824\n",
      "Model trainable variables:\n",
      "  mus shape=(5143, 2, 30) id=22755893305920\n",
      "  logsigmas shape=(5143, 2, 1) id=22755893306880\n",
      "  mixture shape=(5143, 2) id=22755893302176\n",
      "Optimizer variables (including slots):\n",
      "  iteration shape=() id=22755893310864\n",
      "  learning_rate shape=() id=22755893302464\n",
      "  word2gm_model_mus_accumulator shape=(5143, 2, 30) id=22755893397840\n",
      "  word2gm_model_logsigmas_accumulator shape=(5143, 2, 1) id=22755893397168\n",
      "  word2gm_model_mixture_accumulator shape=(5143, 2) id=22755893398224\n",
      "Batch indices (first 10): [4613 3113 2794 2223 4056 4572 5045 3113 4971 4868] [3064 2028 2028 2486 3266 3164 1996 1996 2070 2028] [2462 2673 2344 1637 2822 3360 2952 1214 2158 4984]\n",
      "Batch max index: 5133\n",
      "[Diagnostics][train_step] Model trainable variables:\n",
      "  mus: shape (5143, 2, 30)\n",
      "  logsigmas: shape (5143, 2, 1)\n",
      "  mixture: shape (5143, 2)\n",
      "[Diagnostics][train_step] Optimizer variables:\n",
      "  iteration: shape ()\n",
      "  learning_rate: shape ()\n",
      "  word2gm_model_mus_accumulator: shape (5143, 2, 30)\n",
      "  word2gm_model_logsigmas_accumulator: shape (5143, 2, 1)\n",
      "  word2gm_model_mixture_accumulator: shape (5143, 2)\n",
      "\n",
      "Exception during train_step:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/state/partition1/job-63043526/ipykernel_2194746/3796053952.py\", line 25, in diagnostic_train_one_epoch\n",
      "    loss = train_loop_mod.train_step(model, optimizer, w, p, n)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/word2gm-fast/src/word2gm_fast/training/training_utils.py\", line 88, in train_step\n",
      "    optimizer.apply_gradients(grads_and_vars)\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 463, in apply_gradients\n",
      "    self.apply(grads, trainable_variables)\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 527, in apply\n",
      "    self._backend_apply_gradients(grads, trainable_variables)\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 593, in _backend_apply_gradients\n",
      "    self._backend_update_step(\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 120, in _backend_update_step\n",
      "    tf.__internal__.distribute.interim.maybe_merge_call(\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/distribute/merge_call_interim.py\", line 51, in maybe_merge_call\n",
      "    return fn(strategy, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 134, in _distributed_tf_update_step\n",
      "    distribution.extended.update(\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 3005, in update\n",
      "    return self._update(var, fn, args, kwargs, group)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 4075, in _update\n",
      "    return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 4081, in _update_non_slot\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 596, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 131, in apply_grad_to_update_var\n",
      "    return self.update_step(grad, var, learning_rate)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/optimizers/adagrad.py\", line 88, in update_step\n",
      "    ops.divide(\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/ops/numpy.py\", line 6290, in divide\n",
      "    return backend.numpy.divide(x1, x2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py\", line 770, in sparse_wrapper\n",
      "    return tf.cond(\n",
      "           ^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py\", line 758, in func_for_union_indices\n",
      "    ) = indexed_slices_union_indices_and_values(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py\", line 139, in indexed_slices_union_indices_and_values\n",
      "    x1_values_for_union_indices = tf.cond(\n",
      "                                  ^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py\", line 142, in <lambda>\n",
      "    lambda: values_for_union(\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py\", line 136, in values_for_union\n",
      "    return tf.gather(values_with_leading_zeros, to_union_indices)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[6] = 1421 is not in [0, 769) [Op:GatherV2] name: \n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[6] = 1421 is not in [0, 769) [Op:GatherV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Now call run_notebook_training for 1 epoch to trigger the error and print diagnostics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mnotebook_training_mod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_notebook_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtriplets_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_mixtures\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspherical\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43madagrad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormclip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnorm_cap\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlower_sig\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupper_sig\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvar_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_epsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensorboard_log_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtensorboard\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/word2gm-fast/src/word2gm_fast/training/notebook_training.py:135\u001b[39m, in \u001b[36mrun_notebook_training\u001b[39m\u001b[34m(training_dataset, save_path, vocab_size, embedding_size, num_mixtures, spherical, learning_rate, epochs, adagrad, normclip, norm_cap, lower_sig, upper_sig, wout, tensorboard_log_path, monitor_interval, profile, var_scale, loss_epsilon)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args.epochs_to_train):\n\u001b[32m    133\u001b[39m     epoch_start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     epoch_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43msummary_writer\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_writer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     epoch_time = time.time() - epoch_start\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# Print a single summary line per epoch\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mdiagnostic_train_one_epoch\u001b[39m\u001b[34m(model, optimizer, dataset, summary_writer, epoch)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBatch max index:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mmax\u001b[39m(w.numpy().max(), p.numpy().max(), n.numpy().max()))\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     loss = \u001b[43mtrain_loop_mod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtrain_step loss:\u001b[39m\u001b[33m\"\u001b[39m, loss.numpy())\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/word2gm-fast/src/word2gm_fast/training/training_utils.py:88\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(model, optimizer, word_idxs, pos_idxs, neg_idxs, normclip, norm_cap, lower_sig, upper_sig, wout)\u001b[39m\n\u001b[32m     86\u001b[39m grads_and_vars = [(g, v) \u001b[38;5;28;01mfor\u001b[39;00m g, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grads, variables) \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grads_and_vars:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m normclip:\n\u001b[32m     90\u001b[39m     clipped_mu = tf.clip_by_norm(model.mus, clip_norm=norm_cap, axes=[-\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:463\u001b[39m, in \u001b[36mBaseOptimizer.apply_gradients\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[32m    462\u001b[39m     grads, trainable_variables = \u001b[38;5;28mzip\u001b[39m(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterations\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:527\u001b[39m, in \u001b[36mBaseOptimizer.apply\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    524\u001b[39m     grads = [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g / scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[32m    526\u001b[39m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:593\u001b[39m, in \u001b[36mBaseOptimizer._backend_apply_gradients\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28mself\u001b[39m._apply_weight_decay(trainable_variables)\n\u001b[32m    592\u001b[39m     \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_ema:\n\u001b[32m    598\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_model_variables_moving_average(\n\u001b[32m    599\u001b[39m         \u001b[38;5;28mself\u001b[39m._trainable_variables\n\u001b[32m    600\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:120\u001b[39m, in \u001b[36mTFOptimizer._backend_update_step\u001b[39m\u001b[34m(self, grads, trainable_variables, learning_rate)\u001b[39m\n\u001b[32m    118\u001b[39m grads_and_vars = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[32m    119\u001b[39m grads_and_vars = \u001b[38;5;28mself\u001b[39m._all_reduce_sum_gradients(grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__internal__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistribute\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_distributed_tf_update_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[39m, in \u001b[36mmaybe_merge_call\u001b[39m\u001b[34m(fn, strategy, *args, **kwargs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \u001b[33;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m \u001b[33;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib.get_replica_context().merge_call(\n\u001b[32m     54\u001b[39m       fn, args=args, kwargs=kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:134\u001b[39m, in \u001b[36mTFOptimizer._distributed_tf_update_step\u001b[39m\u001b[34m(self, distribution, grads_and_vars, learning_rate)\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.update_step(grad, var, learning_rate)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[43mdistribution\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextended\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:3005\u001b[39m, in \u001b[36mStrategyExtendedV2.update\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   3002\u001b[39m   fn = autograph.tf_convert(\n\u001b[32m   3003\u001b[39m       fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3004\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3005\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3007\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._replica_ctx_update(\n\u001b[32m   3008\u001b[39m       var, fn, args=args, kwargs=kwargs, group=group)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4075\u001b[39m, in \u001b[36m_DefaultDistributionExtended._update\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   4072\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[32m   4073\u001b[39m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[32m   4074\u001b[39m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4075\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4081\u001b[39m, in \u001b[36m_DefaultDistributionExtended._update_non_slot\u001b[39m\u001b[34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[39m\n\u001b[32m   4077\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[32m   4078\u001b[39m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[32m   4079\u001b[39m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[32m   4080\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[32m-> \u001b[39m\u001b[32m4081\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4082\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[32m   4083\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[39m, in \u001b[36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    595\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:131\u001b[39m, in \u001b[36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001b[39m\u001b[34m(var, grad, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_grad_to_update_var\u001b[39m(var, grad, learning_rate):\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/optimizers/adagrad.py:88\u001b[39m, in \u001b[36mAdagrad.update_step\u001b[39m\u001b[34m(self, gradient, variable, learning_rate)\u001b[39m\n\u001b[32m     83\u001b[39m accumulator = \u001b[38;5;28mself\u001b[39m._accumulators[\u001b[38;5;28mself\u001b[39m._get_variable_index(variable)]\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m.assign_add(accumulator, ops.square(gradient))\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m.assign_sub(\n\u001b[32m     87\u001b[39m     variable,\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdivide\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccumulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     92\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/ops/numpy.py:6290\u001b[39m, in \u001b[36mdivide\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m   6288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[32m   6289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Divide().symbolic_call(x1, x2)\n\u001b[32m-> \u001b[39m\u001b[32m6290\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdivide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py:770\u001b[39m, in \u001b[36melementwise_division.<locals>.sparse_wrapper\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    754\u001b[39m                 (\n\u001b[32m    755\u001b[39m                     union_indices,\n\u001b[32m    756\u001b[39m                     x1_values_for_union,\n\u001b[32m   (...)\u001b[39m\u001b[32m    759\u001b[39m                     x1, x2_zeros_and_nan_indices\n\u001b[32m    760\u001b[39m                 )\n\u001b[32m    761\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m tf.IndexedSlices(\n\u001b[32m    762\u001b[39m                     func(\n\u001b[32m    763\u001b[39m                         x1_values_for_union,\n\u001b[32m   (...)\u001b[39m\u001b[32m    767\u001b[39m                     x1.dense_shape,\n\u001b[32m    768\u001b[39m                 )\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce_any\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2_zeros_and_nans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc_for_union_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc_for_x1_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x2, tf.IndexedSlices):\n\u001b[32m    776\u001b[39m     \u001b[38;5;66;03m# x2 is a IndexedSlices.\u001b[39;00m\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# Divisor is slices, densify to do the divisions by zero correctly.\u001b[39;00m\n\u001b[32m    778\u001b[39m     x2 = tf.convert_to_tensor(x2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py:758\u001b[39m, in \u001b[36melementwise_division.<locals>.sparse_wrapper.<locals>.func_for_union_indices\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    750\u001b[39m x2_zeros_and_nan_indices = tf.squeeze(\n\u001b[32m    751\u001b[39m     tf.where(x2_zeros_and_nans), axis=-\u001b[32m1\u001b[39m\n\u001b[32m    752\u001b[39m )\n\u001b[32m    753\u001b[39m \u001b[38;5;66;03m# Compute the union of indices to keep zeros and NaNs.\u001b[39;00m\n\u001b[32m    754\u001b[39m (\n\u001b[32m    755\u001b[39m     union_indices,\n\u001b[32m    756\u001b[39m     x1_values_for_union,\n\u001b[32m    757\u001b[39m     _,\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m ) = \u001b[43mindexed_slices_union_indices_and_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_zeros_and_nan_indices\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tf.IndexedSlices(\n\u001b[32m    762\u001b[39m     func(\n\u001b[32m    763\u001b[39m         x1_values_for_union,\n\u001b[32m   (...)\u001b[39m\u001b[32m    767\u001b[39m     x1.dense_shape,\n\u001b[32m    768\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py:139\u001b[39m, in \u001b[36mindexed_slices_union_indices_and_values\u001b[39m\u001b[34m(x1, x2_indices, x2_values)\u001b[39m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.gather(values_with_leading_zeros, to_union_indices)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Only recompute values if some indices were added.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m x1_values_for_union_indices = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1_indices_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munion_indices_count\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues_for_union\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx1_indices_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1_indices_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x2_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    147\u001b[39m     x2_values_for_union_indices = tf.cond(\n\u001b[32m    148\u001b[39m         tf.equal(x2_indices_count, union_indices_count),\n\u001b[32m    149\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: x2_values,\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m         ),\n\u001b[32m    153\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py:142\u001b[39m, in \u001b[36mindexed_slices_union_indices_and_values.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.gather(values_with_leading_zeros, to_union_indices)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Only recompute values if some indices were added.\u001b[39;00m\n\u001b[32m    139\u001b[39m x1_values_for_union_indices = tf.cond(\n\u001b[32m    140\u001b[39m     tf.equal(x1_indices_count, union_indices_count),\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: x1.values,\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mvalues_for_union\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx1_indices_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1_indices_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    145\u001b[39m )\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x2_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    147\u001b[39m     x2_values_for_union_indices = tf.cond(\n\u001b[32m    148\u001b[39m         tf.equal(x2_indices_count, union_indices_count),\n\u001b[32m    149\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: x2_values,\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m         ),\n\u001b[32m    153\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py:136\u001b[39m, in \u001b[36mindexed_slices_union_indices_and_values.<locals>.values_for_union\u001b[39m\u001b[34m(indices_expanded, indices_count, values)\u001b[39m\n\u001b[32m    132\u001b[39m to_union_indices = tf.gather(indices_indices, union_indices)\n\u001b[32m    133\u001b[39m values_with_leading_zeros = tf.concat(\n\u001b[32m    134\u001b[39m     [tf.zeros_like(values[\u001b[32m0\u001b[39m:\u001b[32m1\u001b[39m]), values], axis=\u001b[32m0\u001b[39m\n\u001b[32m    135\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues_with_leading_zeros\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_union_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[6] = 1421 is not in [0, 769) [Op:GatherV2] name: "
     ]
    }
   ],
   "source": [
    "# Now call run_notebook_training for 1 epoch to trigger the error and print diagnostics\n",
    "notebook_training_mod.run_notebook_training(\n",
    "    training_dataset=triplets_ds,\n",
    "    save_path=output_dir,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=30,\n",
    "    num_mixtures=2,\n",
    "    spherical=True,\n",
    "    learning_rate=0.01,\n",
    "    epochs=1,\n",
    "    adagrad=True,\n",
    "    normclip=True,\n",
    "    norm_cap=1.0,\n",
    "    lower_sig=0.01,\n",
    "    upper_sig=2.0,\n",
    "    var_scale=10.0,\n",
    "    loss_epsilon=1e-3,\n",
    "    wout=False,\n",
    "    tensorboard_log_path=os.path.join(output_dir, 'tensorboard'),\n",
    "    monitor_interval=1,\n",
    "    profile=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Inspect loss, gradients, and parameter stats after one batch\n",
    "from word2gm_fast.models.word2gm_model import Word2GMModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Get a single batch for diagnostics\n",
    "for batch in triplets_ds.take(1):\n",
    "    w, p, n = batch\n",
    "    break\n",
    "\n",
    "# Build a fresh model for diagnostics (same config as training)\n",
    "class Config:\n",
    "    vocab_size = vocab_size\n",
    "    embedding_size = 30\n",
    "    num_mixtures = 2\n",
    "    spherical = True\n",
    "    lower_sig = 0.01\n",
    "    upper_sig = 2.0\n",
    "    var_scale = 1.0\n",
    "    loss_epsilon = 1e-3\n",
    "    wout = False\n",
    "    max_pe = False\n",
    "\n",
    "diagnostic_model = Word2GMModel(Config())\n",
    "diagnostic_model.build([(None,), (None,), (None,)])\n",
    "\n",
    "# Forward pass and loss computation (margin=0.1 for diagnostics)\n",
    "margin = 0.1\n",
    "with tf.GradientTape() as tape:\n",
    "    # Patch the model's call to use a custom margin for this diagnostic\n",
    "    orig_call = diagnostic_model.call\n",
    "    def call_with_margin(inputs, training=None):\n",
    "        word_ids, pos_ids, neg_ids = inputs\n",
    "        word_mus, word_vars, word_weights = diagnostic_model.get_word_distributions(word_ids)\n",
    "        pos_mus, pos_vars, pos_weights = diagnostic_model.get_word_distributions(pos_ids, use_output=diagnostic_model.config.wout)\n",
    "        neg_mus, neg_vars, neg_weights = diagnostic_model.get_word_distributions(neg_ids, use_output=diagnostic_model.config.wout)\n",
    "        pos_energy = diagnostic_model.expected_likelihood_kernel(word_mus, word_vars, word_weights, pos_mus, pos_vars, pos_weights)\n",
    "        neg_energy = diagnostic_model.expected_likelihood_kernel(word_mus, word_vars, word_weights, neg_mus, neg_vars, neg_weights)\n",
    "        pos_energy = tf.clip_by_value(pos_energy, diagnostic_model.config.loss_epsilon, 1e6)\n",
    "        neg_energy = tf.clip_by_value(neg_energy, diagnostic_model.config.loss_epsilon, 1e6)\n",
    "        pos_log_energy = -tf.math.log(pos_energy)\n",
    "        neg_log_energy = -tf.math.log(neg_energy)\n",
    "        zero = tf.constant(0.0, dtype=pos_log_energy.dtype)\n",
    "        loss_per_sample = tf.maximum(zero, margin + pos_log_energy - neg_log_energy)\n",
    "        return tf.reduce_mean(loss_per_sample), {\n",
    "            'per_sample_loss': loss_per_sample,\n",
    "            'log_pos': pos_log_energy,\n",
    "            'log_neg': neg_log_energy,\n",
    "        }\n",
    "    diagnostic_model.call = call_with_margin\n",
    "    loss, details = diagnostic_model((w, p, n), training=True)\n",
    "    diagnostic_model.call = orig_call\n",
    "\n",
    "# Print loss and per-sample loss\n",
    "print(f\"Batch loss: {loss.numpy():.6f}\")\n",
    "print(\"Per-sample loss (first 10):\", details['per_sample_loss'][:10].numpy())\n",
    "\n",
    "# Print log energies (first 10)\n",
    "print(\"Log energies (pos, first 10):\", details['log_pos'][:10].numpy())\n",
    "print(\"Log energies (neg, first 10):\", details['log_neg'][:10].numpy())\n",
    "\n",
    "# Check for NaNs/Infs in loss and log energies\n",
    "print(\"Any NaN in per-sample loss?\", tf.math.reduce_any(tf.math.is_nan(details['per_sample_loss'])).numpy())\n",
    "print(\"Any Inf in per-sample loss?\", tf.math.reduce_any(tf.math.is_inf(details['per_sample_loss'])).numpy())\n",
    "\n",
    "# Compute gradients\n",
    "grads = tape.gradient(loss, diagnostic_model.trainable_variables)\n",
    "for i, (g, v) in enumerate(zip(grads, diagnostic_model.trainable_variables)):\n",
    "    if g is not None:\n",
    "        print(f\"Grad {i} ({v.name}): min={tf.reduce_min(g).numpy():.4e}, max={tf.reduce_max(g).numpy():.4e}, mean={tf.reduce_mean(g).numpy():.4e}\")\n",
    "    else:\n",
    "        print(f\"Grad {i} ({v.name}): None\")\n",
    "\n",
    "# Print parameter stats (first mixture mean, log-sigma, and mixture logits)\n",
    "mus = diagnostic_model.mus.numpy()\n",
    "logsigmas = diagnostic_model.logsigmas.numpy()\n",
    "mixture = diagnostic_model.mixture.numpy()\n",
    "print(\"mus[0,0,:5]:\", mus[0,0,:5])\n",
    "print(\"logsigmas[0,0,:5]:\", logsigmas[0,0,:5])\n",
    "print(\"mixture[0,:]:\", mixture[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Print raw expected_likelihood_kernel outputs for a few word pairs\n",
    "num_samples = 5\n",
    "w_np, p_np, n_np = w.numpy(), p.numpy(), n.numpy()\n",
    "for i in range(num_samples):\n",
    "    idx_w, idx_p, idx_n = w_np[i], p_np[i], n_np[i]\n",
    "    # Get single-word distributions\n",
    "    mus_w, vars_w, weights_w = diagnostic_model.get_word_distributions(tf.constant([idx_w]))\n",
    "    mus_p, vars_p, weights_p = diagnostic_model.get_word_distributions(tf.constant([idx_p]))\n",
    "    mus_n, vars_n, weights_n = diagnostic_model.get_word_distributions(tf.constant([idx_n]))\n",
    "    # Compute kernel values\n",
    "    k_pos = diagnostic_model.expected_likelihood_kernel(mus_w, vars_w, weights_w, mus_p, vars_p, weights_p).numpy()[0]\n",
    "    k_neg = diagnostic_model.expected_likelihood_kernel(mus_w, vars_w, weights_w, mus_n, vars_n, weights_n).numpy()[0]\n",
    "    print(f'Sample {i}:')\n",
    "    print(f'  w={idx_w}, p={idx_p}, n={idx_n}')\n",
    "    print(f'  kernel(w, p) = {k_pos}')\n",
    "    print(f'  kernel(w, n) = {k_neg}')\n",
    "\n",
    "    print(f'  -log(kernel(w, p)) = {-np.log(k_pos)}')\n",
    "    print(f'  -log(kernel(w, n)) = {-np.log(k_neg)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diagnostic: Check model/loss behavior on a single batch and optimizer step (margin=0.1) ---\n",
    "\n",
    "from word2gm_fast.models.word2gm_model import Word2GMModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Get a single batch from the dataset\n",
    "for batch in triplets_ds.take(1):\n",
    "    word_ids, pos_ids, neg_ids = batch\n",
    "    break\n",
    "else:\n",
    "    raise RuntimeError('No batch found in triplets_ds!')\n",
    "\n",
    "# Build a fresh model with the same config as training\n",
    "model = Word2GMModel(type('Config', (), dict(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=30,  # match your training config\n",
    "    num_mixtures=2,\n",
    "    spherical=True,\n",
    "    var_scale=1.0,  # match training config\n",
    "    loss_epsilon=1e-3,\n",
    "    wout=False,\n",
    "    max_pe=False\n",
    "))())\n",
    "model.build([(None,), (None,), (None,)])\n",
    "\n",
    "# Save initial weights for comparison\n",
    "init_weights = [w.numpy().copy() for w in model.trainable_weights]\n",
    "\n",
    "# Forward pass: compute loss for the batch with a smaller margin\n",
    "margin = 0.1\n",
    "with tf.GradientTape() as tape:\n",
    "    # Patch the model's call to use a custom margin for this diagnostic\n",
    "    orig_call = model.call\n",
    "    def call_with_margin(inputs, training=None):\n",
    "        word_ids, pos_ids, neg_ids = inputs\n",
    "        word_mus, word_vars, word_weights = model.get_word_distributions(word_ids)\n",
    "        pos_mus, pos_vars, pos_weights = model.get_word_distributions(pos_ids, use_output=model.config.wout)\n",
    "        neg_mus, neg_vars, neg_weights = model.get_word_distributions(neg_ids, use_output=model.config.wout)\n",
    "        pos_energy = model.expected_likelihood_kernel(word_mus, word_vars, word_weights, pos_mus, pos_vars, pos_weights)\n",
    "        neg_energy = model.expected_likelihood_kernel(word_mus, word_vars, word_weights, neg_mus, neg_vars, neg_weights)\n",
    "        pos_energy = tf.clip_by_value(pos_energy, model.config.loss_epsilon, 1e6)\n",
    "        neg_energy = tf.clip_by_value(neg_energy, model.config.loss_epsilon, 1e6)\n",
    "        pos_log_energy = -tf.math.log(pos_energy)\n",
    "        neg_log_energy = -tf.math.log(neg_energy)\n",
    "        zero = tf.constant(0.0, dtype=pos_log_energy.dtype)\n",
    "        loss_per_sample = tf.maximum(zero, margin + pos_log_energy - neg_log_energy)\n",
    "        return tf.reduce_mean(loss_per_sample)\n",
    "    model.call = call_with_margin\n",
    "    loss = model((word_ids, pos_ids, neg_ids), training=True)\n",
    "    model.call = orig_call\n",
    "print(f'Initial loss for batch (margin={margin}): {loss.numpy()}')\n",
    "\n",
    "# Compute gradients and apply one optimizer step\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "grads = tape.gradient(loss, model.trainable_weights)\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "# Check if any weights changed\n",
    "changed = [not np.allclose(w0, w1.numpy()) for w0, w1 in zip(init_weights, model.trainable_weights)]\n",
    "print('Any weights changed after one optimizer step?', any(changed))\n",
    "for i, (w0, w1) in enumerate(zip(init_weights, model.trainable_weights)):\n",
    "    if not np.allclose(w0, w1.numpy()):\n",
    "        print(f'Weight {i} changed: max diff = {np.abs(w0 - w1.numpy()).max()}')\n",
    "    else:\n",
    "        print(f'Weight {i} did NOT change.')\n",
    "\n",
    "# Forward pass again to see if loss changes\n",
    "loss2 = model((word_ids, pos_ids, neg_ids), training=True)\n",
    "print(f'Loss after one optimizer step: {loss2.numpy()}')\n",
    "\n",
    "# Check for NaNs/Infs in loss and weights\n",
    "print('Loss has NaN:', np.isnan(loss2.numpy()).any(), 'Inf:', np.isinf(loss2.numpy()).any())\n",
    "for i, w in enumerate(model.trainable_weights):\n",
    "    print(f'Weight {i}: min={w.numpy().min()}, max={w.numpy().max()}, has NaN={np.isnan(w.numpy()).any()}, has Inf={np.isinf(w.numpy()).any()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84652881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
