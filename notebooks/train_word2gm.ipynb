{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e28a5d",
   "metadata": {},
   "source": [
    "## Word2GM Visualization and Analysis\n",
    "\n",
    "Create visualizations of the trained Word2GM embeddings, including t-SNE plots of mixture components and interactive analysis tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Advanced Visualization (requires sklearn)\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    print(\"Creating t-SNE visualization of Word2GM embeddings...\")\n",
    "    \n",
    "    # Select a subset of words for visualization (top 1000 most frequent)\n",
    "    viz_size = min(1000, len(words))\n",
    "    viz_word_ids = list(range(viz_size))\n",
    "    viz_words = [id_to_word[i] for i in viz_word_ids if i in id_to_word]\n",
    "    \n",
    "    # Get embeddings for visualization\n",
    "    # For Word2GM, we'll use the mixture-weighted means as representative embeddings\n",
    "    viz_embeddings = []\n",
    "    for word_id in viz_word_ids:\n",
    "        if word_id in id_to_word:\n",
    "            embedding = model.get_word_embedding(word_id)\n",
    "            viz_embeddings.append(embedding)\n",
    "    \n",
    "    viz_embeddings = np.array(viz_embeddings)\n",
    "    \n",
    "    if len(viz_embeddings) > 10:  # Only proceed if we have enough words\n",
    "        print(f\"Computing t-SNE for {len(viz_embeddings)} words...\")\n",
    "        \n",
    "        # First reduce dimensionality with PCA for faster t-SNE\n",
    "        pca = PCA(n_components=min(50, viz_embeddings.shape[1]))\n",
    "        embeddings_pca = pca.fit_transform(viz_embeddings)\n",
    "        \n",
    "        # t-SNE visualization\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "        embeddings_2d = tsne.fit_transform(embeddings_pca)\n",
    "        \n",
    "        # Plot t-SNE\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                            alpha=0.6, s=20, c=range(len(embeddings_2d)), cmap='viridis')\n",
    "        \n",
    "        # Annotate some words\n",
    "        annotate_words = viz_words[:50] if len(viz_words) >= 50 else viz_words\n",
    "        for i, word in enumerate(annotate_words):\n",
    "            if i < len(embeddings_2d):\n",
    "                plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                           fontsize=8, alpha=0.7)\n",
    "        \n",
    "        plt.title('t-SNE Visualization of Word2GM Embeddings\\n(Mixture-weighted means)')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        plt.colorbar(scatter)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # If we have multiple mixture components, visualize them separately\n",
    "        if config.num_mixtures > 1:\n",
    "            print(\"Creating component-specific visualizations...\")\n",
    "            \n",
    "            fig, axes = plt.subplots(1, config.num_mixtures, figsize=(6*config.num_mixtures, 6))\n",
    "            if config.num_mixtures == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for comp in range(config.num_mixtures):\n",
    "                # Get component-specific embeddings\n",
    "                comp_embeddings = []\n",
    "                for word_id in viz_word_ids[:200]:  # Use fewer words for component viz\n",
    "                    if word_id in id_to_word:\n",
    "                        embedding = model.get_word_embedding(word_id, component=comp)\n",
    "                        comp_embeddings.append(embedding)\n",
    "                \n",
    "                comp_embeddings = np.array(comp_embeddings)\n",
    "                \n",
    "                if len(comp_embeddings) > 10:\n",
    "                    # PCA + t-SNE for this component\n",
    "                    comp_pca = pca.fit_transform(comp_embeddings)\n",
    "                    comp_tsne = TSNE(n_components=2, random_state=42).fit_transform(comp_pca)\n",
    "                    \n",
    "                    axes[comp].scatter(comp_tsne[:, 0], comp_tsne[:, 1], \n",
    "                                     alpha=0.6, s=30, c=range(len(comp_tsne)), cmap='plasma')\n",
    "                    axes[comp].set_title(f'Component {comp} Embeddings')\n",
    "                    axes[comp].set_xlabel('t-SNE 1')\n",
    "                    axes[comp].set_ylabel('t-SNE 2')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    print(\"✓ Visualization complete\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Scikit-learn not available. Skipping t-SNE visualization.\")\n",
    "    print(\"To enable visualization, install scikit-learn: pip install scikit-learn\")\n",
    "\n",
    "# Interactive word exploration function\n",
    "def explore_word(word):\n",
    "    \"\"\"Interactive function to explore a word's mixture components and neighbors.\"\"\"\n",
    "    if word not in word_to_id:\n",
    "        print(f\"Word '{word}' not found in vocabulary.\")\n",
    "        available = [w for w in word_to_id.keys() if w.startswith(word[:3])][:10]\n",
    "        if available:\n",
    "            print(f\"Similar words available: {', '.join(available)}\")\n",
    "        return\n",
    "    \n",
    "    word_id = word_to_id[word]\n",
    "    print(f\"\\nExploring word: '{word}' (ID: {word_id})\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Get mixture parameters\n",
    "    mus, vars, weights = model.get_word_distributions(tf.constant([word_id]))\n",
    "    mus, vars, weights = mus[0], vars[0], weights[0]\n",
    "    \n",
    "    print(f\"Mixture weights: {weights.numpy()}\")\n",
    "    print(f\"Number of components: {config.num_mixtures}\")\n",
    "    \n",
    "    # Component analysis\n",
    "    for comp in range(config.num_mixtures):\n",
    "        print(f\"\\nComponent {comp} (weight: {weights[comp]:.3f}):\")\n",
    "        print(f\"  Mean norm: {tf.norm(mus[comp]):.4f}\")\n",
    "        if config.spherical:\n",
    "            print(f\"  Variance: {vars[comp, 0]:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Mean variance: {tf.reduce_mean(vars[comp]):.4f}\")\n",
    "        \n",
    "        # Find neighbors for this component\n",
    "        neighbors = find_nearest_neighbors(model, word, word_to_id, id_to_word, k=5, component=comp)\n",
    "        if neighbors:\n",
    "            print(f\"  Nearest neighbors:\")\n",
    "            for i, (neighbor, score) in enumerate(neighbors):\n",
    "                print(f\"    {i+1}. {neighbor} ({score:.4f})\")\n",
    "    \n",
    "    # Overall neighbors\n",
    "    print(f\"\\nOverall nearest neighbors:\")\n",
    "    neighbors = find_nearest_neighbors(model, word, word_to_id, id_to_word, k=10)\n",
    "    for i, (neighbor, score) in enumerate(neighbors):\n",
    "        print(f\"  {i+1:2d}. {neighbor} ({score:.4f})\")\n",
    "\n",
    "# Examples of interactive exploration (run these in separate cells if desired)\n",
    "print(\"\\nInteractive Word Exploration\")\n",
    "print(\"=\" * 30)\n",
    "print(\"You can explore any word using the explore_word() function.\")\n",
    "print(\"Example usage:\")\n",
    "print(\"  explore_word('bank')\")\n",
    "print(\"  explore_word('rock')\")\n",
    "print(\"  explore_word('spring')\")\n",
    "\n",
    "# Demo with a common word if available\n",
    "demo_words = ['the', 'and', 'of', 'to', 'a', 'in', 'that', 'is', 'was', 'he']\n",
    "demo_word = None\n",
    "for word in demo_words:\n",
    "    if word in word_to_id:\n",
    "        demo_word = word\n",
    "        break\n",
    "\n",
    "if demo_word:\n",
    "    print(f\"\\nDemo exploration of '{demo_word}':\")\n",
    "    explore_word(demo_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8da507",
   "metadata": {},
   "source": [
    "# Word2GM Training & Evaluation\n",
    "\n",
    "**GPU-friendly TensorFlow port of Word2GM (Word to Gaussian Mixture) embeddings**\n",
    "\n",
    "This notebook demonstrates training and evaluation of the Word2GM model - a neural embedding approach that represents each word as a Gaussian Mixture Model instead of a single point vector.\n",
    "\n",
    "## Background\n",
    "\n",
    "Word2GM is based on the paper [\"Multimodal Word Distributions\"](https://arxiv.org/abs/1704.08424) by Athiwaratkun and Wilson (ACL 2017). The key innovation is representing words as **Gaussian mixture distributions** rather than point vectors, enabling:\n",
    "\n",
    "- **Multimodal representations**: Words like \"bank\" can have separate components for financial and geographical meanings\n",
    "- **Uncertainty modeling**: Capture confidence and variability in word meanings\n",
    "- **Richer semantic relationships**: Better capture entailment, similarity, and polysemy\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Each word `w` is represented as a Gaussian Mixture Model with `K` components:\n",
    "- **Means (μ)**: `K × d` dimensional centers\n",
    "- **Covariances (Σ)**: `K × d` diagonal/spherical covariances  \n",
    "- **Mixture weights (π)**: `K` dimensional probability weights\n",
    "\n",
    "**Training**: Max-margin objective using Expected Likelihood Kernel similarity between word distributions.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Load Training Data**: TFRecord triplets from data preparation pipeline\n",
    "2. **Model Training**: Word2GM with configurable mixture components\n",
    "3. **Evaluation**: Nearest neighbors, word similarity, polysemy analysis\n",
    "4. **Visualization**: t-SNE plots of mixture components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441c31d",
   "metadata": {},
   "source": [
    "## Environment Setup and GPU Configuration\n",
    "\n",
    "Configure the environment for optimal GPU usage during Word2GM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f62f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup project path\n",
    "project_root = Path('/scratch/edk202/word2gm-fast')\n",
    "os.chdir(project_root)\n",
    "src_path = project_root / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Configure TensorFlow for GPU usage with memory growth\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce TF logging\n",
    "# Remove CPU-only constraint to enable GPU training\n",
    "if 'CUDA_VISIBLE_DEVICES' in os.environ:\n",
    "    del os.environ['CUDA_VISIBLE_DEVICES']\n",
    "\n",
    "print(\"Environment configured for GPU training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb19f8",
   "metadata": {},
   "source": [
    "## Import Required Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow with GPU memory growth enabled\n",
    "from word2gm_fast.utils.tf_silence import import_tensorflow_silently\n",
    "tf = import_tensorflow_silently(gpu_memory_growth=True)\n",
    "\n",
    "# Configure GPU memory growth for training\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Word2GM modules\n",
    "from word2gm_fast.models.word2gm_model import Word2GMModel\n",
    "from word2gm_fast.models.config import Word2GMConfig\n",
    "from word2gm_fast.dataprep.tfrecord_io import load_triplets_from_tfrecord, load_vocab_from_tfrecord\n",
    "from word2gm_fast.training.training_utils import train_step, log_training_metrics, summarize_dataset_pipeline\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d474d",
   "metadata": {},
   "source": [
    "## Verify GPU Availability\n",
    "\n",
    "Check for available GPUs and print device information to ensure GPU resources are accessible for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0da582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(\"GPU Device Information:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Found {len(gpus)} GPU(s):\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu.name}\")\n",
    "        # Get GPU memory info if available\n",
    "        try:\n",
    "            gpu_details = tf.config.experimental.get_device_details(gpu)\n",
    "            if 'device_name' in gpu_details:\n",
    "                print(f\"    Device: {gpu_details['device_name']}\")\n",
    "        except:\n",
    "            pass\n",
    "    print()\n",
    "    \n",
    "    # Test GPU computation\n",
    "    print(\"Testing GPU computation...\")\n",
    "    with tf.device('/GPU:0'):\n",
    "        a = tf.random.normal([1000, 1000])\n",
    "        b = tf.random.normal([1000, 1000])\n",
    "        start = time.time()\n",
    "        c = tf.matmul(a, b)\n",
    "        gpu_time = time.time() - start\n",
    "        print(f\"  GPU matrix multiply (1000x1000): {gpu_time:.4f}s\")\n",
    "    \n",
    "    # Compare with CPU\n",
    "    with tf.device('/CPU:0'):\n",
    "        start = time.time()\n",
    "        c_cpu = tf.matmul(a, b)\n",
    "        cpu_time = time.time() - start\n",
    "        print(f\"  CPU matrix multiply (1000x1000): {cpu_time:.4f}s\")\n",
    "        print(f\"  GPU speedup: {cpu_time/gpu_time:.1f}x\")\n",
    "else:\n",
    "    print(\"No GPUs found. Running on CPU.\")\n",
    "    \n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2742d",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "\n",
    "Load TFRecord artifacts generated by the data preparation pipeline for Word2GM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018941fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these paths to match your processed data\n",
    "corpus_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data\"\n",
    "\n",
    "# Choose a year with processed artifacts for training (using 1700 as it exists)\n",
    "year = \"1700\"  # We have artifacts for this year\n",
    "artifacts_dir = f\"{corpus_dir}/{year}_artifacts\"\n",
    "\n",
    "print(f\"Loading training data from: {artifacts_dir}\")\n",
    "\n",
    "# Verify files exist\n",
    "triplets_path = f\"{artifacts_dir}/triplets.tfrecord\"\n",
    "vocab_path = f\"{artifacts_dir}/vocab.tfrecord\"\n",
    "\n",
    "if os.path.exists(triplets_path) and os.path.exists(vocab_path):\n",
    "    print(\"✓ TFRecord files found\")\n",
    "    \n",
    "    # Load vocabulary\n",
    "    print(\"Loading vocabulary...\")\n",
    "    vocab_table = load_vocab_from_tfrecord(vocab_path)\n",
    "    vocab_size = int(vocab_table.size())\n",
    "    print(f\"  Vocabulary size: {vocab_size:,} words\")\n",
    "    \n",
    "    # Load training triplets\n",
    "    print(\"Loading training triplets...\")\n",
    "    dataset = load_triplets_from_tfrecord(triplets_path)\n",
    "    \n",
    "    # Inspect dataset structure\n",
    "    print(\"Dataset pipeline structure:\")\n",
    "    summarize_dataset_pipeline(dataset)\n",
    "    \n",
    "    # Take a sample to verify data format\n",
    "    sample_batch = next(iter(dataset.batch(5)))\n",
    "    word_ids, pos_ids, neg_ids = sample_batch\n",
    "    print(f\"\\nSample batch shapes:\")\n",
    "    print(f\"  Word IDs: {word_ids.shape}\")\n",
    "    print(f\"  Positive IDs: {pos_ids.shape}\")\n",
    "    print(f\"  Negative IDs: {neg_ids.shape}\")\n",
    "    print(f\"  Sample values: {word_ids[:3].numpy()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ TFRecord files not found!\")\n",
    "    print(\"Please run the data preparation pipeline first.\")\n",
    "    print(f\"Expected files:\")\n",
    "    print(f\"  {triplets_path}\")\n",
    "    print(f\"  {vocab_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e29ad",
   "metadata": {},
   "source": [
    "## Create Word2GM Model\n",
    "\n",
    "Configure and initialize the Word2GM model with Gaussian mixture components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4697d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (matching original Word2GM paper settings)\n",
    "config = Word2GMConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=50,        # Embedding dimension\n",
    "    num_mixtures=2,           # Number of Gaussian components per word\n",
    "    spherical=True,           # Use spherical (not diagonal) covariances\n",
    "    learning_rate=0.05,       # Initial learning rate\n",
    "    batch_size=128,           # Training batch size\n",
    "    epochs_to_train=5,        # Number of training epochs (reduced for demo)\n",
    "    adagrad=True,             # Use Adagrad optimizer\n",
    "    var_scale=0.05,           # Variance scale for initialization\n",
    "    normclip=True,            # Enable gradient/parameter clipping\n",
    "    norm_cap=5.0,             # Norm clipping threshold\n",
    "    lower_sig=0.05,           # Lower bound for variances\n",
    "    upper_sig=1.0,            # Upper bound for variances\n",
    "    wout=False                # Use separate output embeddings\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Vocabulary size: {config.vocab_size:,}\")\n",
    "print(f\"Embedding size: {config.embedding_size}\")\n",
    "print(f\"Mixture components: {config.num_mixtures}\")\n",
    "print(f\"Covariance type: {'Spherical' if config.spherical else 'Diagonal'}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Training epochs: {config.epochs_to_train}\")\n",
    "print()\n",
    "\n",
    "# Create model\n",
    "model = Word2GMModel(config)\n",
    "\n",
    "# Create optimizer\n",
    "if config.adagrad:\n",
    "    optimizer = tf.keras.optimizers.Adagrad(learning_rate=config.learning_rate)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "print(f\"Model created with {config.num_mixtures} mixture components per word\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "print(f\"Optimizer: {'Adagrad' if config.adagrad else 'SGD'}\")\n",
    "\n",
    "# Print model summary for first few words\n",
    "print(f\"\\nModel structure (first 3 words):\")\n",
    "sample_word_ids = tf.constant([0, 1, 2])\n",
    "mus, vars, weights = model.get_word_distributions(sample_word_ids)\n",
    "print(f\"  Means shape: {mus.shape}\")\n",
    "print(f\"  Variances shape: {vars.shape}\")\n",
    "print(f\"  Weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39974cd6",
   "metadata": {},
   "source": [
    "## Train Word2GM Model\n",
    "\n",
    "Train the model using GPU-accelerated operations with the max-margin objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training\n",
    "batch_size = config.batch_size\n",
    "train_dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Training metrics\n",
    "training_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Starting Word2GM training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(config.epochs_to_train):\n",
    "    epoch_start = time.time()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{config.epochs_to_train}\")\n",
    "    \n",
    "    for batch_idx, (word_ids, pos_ids, neg_ids) in enumerate(train_dataset):\n",
    "        # Training step with GPU acceleration\n",
    "        loss, grads = train_step(\n",
    "            model, optimizer, word_ids, pos_ids, neg_ids,\n",
    "            normclip=config.normclip,\n",
    "            norm_cap=config.norm_cap,\n",
    "            lower_sig=config.lower_sig,\n",
    "            upper_sig=config.upper_sig,\n",
    "            wout=config.wout\n",
    "        )\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Print progress every 100 batches\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            print(f\"  Batch {batch_idx}: loss = {loss:.6f}, avg = {avg_loss:.6f}\")\n",
    "    \n",
    "    # Epoch summary\n",
    "    avg_loss = epoch_loss / max(1, num_batches)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    training_losses.append(float(avg_loss))\n",
    "    \n",
    "    print(f\"  Epoch {epoch + 1} complete:\")\n",
    "    print(f\"    Average loss: {avg_loss:.6f}\")\n",
    "    print(f\"    Time: {epoch_time:.1f}s\")\n",
    "    print(f\"    Batches processed: {num_batches}\")\n",
    "    \n",
    "    # Log model statistics\n",
    "    mean_mu_norm = tf.reduce_mean(tf.norm(model.mus, axis=-1))\n",
    "    mean_sigma = tf.reduce_mean(tf.exp(model.logsigmas))\n",
    "    print(f\"    Mean μ norm: {mean_mu_norm:.4f}\")\n",
    "    print(f\"    Mean σ: {mean_sigma:.4f}\")\n",
    "    print()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training complete! Total time: {total_time:.1f}s\")\n",
    "print(f\"Final loss: {training_losses[-1]:.6f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_losses, 'b-', linewidth=2)\n",
    "plt.title('Word2GM Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = f\"{artifacts_dir}/word2gm_model\"\n",
    "print(f\"Saving trained model to: {model_save_path}\")\n",
    "model.save_weights(model_save_path)\n",
    "print(\"✓ Model saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaf17f8",
   "metadata": {},
   "source": [
    "## Evaluate Trained Model\n",
    "\n",
    "Analyze the trained Word2GM model by examining word representations and finding nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reverse vocabulary lookup\n",
    "print(\"Model Evaluation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Extract vocabulary as numpy arrays for analysis\n",
    "vocab_keys, vocab_values = vocab_table.export()\n",
    "words = [key.numpy().decode('utf-8') for key in vocab_keys]\n",
    "word_ids = [int(val.numpy()) for val in vocab_values]\n",
    "\n",
    "# Create word-to-id and id-to-word mappings\n",
    "word_to_id = {word: word_id for word, word_id in zip(words, word_ids)}\n",
    "id_to_word = {word_id: word for word_id, word in zip(word_ids, words)}\n",
    "\n",
    "print(f\"Vocabulary loaded: {len(words):,} words\")\n",
    "\n",
    "# Analyze mixture components for sample words\n",
    "def analyze_word_mixtures(model, word_ids, id_to_word_map, num_words=10):\n",
    "    \"\"\"Analyze mixture components for given words.\"\"\"\n",
    "    if len(word_ids) > num_words:\n",
    "        word_ids = word_ids[:num_words]\n",
    "    \n",
    "    mus, vars, weights = model.get_word_distributions(tf.constant(word_ids))\n",
    "    \n",
    "    print(f\"\\nMixture Analysis for {len(word_ids)} words:\")\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        word = id_to_word_map.get(word_id, f\"<UNK_{word_id}>\")\n",
    "        print(f\"\\nWord: '{word}' (ID: {word_id})\")\n",
    "        print(f\"  Mixture weights: {weights[i].numpy()}\")\n",
    "        print(f\"  Component means (first 5 dims):\")\n",
    "        for k in range(config.num_mixtures):\n",
    "            mean_preview = mus[i, k, :5].numpy()\n",
    "            var_preview = vars[i, k, :5].numpy() if not config.spherical else vars[i, k, 0].numpy()\n",
    "            print(f\"    Component {k}: μ={mean_preview} σ²={var_preview}\")\n",
    "\n",
    "# Function to find nearest neighbors\n",
    "def find_nearest_neighbors(model, query_word, word_to_id_map, id_to_word_map, k=10, component=None):\n",
    "    \"\"\"Find nearest neighbors for a word using expected likelihood kernel.\"\"\"\n",
    "    if query_word not in word_to_id_map:\n",
    "        print(f\"Word '{query_word}' not found in vocabulary\")\n",
    "        return []\n",
    "    \n",
    "    query_id = word_to_id_map[query_word]\n",
    "    try:\n",
    "        neighbors = model.get_nearest_neighbors(query_id, k=k, component=component)\n",
    "        result = []\n",
    "        for neighbor_id, score in neighbors:\n",
    "            neighbor_word = id_to_word_map.get(neighbor_id, f\"<UNK_{neighbor_id}>\")\n",
    "            result.append((neighbor_word, score))\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding neighbors: {e}\")\n",
    "        return []\n",
    "\n",
    "# Analyze first 5 words\n",
    "sample_word_ids = list(range(min(5, len(words))))\n",
    "analyze_word_mixtures(model, sample_word_ids, id_to_word)\n",
    "\n",
    "# Example words for polysemy analysis (if they exist in vocabulary)\n",
    "example_words = ['bank', 'rock', 'spring', 'light', 'star', 'plant', 'left', 'right']\n",
    "existing_examples = [word for word in example_words if word in word_to_id]\n",
    "\n",
    "if existing_examples:\n",
    "    print(f\"\\nNearest Neighbor Analysis for Example Words:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for word in existing_examples[:3]:  # Analyze first 3 existing examples\n",
    "        print(f\"\\nWord: '{word}'\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        # Overall nearest neighbors\n",
    "        neighbors = find_nearest_neighbors(model, word, word_to_id, id_to_word, k=10)\n",
    "        if neighbors:\n",
    "            print(\"Overall nearest neighbors:\")\n",
    "            for i, (neighbor, score) in enumerate(neighbors):\n",
    "                print(f\"  {i+1:2d}. {neighbor} ({score:.4f})\")\n",
    "        \n",
    "        # Component-specific neighbors (if multiple components)\n",
    "        if config.num_mixtures > 1:\n",
    "            for comp in range(config.num_mixtures):\n",
    "                comp_neighbors = find_nearest_neighbors(model, word, word_to_id, id_to_word, k=5, component=comp)\n",
    "                if comp_neighbors:\n",
    "                    print(f\"Component {comp} neighbors:\")\n",
    "                    for i, (neighbor, score) in enumerate(comp_neighbors):\n",
    "                        print(f\"  {i+1}. {neighbor} ({score:.4f})\")\n",
    "\n",
    "# Examine parameter distributions\n",
    "print(f\"\\nModel Parameter Statistics:\")\n",
    "print(f\"=\" * 30)\n",
    "\n",
    "# Means statistics\n",
    "mu_norms = tf.norm(model.mus, axis=-1)  # [vocab_size, num_mixtures]\n",
    "print(f\"Mean norms:\")\n",
    "print(f\"  Min: {tf.reduce_min(mu_norms):.4f}\")\n",
    "print(f\"  Max: {tf.reduce_max(mu_norms):.4f}\")\n",
    "print(f\"  Mean: {tf.reduce_mean(mu_norms):.4f}\")\n",
    "print(f\"  Std: {tf.math.reduce_std(mu_norms):.4f}\")\n",
    "\n",
    "# Variance statistics\n",
    "sigmas = tf.exp(model.logsigmas)\n",
    "print(f\"Variances:\")\n",
    "print(f\"  Min: {tf.reduce_min(sigmas):.4f}\")\n",
    "print(f\"  Max: {tf.reduce_max(sigmas):.4f}\")\n",
    "print(f\"  Mean: {tf.reduce_mean(sigmas):.4f}\")\n",
    "\n",
    "# Mixture weights statistics\n",
    "mixture_probs = tf.nn.softmax(model.mixture, axis=-1)\n",
    "print(f\"Mixture weights:\")\n",
    "print(f\"  Min: {tf.reduce_min(mixture_probs):.4f}\")\n",
    "print(f\"  Max: {tf.reduce_max(mixture_probs):.4f}\")\n",
    "print(f\"  Mean: {tf.reduce_mean(mixture_probs):.4f}\")\n",
    "\n",
    "# Plot parameter distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Mean norms histogram\n",
    "axes[0,0].hist(mu_norms.numpy().flatten(), bins=50, alpha=0.7, color='blue')\n",
    "axes[0,0].set_title('Distribution of Mean Norms')\n",
    "axes[0,0].set_xlabel('Norm')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "\n",
    "# Variance histogram\n",
    "axes[0,1].hist(sigmas.numpy().flatten(), bins=50, alpha=0.7, color='green')\n",
    "axes[0,1].set_title('Distribution of Variances')\n",
    "axes[0,1].set_xlabel('Variance')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# Mixture weights histogram\n",
    "axes[1,0].hist(mixture_probs.numpy().flatten(), bins=50, alpha=0.7, color='red')\n",
    "axes[1,0].set_title('Distribution of Mixture Weights')\n",
    "axes[1,0].set_xlabel('Weight')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Training loss\n",
    "axes[1,1].plot(training_losses, 'b-', linewidth=2)\n",
    "axes[1,1].set_title('Training Loss')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('Loss')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training and evaluation complete!\")\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(\"You can now use the trained Word2GM model for downstream tasks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
