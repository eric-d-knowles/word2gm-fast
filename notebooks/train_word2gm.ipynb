{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c5f9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>Autoreload enabled</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Project root: /scratch/edk202/word2gm-fast\n",
       "TensorFlow version: 2.19.0\n",
       "Device mode: GPU-enabled</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Training environment ready!</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set project root directory and add `src` to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import the notebook setup utilities\n",
    "from word2gm_fast.utils.notebook_setup import setup_training_notebook, enable_autoreload, run_silent_subprocess\n",
    "\n",
    "# Enable autoreload for development\n",
    "enable_autoreload()\n",
    "\n",
    "# Set up environment (deterministic=True for reproducibility)\n",
    "env = setup_training_notebook(project_root=PROJECT_ROOT)\n",
    "\n",
    "# Extract commonly used modules for convenience\n",
    "tf = env['tensorflow']\n",
    "np = env['numpy']\n",
    "pd = env['pandas']\n",
    "print_resource_summary = env['print_resource_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a9987c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>SYSTEM RESOURCE SUMMARY\n",
       "============================================================\n",
       "Hostname: cm040.hpc.nyu.edu\n",
       "\n",
       "Job Allocation:\n",
       "   CPUs: 14\n",
       "   Memory: 125.0 GB\n",
       "   Requested partitions: short,cs,cm,cpu_a100_2,cpu_a100_1,cpu_gpu\n",
       "   Running on: SSH failed: Host key verification failed.\n",
       "   Job ID: 63030133\n",
       "   Node list: cm040\n",
       "\n",
       "GPU Information:\n",
       "   Error: NVML Shared Library Not Found\n",
       "\n",
       "TensorFlow GPU Detection:\n",
       "   TensorFlow detects 0 GPU(s)\n",
       "   Built with CUDA: True\n",
       "============================================================</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51623d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from word2gm_fast.training.notebook_training import run_notebook_training\n",
    "from word2gm_fast.utils.tfrecord_io import load_pipeline_artifacts\n",
    "\n",
    "# Define paths for your small corpus aartifacts and output\n",
    "artifacts_dir = '/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1750_artifacts'\n",
    "output_dir = '/scratch/edk202/word2gm-fast/output/test_small_corpus'\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21be501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>Loading pipeline artifacts from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1750_artifacts</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading vocabulary TFRecord from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1750_artifacts/vocab.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading triplet TFRecord from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1750_artifacts/triplets.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Triplet TFRecord loaded and parsed</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>All artifacts loaded successfully!</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the dataset pipeline: cache → shuffle → batch → prefetch\n",
    "\n",
    "# Set the path to your artifacts directory (already defined as artifacts_dir)\n",
    "artifacts = load_pipeline_artifacts(artifacts_dir)\n",
    "\n",
    "# Unpack the loaded artifacts\n",
    "vocab_table = artifacts['vocab_table']\n",
    "triplets_ds = artifacts['triplets_ds']\n",
    "vocab_size = artifacts['vocab_size']\n",
    "\n",
    "# Build the dataset pipeline: cache -> shuffle -> batch -> prefetch\n",
    "triplets_ds = triplets_ds.cache()\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 10\n",
    "triplets_ds = triplets_ds.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "triplets_ds = triplets_ds.batch(BATCH_SIZE)\n",
    "triplets_ds = triplets_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04ed9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 5143\n",
      "max index in triplets: 5142\n",
      "max index in triplets: 5142\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab_size:\", vocab_size)\n",
    "max_idx = -1\n",
    "for batch in triplets_ds.unbatch():\n",
    "    w, p, n = batch\n",
    "    max_idx = max(max_idx, w.numpy(), p.numpy(), n.numpy())\n",
    "print(\"max index in triplets:\", max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ed4132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset pipeline structure (oldest to newest):\n",
      "  [0] TFRecordDatasetV2\n",
      "  [1] _ParallelMapDataset\n",
      "  [2] _ParallelMapDataset\n",
      "  [3] CacheDataset\n",
      "  [4] _ShuffleDataset\n",
      "  [5] _BatchDataset\n",
      "  [6] _PrefetchDataset\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<pre>\n",
       "Starting Word2GM training</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Writing TensorBoard logs to /scratch/edk202/word2gm-fast/output/test_small_corpus/tensorboard</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Epoch 1/5 | Loss: 1.00000 | Time: 2.92 sec</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Epoch 2/5 | Loss: 1.00000 | Time: 2.61 sec</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Epoch 3/5 | Loss: 1.00000 | Time: 2.59 sec</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Epoch 4/5 | Loss: 1.00000 | Time: 2.79 sec</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Epoch 5/5 | Loss: 1.00000 | Time: 2.60 sec</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Total training time: 13.67 seconds</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_notebook_training(\n",
    "    training_dataset=triplets_ds,\n",
    "    save_path=output_dir,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=30,\n",
    "    num_mixtures=2,\n",
    "    spherical=True,\n",
    "    learning_rate=0.01,\n",
    "    epochs=5,\n",
    "    adagrad=True,\n",
    "    normclip=True,\n",
    "    norm_cap=1.0,\n",
    "    lower_sig=0.01,  # widened lower bound\n",
    "    upper_sig=2.0,   # widened upper bound\n",
    "    var_scale=1.0,   # larger initial variance\n",
    "    loss_epsilon=1e-3,\n",
    "    wout=False,\n",
    "    tensorboard_log_path=os.path.join(output_dir, 'tensorboard'),\n",
    "    monitor_interval=2,\n",
    "    profile=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b645bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.100000\n",
      "Per-sample loss (first 10): [0.0999999 0.0999999 0.0999999 0.0999999 0.0999999 0.0999999 0.0999999\n",
      " 0.0999999 0.0999999 0.0999999]\n",
      "Log energies (pos, first 10): [6.9077554 6.9077554 6.9077554 6.9077554 6.9077554 6.9077554 6.9077554\n",
      " 6.9077554 6.9077554 6.9077554]\n",
      "Log energies (neg, first 10): [6.9077554 6.9077554 6.9077554 6.9077554 6.9077554 6.9077554 6.9077554\n",
      " 6.9077554 6.9077554 6.9077554]\n",
      "Any NaN in per-sample loss? False\n",
      "Any Inf in per-sample loss? False\n",
      "Grad 0 (mus): min=0.0000e+00, max=0.0000e+00, mean=0.0000e+00\n",
      "Grad 1 (logsigmas): min=0.0000e+00, max=0.0000e+00, mean=0.0000e+00\n",
      "Grad 2 (mixture): min=0.0000e+00, max=0.0000e+00, mean=0.0000e+00\n",
      "mus[0,0,:5]: [ 1.4378221 -0.7192576  1.562633  -1.2091137 -0.609277 ]\n",
      "logsigmas[0,0,:5]: [0.00225813]\n",
      "mixture[0,:]: [-0.00554704 -0.00301867]\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Inspect loss, gradients, and parameter stats after one batch\n",
    "from word2gm_fast.models.word2gm_model import Word2GMModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Get a single batch for diagnostics\n",
    "for batch in triplets_ds.take(1):\n",
    "    w, p, n = batch\n",
    "    break\n",
    "\n",
    "# Build a fresh model for diagnostics (same config as training)\n",
    "class Config:\n",
    "    vocab_size = vocab_size\n",
    "    embedding_size = 30\n",
    "    num_mixtures = 2\n",
    "    spherical = True\n",
    "    lower_sig = 0.01\n",
    "    upper_sig = 2.0\n",
    "    var_scale = 1.0\n",
    "    loss_epsilon = 1e-3\n",
    "    wout = False\n",
    "    max_pe = False\n",
    "\n",
    "diagnostic_model = Word2GMModel(Config())\n",
    "diagnostic_model.build([(None,), (None,), (None,)])\n",
    "\n",
    "# Forward pass and loss computation (margin=0.1 for diagnostics)\n",
    "margin = 0.1\n",
    "with tf.GradientTape() as tape:\n",
    "    # Patch the model's call to use a custom margin for this diagnostic\n",
    "    orig_call = diagnostic_model.call\n",
    "    def call_with_margin(inputs, training=None):\n",
    "        word_ids, pos_ids, neg_ids = inputs\n",
    "        word_mus, word_vars, word_weights = diagnostic_model.get_word_distributions(word_ids)\n",
    "        pos_mus, pos_vars, pos_weights = diagnostic_model.get_word_distributions(pos_ids, use_output=diagnostic_model.config.wout)\n",
    "        neg_mus, neg_vars, neg_weights = diagnostic_model.get_word_distributions(neg_ids, use_output=diagnostic_model.config.wout)\n",
    "        pos_energy = diagnostic_model.expected_likelihood_kernel(word_mus, word_vars, word_weights, pos_mus, pos_vars, pos_weights)\n",
    "        neg_energy = diagnostic_model.expected_likelihood_kernel(word_mus, word_vars, word_weights, neg_mus, neg_vars, neg_weights)\n",
    "        pos_energy = tf.clip_by_value(pos_energy, diagnostic_model.config.loss_epsilon, 1e6)\n",
    "        neg_energy = tf.clip_by_value(neg_energy, diagnostic_model.config.loss_epsilon, 1e6)\n",
    "        pos_log_energy = -tf.math.log(pos_energy)\n",
    "        neg_log_energy = -tf.math.log(neg_energy)\n",
    "        zero = tf.constant(0.0, dtype=pos_log_energy.dtype)\n",
    "        loss_per_sample = tf.maximum(zero, margin + pos_log_energy - neg_log_energy)\n",
    "        return tf.reduce_mean(loss_per_sample), {\n",
    "            'per_sample_loss': loss_per_sample,\n",
    "            'log_pos': pos_log_energy,\n",
    "            'log_neg': neg_log_energy,\n",
    "        }\n",
    "    diagnostic_model.call = call_with_margin\n",
    "    loss, details = diagnostic_model((w, p, n), training=True)\n",
    "    diagnostic_model.call = orig_call\n",
    "\n",
    "# Print loss and per-sample loss\n",
    "print(f\"Batch loss: {loss.numpy():.6f}\")\n",
    "print(\"Per-sample loss (first 10):\", details['per_sample_loss'][:10].numpy())\n",
    "\n",
    "# Print log energies (first 10)\n",
    "print(\"Log energies (pos, first 10):\", details['log_pos'][:10].numpy())\n",
    "print(\"Log energies (neg, first 10):\", details['log_neg'][:10].numpy())\n",
    "\n",
    "# Check for NaNs/Infs in loss and log energies\n",
    "print(\"Any NaN in per-sample loss?\", tf.math.reduce_any(tf.math.is_nan(details['per_sample_loss'])).numpy())\n",
    "print(\"Any Inf in per-sample loss?\", tf.math.reduce_any(tf.math.is_inf(details['per_sample_loss'])).numpy())\n",
    "\n",
    "# Compute gradients\n",
    "grads = tape.gradient(loss, diagnostic_model.trainable_variables)\n",
    "for i, (g, v) in enumerate(zip(grads, diagnostic_model.trainable_variables)):\n",
    "    if g is not None:\n",
    "        print(f\"Grad {i} ({v.name}): min={tf.reduce_min(g).numpy():.4e}, max={tf.reduce_max(g).numpy():.4e}, mean={tf.reduce_mean(g).numpy():.4e}\")\n",
    "    else:\n",
    "        print(f\"Grad {i} ({v.name}): None\")\n",
    "\n",
    "# Print parameter stats (first mixture mean, log-sigma, and mixture logits)\n",
    "mus = diagnostic_model.mus.numpy()\n",
    "logsigmas = diagnostic_model.logsigmas.numpy()\n",
    "mixture = diagnostic_model.mixture.numpy()\n",
    "print(\"mus[0,0,:5]:\", mus[0,0,:5])\n",
    "print(\"logsigmas[0,0,:5]:\", logsigmas[0,0,:5])\n",
    "print(\"mixture[0,:]:\", mixture[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13bb1886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "  w=3620, p=2028, n=2346\n",
      "  kernel(w, p) = 1.3182880622634313e-13\n",
      "  kernel(w, n) = 1.212661047966801e-12\n",
      "  -log(kernel(w, p)) = 29.657272338867188\n",
      "  -log(kernel(w, n)) = 27.438203811645508\n",
      "Sample 1:\n",
      "  w=1919, p=2028, n=3109\n",
      "  kernel(w, p) = 1.6754596299760338e-13\n",
      "  kernel(w, n) = 1.7411729881251087e-13\n",
      "  -log(kernel(w, p)) = 29.417518615722656\n",
      "  -log(kernel(w, n)) = 29.379047393798828\n",
      "Sample 2:\n",
      "  w=3164, p=2836, n=3654\n",
      "  kernel(w, p) = 1.1842838341935558e-11\n",
      "  kernel(w, n) = 1.9210861200436025e-12\n",
      "  -log(kernel(w, p)) = 25.159297943115234\n",
      "  -log(kernel(w, n)) = 26.978130340576172\n",
      "Sample 3:\n",
      "  w=3058, p=372, n=2497\n",
      "  kernel(w, p) = 2.1205396650864766e-13\n",
      "  kernel(w, n) = 1.7321638914880033e-12\n",
      "  -log(kernel(w, p)) = 29.181936264038086\n",
      "  -log(kernel(w, n)) = 27.081649780273438\n",
      "Sample 4:\n",
      "  w=3743, p=2028, n=2427\n",
      "  kernel(w, p) = 3.81823817897399e-14\n",
      "  kernel(w, n) = 8.794113576455001e-14\n",
      "  -log(kernel(w, p)) = 30.89640235900879\n",
      "  -log(kernel(w, n)) = 30.062108993530273\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Print raw expected_likelihood_kernel outputs for a few word pairs\n",
    "num_samples = 5\n",
    "w_np, p_np, n_np = w.numpy(), p.numpy(), n.numpy()\n",
    "for i in range(num_samples):\n",
    "    idx_w, idx_p, idx_n = w_np[i], p_np[i], n_np[i]\n",
    "    # Get single-word distributions\n",
    "    mus_w, vars_w, weights_w = diagnostic_model.get_word_distributions(tf.constant([idx_w]))\n",
    "    mus_p, vars_p, weights_p = diagnostic_model.get_word_distributions(tf.constant([idx_p]))\n",
    "    mus_n, vars_n, weights_n = diagnostic_model.get_word_distributions(tf.constant([idx_n]))\n",
    "    # Compute kernel values\n",
    "    k_pos = diagnostic_model.expected_likelihood_kernel(mus_w, vars_w, weights_w, mus_p, vars_p, weights_p).numpy()[0]\n",
    "    k_neg = diagnostic_model.expected_likelihood_kernel(mus_w, vars_w, weights_w, mus_n, vars_n, weights_n).numpy()[0]\n",
    "    print(f'Sample {i}:')\n",
    "    print(f'  w={idx_w}, p={idx_p}, n={idx_n}')\n",
    "    print(f'  kernel(w, p) = {k_pos}')\n",
    "    print(f'  kernel(w, n) = {k_neg}')\n",
    "\n",
    "    print(f'  -log(kernel(w, p)) = {-np.log(k_pos)}')\n",
    "    print(f'  -log(kernel(w, n)) = {-np.log(k_neg)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf5ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss for batch (margin=0.1): 0.09999990463256836\n",
      "Any weights changed after one optimizer step? False\n",
      "Weight 0 did NOT change.\n",
      "Weight 1 did NOT change.\n",
      "Weight 2 did NOT change.\n",
      "Loss after one optimizer step: 1.0\n",
      "Loss has NaN: False Inf: False\n",
      "Weight 0: min=-4.589941501617432, max=4.4528326988220215, has NaN=False, has Inf=False\n",
      "Weight 1: min=-0.19939996302127838, max=0.19155453145503998, has NaN=False, has Inf=False\n",
      "Weight 2: min=-0.03728317469358444, max=0.03973766416311264, has NaN=False, has Inf=False\n"
     ]
    }
   ],
   "source": [
    "# --- Diagnostic: Check model/loss behavior on a single batch and optimizer step (margin=0.1) ---\n",
    "\n",
    "from word2gm_fast.models.word2gm_model import Word2GMModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Get a single batch from the dataset\n",
    "for batch in triplets_ds.take(1):\n",
    "    word_ids, pos_ids, neg_ids = batch\n",
    "    break\n",
    "else:\n",
    "    raise RuntimeError('No batch found in triplets_ds!')\n",
    "\n",
    "# Build a fresh model with the same config as training\n",
    "model = Word2GMModel(type('Config', (), dict(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=30,  # match your training config\n",
    "    num_mixtures=2,\n",
    "    spherical=True,\n",
    "    var_scale=1.0,  # match training config\n",
    "    loss_epsilon=1e-3,\n",
    "    wout=False,\n",
    "    max_pe=False\n",
    "))())\n",
    "model.build([(None,), (None,), (None,)])\n",
    "\n",
    "# Save initial weights for comparison\n",
    "init_weights = [w.numpy().copy() for w in model.trainable_weights]\n",
    "\n",
    "# Forward pass: compute loss for the batch with a smaller margin\n",
    "margin = 0.1\n",
    "with tf.GradientTape() as tape:\n",
    "    # Patch the model's call to use a custom margin for this diagnostic\n",
    "    orig_call = model.call\n",
    "    def call_with_margin(inputs, training=None):\n",
    "        word_ids, pos_ids, neg_ids = inputs\n",
    "        word_mus, word_vars, word_weights = model.get_word_distributions(word_ids)\n",
    "        pos_mus, pos_vars, pos_weights = model.get_word_distributions(pos_ids, use_output=model.config.wout)\n",
    "        neg_mus, neg_vars, neg_weights = model.get_word_distributions(neg_ids, use_output=model.config.wout)\n",
    "        pos_energy = model.expected_likelihood_kernel(word_mus, word_vars, word_weights, pos_mus, pos_vars, pos_weights)\n",
    "        neg_energy = model.expected_likelihood_kernel(word_mus, word_vars, word_weights, neg_mus, neg_vars, neg_weights)\n",
    "        pos_energy = tf.clip_by_value(pos_energy, model.config.loss_epsilon, 1e6)\n",
    "        neg_energy = tf.clip_by_value(neg_energy, model.config.loss_epsilon, 1e6)\n",
    "        pos_log_energy = -tf.math.log(pos_energy)\n",
    "        neg_log_energy = -tf.math.log(neg_energy)\n",
    "        zero = tf.constant(0.0, dtype=pos_log_energy.dtype)\n",
    "        loss_per_sample = tf.maximum(zero, margin + pos_log_energy - neg_log_energy)\n",
    "        return tf.reduce_mean(loss_per_sample)\n",
    "    model.call = call_with_margin\n",
    "    loss = model((word_ids, pos_ids, neg_ids), training=True)\n",
    "    model.call = orig_call\n",
    "print(f'Initial loss for batch (margin={margin}): {loss.numpy()}')\n",
    "\n",
    "# Compute gradients and apply one optimizer step\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "grads = tape.gradient(loss, model.trainable_weights)\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "# Check if any weights changed\n",
    "changed = [not np.allclose(w0, w1.numpy()) for w0, w1 in zip(init_weights, model.trainable_weights)]\n",
    "print('Any weights changed after one optimizer step?', any(changed))\n",
    "for i, (w0, w1) in enumerate(zip(init_weights, model.trainable_weights)):\n",
    "    if not np.allclose(w0, w1.numpy()):\n",
    "        print(f'Weight {i} changed: max diff = {np.abs(w0 - w1.numpy()).max()}')\n",
    "    else:\n",
    "        print(f'Weight {i} did NOT change.')\n",
    "\n",
    "# Forward pass again to see if loss changes\n",
    "loss2 = model((word_ids, pos_ids, neg_ids), training=True)\n",
    "print(f'Loss after one optimizer step: {loss2.numpy()}')\n",
    "\n",
    "# Check for NaNs/Infs in loss and weights\n",
    "print('Loss has NaN:', np.isnan(loss2.numpy()).any(), 'Inf:', np.isinf(loss2.numpy()).any())\n",
    "for i, w in enumerate(model.trainable_weights):\n",
    "    print(f'Weight {i}: min={w.numpy().min()}, max={w.numpy().max()}, has NaN={np.isnan(w.numpy()).any()}, has Inf={np.isinf(w.numpy()).any()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84652881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
