{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ce6ba",
   "metadata": {},
   "source": [
    "# Word2GM Training Data Pipeline\n",
    "\n",
    "**Pipeline: Corpus file → TFRecord training artifacts (triplets and vocabulary)**\n",
    "\n",
    "Use this notebook to prepare a Google 5gram corpora for Word2GM skip-gram training.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "1. **Input**: Preprocessed corpus file (e.g., `2019.txt`) in `/vast` NVMe storage\n",
    "2. **Processing**: TensorFlow-native filtering, vocabulary building, and triplet generation\n",
    "3. **Output**: TFRecord artifacts in organized subdirectories (e.g., `2019_artifacts/`)\n",
    "\n",
    "### **Artifact Storage**\n",
    "The pipeline creates year-specific subdirectories alongside the original text corpora:\n",
    "<pre>\n",
    "/vast/edk202/NLP_corpora/.../data/\n",
    "├── 2018.txt\n",
    "├── 2019.txt\n",
    "├── 2020.txt\n",
    "├── 2018_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "├── 2019_artifacts/\n",
    "│   ├── triplets.tfrecord.gz\n",
    "│   └── vocab.tfrecord.gz\n",
    "└── 2020_artifacts/\n",
    "    ├── triplets.tfrecord.gz\n",
    "    └── vocab.tfrecord.gz\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49f604",
   "metadata": {},
   "source": [
    "## Set Up for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89181d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:40:48.131008: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-14 17:40:48.147376: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752529248.165213 2041250 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752529248.170561 2041250 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752529248.184454 2041250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752529248.184468 2041250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752529248.184470 2041250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752529248.184471 2041250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-14 17:40:48.189022: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete - ready for data preparation\n"
     ]
    }
   ],
   "source": [
    "# Basic setup - add project src to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set project root and add src to path\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Enable autoreload for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Basic imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import the batch processing function directly\n",
    "from word2gm_fast.dataprep.pipeline import batch_prepare_training_data\n",
    "\n",
    "print(\"Setup complete - ready for data preparation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72a7c6",
   "metadata": {},
   "source": [
    "## Print Resource Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43483cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:40:52.170233: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<pre>SYSTEM RESOURCE SUMMARY\n",
       "=============================================\n",
       "Hostname: cm047.hpc.nyu.edu\n",
       "\n",
       "Job Allocation:\n",
       "   CPUs: 14\n",
       "   Memory: 125.0 GB\n",
       "   Partition: short\n",
       "   Job ID: 63723940\n",
       "   Node list: cm047\n",
       "\n",
       "Physical GPU Hardware:\n",
       "   No physical GPUs allocated to this job\n",
       "\n",
       "TensorFlow GPU Recognition:\n",
       "   TensorFlow can access 0 GPU(s)\n",
       "   Built with CUDA support: True\n",
       "=============================================</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import and run resource summary\n",
    "from word2gm_fast.utils.resource_summary import print_resource_summary\n",
    "\n",
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301858",
   "metadata": {},
   "source": [
    "## Prepare Corpora\n",
    "\n",
    "Here, we run the data-preparation pipeline from start to finish — reading preprocessed ngram corpora, generating all valid triplets, extracting the vocabulary, and saving the triplets and vocabulary as `tfrecord` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14da0c0",
   "metadata": {},
   "source": [
    "### Options for Data Preparation\n",
    "\n",
    "You can control which years are processed and how the batch preparation runs by adjusting the arguments to `batch_prepare_training_data`:\n",
    "\n",
    "**Ways to specify years:**\n",
    "- `year_range=\"2010\"` — Process a single year (e.g., only 2010).\n",
    "- `year_range=\"2010,2012,2015\"` — Process a comma-separated list of years.\n",
    "- `year_range=\"2010-2015\"` — Process a range of years, inclusive (2010 through 2015).\n",
    "- `year_range=\"2010,2012-2014,2016\"` — Combine individual years and ranges (2010, 2012, 2013, 2014, 2016).\n",
    "\n",
    "**Other options:**\n",
    "- `compress` — If `True`, output TFRecords are gzip-compressed. If `False`, output is uncompressed.\n",
    "- `show_progress` — If `True`, display a progress bar for each year.\n",
    "- `show_summary` — If `True`, print a summary of the processed data for each year.\n",
    "- `use_multiprocessing` — If `True`, process years in parallel using multiple CPU cores (recommended for large datasets).\n",
    "\n",
    "**TensorFlow Logging:**\n",
    "- TensorFlow logging is set to ERROR level to reduce verbose output\n",
    "- The pipeline still works normally, but with cleaner console output\n",
    "- Critical errors will still be displayed if they occur\n",
    "\n",
    "See the function docstring or source for more advanced options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39ae7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing refactored frequency scanner with modular approach...\n",
      "============================================================\n",
      "Step 1: Creating TextLineDataset from corpus...\n",
      "Dataset created from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/2019.txt\n",
      "\n",
      "Step 2: Scanning frequencies from dataset...\n",
      "Converting dataset to frequency table...\n",
      "  Processed 1,000,000 lines, collected 5,000,000 tokens\n",
      "  Processed 1,000,000 lines, collected 5,000,000 tokens\n",
      "  Processed 2,000,000 lines, collected 10,000,000 tokens\n",
      "  Processed 2,000,000 lines, collected 10,000,000 tokens\n",
      "  Processed 3,000,000 lines, collected 15,000,000 tokens\n",
      "  Processed 3,000,000 lines, collected 15,000,000 tokens\n",
      "  Processed 4,000,000 lines, collected 20,000,000 tokens\n",
      "  Processed 4,000,000 lines, collected 20,000,000 tokens\n"
     ]
    }
   ],
   "source": [
    "# Test the refactored frequency scanner with corpus_to_dataset\n",
    "from word2gm_fast.dataprep.corpus_to_dataset import make_dataset\n",
    "from word2gm_fast.dataprep.dataset_to_frequency import dataset_to_frequency\n",
    "\n",
    "# Test corpus path\n",
    "test_corpus_path = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/2019.txt\"\n",
    "\n",
    "print(\"Testing refactored frequency scanner with modular approach...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Create dataset from corpus using make_dataset (unpack the tuple)\n",
    "print(\"Step 1: Creating TextLineDataset from corpus...\")\n",
    "dataset, summary = make_dataset(test_corpus_path)\n",
    "print(f\"Dataset created from: {test_corpus_path}\")\n",
    "\n",
    "print(\"\\nStep 2: Scanning frequencies from dataset...\")\n",
    "# Step 2: Scan frequencies from the dataset\n",
    "frequency_table = dataset_to_frequency(dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODULAR FREQUENCY SCANNER TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total unique tokens: {len(frequency_table):,}\")\n",
    "\n",
    "# Show some examples\n",
    "if frequency_table:\n",
    "    print(\"\\nTop 20 most frequent tokens:\")\n",
    "    sorted_items = sorted(frequency_table.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i, (token, freq) in enumerate(sorted_items[:20]):\n",
    "        print(f\"  {i+1:2d}. '{token}' -> {freq:,} times\")\n",
    "    \n",
    "    print(f\"\\nLeast frequent tokens (showing last 10):\")\n",
    "    for i, (token, freq) in enumerate(sorted_items[-10:]):\n",
    "        print(f\"  {len(sorted_items)-9+i:2d}. '{token}' -> {freq:,} times\")\n",
    "    \n",
    "    # Check if UNK is present\n",
    "    unk_freq = frequency_table.get('UNK', 0)\n",
    "    if unk_freq > 0:\n",
    "        print(f\"\\n'UNK' token frequency: {unk_freq:,}\")\n",
    "    else:\n",
    "        print(\"\\n'UNK' token not found in frequency table\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ Modular approach working: make_dataset → dataset_to_frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b484e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
