{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180684c5",
   "metadata": {},
   "source": [
    "# Test Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f7e6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 09:01:31.210611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-05 09:01:31.227736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751720491.245777 4126438 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751720491.251079 4126438 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751720491.264863 4126438 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751720491.264887 4126438 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751720491.264889 4126438 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751720491.264890 4126438 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-05 09:01:31.269229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Autoreload enabled</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Project root: /scratch/edk202/word2gm-fast\n",
       "TensorFlow version: 2.19.0\n",
       "Device mode: GPU-enabled</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Testing environment ready!</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set project root directory and add `src` to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    " \n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import the notebook setup utilities\n",
    "from word2gm_fast.utils.notebook_setup import setup_testing_notebook, enable_autoreload, run_silent_subprocess\n",
    "\n",
    "# Enable mixed precision for GPU training\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Enable autoreload for development\n",
    "enable_autoreload()\n",
    "\n",
    "# Set up environment\n",
    "env = setup_testing_notebook(project_root=PROJECT_ROOT)\n",
    "\n",
    "# Extract commonly used modules for convenience\n",
    "tf = env['tensorflow']\n",
    "np = env['numpy']\n",
    "pd = env['pandas']\n",
    "print_resource_summary = env['print_resource_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51550892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>SYSTEM RESOURCE SUMMARY\n",
       "============================================================\n",
       "Hostname: cm032.hpc.nyu.edu\n",
       "\n",
       "Job Allocation:\n",
       "   CPUs: 4\n",
       "   Memory: 15.6 GB\n",
       "   Requested partitions: short\n",
       "   Running on: SSH failed: Host key verification failed.\n",
       "   Job ID: 63398538\n",
       "   Node list: cm032\n",
       "\n",
       "GPU Information:\n",
       "   Error: NVML Shared Library Not Found\n",
       "\n",
       "TensorFlow GPU Detection:\n",
       "   TensorFlow detects 0 GPU(s)\n",
       "   Built with CUDA: True\n",
       "============================================================</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48ee228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT:\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0 -- /ext3/miniforge3/envs/word2gm-fast2/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /scratch/edk202/word2gm-fast\n",
      "plugins: anyio-4.9.0, timeout-2.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 69 items\n",
      "\n",
      "tests/test_corpus_to_dataset.py::test_corpus_to_dataset \u001b[32mPASSED\u001b[0m\u001b[32m           [  1%]\u001b[0m\n",
      "tests/test_dataset_to_triplets.py::test_basic_triplet_generation \u001b[32mPASSED\u001b[0m\u001b[32m  [  2%]\u001b[0m\n",
      "tests/test_dataset_to_triplets.py::test_center_word_extraction \u001b[32mPASSED\u001b[0m\u001b[32m    [  4%]\u001b[0m\n",
      "tests/test_dataset_to_triplets.py::test_context_word_extraction \u001b[32mPASSED\u001b[0m\u001b[32m   [  5%]\u001b[0m\n",
      "tests/test_dataset_to_triplets.py::test_multiple_triplets_per_line \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_dataset_to_triplets.py::test_negative_sampling_range \u001b[32mPASSED\u001b[0m\u001b[32m   [  8%]\u001b[0m\n",
      "tests/test_dataset_to_triplets.py::test_no_triplets_with_unk_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_dataset_to_triplets.py::test_frequency_based_downsampling \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________ test_frequency_based_downsampling _______________________\u001b[0m\n",
      "\u001b[1m\u001b[31mtests/test_dataset_to_triplets.py\u001b[0m:135: in test_frequency_based_downsampling\n",
      "    \u001b[0mtriplets_ds_downsampled = build_skipgram_triplets(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31msrc/word2gm_fast/dataprep/dataset_to_triplets.py\u001b[0m:42: in build_skipgram_triplets\n",
      "    \u001b[0mfreq_tensor = tf.convert_to_tensor(frequencies, dtype=tf.float32)\u001b[90m\u001b[39;49;00m\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m:153: in error_handler\n",
      "    \u001b[0m\u001b[94mraise\u001b[39;49;00m e.with_traceback(filtered_tb) \u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m:108: in convert_to_eager_tensor\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ops.EagerTensor(value, ctx.device_name, dtype)\u001b[90m\u001b[39;49;00m\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   ValueError: Attempt to convert a value ({'UNK': 100.0, 'the': 1000.0, 'quick': 50.0, 'brown': 30.0, 'fox': 25.0, 'jumps': 20.0, 'over': 15.0, 'lazy': 10.0, 'dog': 8.0}) with an unsupported type (<class 'dict'>) to a Tensor.\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_dataset_to_triplets.py::\u001b[1mtest_frequency_based_downsampling\u001b[0m - ValueError: Attempt to convert a value ({'UNK': 100.0, 'the': 1000.0, 'quic...\n",
      "\u001b[31m!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m7 passed\u001b[0m\u001b[31m in 5.05s\u001b[0m\u001b[31m ==========================\u001b[0m\n",
      "\n",
      "\n",
      "Return code: 1\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run the updated test suite with verbose output\n",
    "result = subprocess.run([\n",
    "    'python', '-m', 'pytest', \n",
    "    'tests/',\n",
    "    '-v',  # Verbose output\n",
    "    '--tb=short',  # Short traceback format\n",
    "    '-x'  # Stop on first failure\n",
    "], capture_output=True, text=True, cwd=PROJECT_ROOT)\n",
    "\n",
    "print(\"STDOUT:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"\\nSTDERR:\")\n",
    "    print(result.stderr)\n",
    "    \n",
    "print(f\"\\nReturn code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d536e599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing updated IO modules...\n",
      "IO Module Tests:\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0 -- /ext3/miniforge3/envs/word2gm-fast2/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /scratch/edk202/word2gm-fast\n",
      "plugins: anyio-4.9.0, timeout-2.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 35 items\n",
      "\n",
      "tests/test_tfrecord_io.py::test_write_and_load_triplets_uncompressed \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_tfrecord_io.py::test_write_and_load_triplets_compressed \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_tfrecord_io.py::test_parse_triplet_example \u001b[32mPASSED\u001b[0m\u001b[32m             [  8%]\u001b[0m\n",
      "tests/test_tfrecord_io.py::test_write_and_load_vocab_uncompressed \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_tfrecord_io.py::test_write_and_load_vocab_with_frequencies \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_tfrecord_io.py::test_write_vocab_without_frequencies \u001b[32mPASSED\u001b[0m\u001b[32m   [ 17%]\u001b[0m\n",
      "tests/test_tfrecord_io.py::test_write_and_load_vocab_compressed \u001b[32mPASSED\u001b[0m\u001b[32m   [ 20%]\u001b[0m\n",
      "tests/test_tfrecord_io.py::test_parse_vocab_example \u001b[32mPASSED\u001b[0m\u001b[32m               [ 22%]\u001b[0m\n",
      "tests/test_tfrecord_io.py::test_parse_vocab_example_default_frequency \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_tfrecord_io.py::test_save_and_load_pipeline_artifacts \u001b[32mPASSED\u001b[0m\u001b[32m  [ 28%]\u001b[0m\n",
      "tests/test_tfrecord_io.py::test_create_index_to_token_table \u001b[32mPASSED\u001b[0m\u001b[32m       [ 31%]\u001b[0m\n",
      "tests/test_io_integration.py::test_vocab_tables_roundtrip_consistency \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_io_integration.py::test_frequency_preservation_through_pipeline \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_io_integration.py::test_complete_artifacts_pipeline \u001b[32mPASSED\u001b[0m\u001b[32m    [ 40%]\u001b[0m\n",
      "tests/test_io_integration.py::test_artifacts_compression_consistency \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_io_integration.py::test_artifacts_auto_detection \u001b[32mPASSED\u001b[0m\u001b[32m       [ 45%]\u001b[0m\n",
      "tests/test_io_integration.py::test_error_handling_missing_files \u001b[32mPASSED\u001b[0m\u001b[32m   [ 48%]\u001b[0m\n",
      "tests/test_pipeline.py::test_get_corpus_years \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 51%]\u001b[0m\n",
      "tests/test_pipeline.py::test_get_corpus_years_empty_directory \u001b[32mPASSED\u001b[0m\u001b[32m     [ 54%]\u001b[0m\n",
      "tests/test_pipeline.py::test_get_corpus_years_nonexistent_directory \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_pipeline.py::test_parse_year_range \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 60%]\u001b[0m\n",
      "tests/test_pipeline.py::test_parse_year_range_invalid \u001b[32mPASSED\u001b[0m\u001b[32m             [ 62%]\u001b[0m\n",
      "tests/test_pipeline.py::test_prepare_training_data_success \u001b[31mFAILED\u001b[0m\u001b[31m        [ 65%]\u001b[0m\n",
      "tests/test_pipeline.py::test_prepare_training_data_file_not_found \u001b[32mPASSED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n",
      "tests/test_pipeline.py::test_prepare_training_data_invalid_directory \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/test_pipeline.py::test_batch_prepare_training_data_auto_discovery \u001b[32mPASSED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/test_pipeline.py::test_batch_prepare_training_data_specific_years \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_pipeline.py::test_batch_prepare_training_data_year_range \u001b[32mPASSED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/test_pipeline.py::test_batch_prepare_training_data_conflicting_params \u001b[32mPASSED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/test_pipeline.py::test_batch_prepare_training_data_missing_files \u001b[32mPASSED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/test_pipeline.py::test_detect_cluster_resources \u001b[32mPASSED\u001b[0m\u001b[31m             [ 88%]\u001b[0m\n",
      "tests/test_pipeline.py::test_detect_cluster_resources_slurm \u001b[32mPASSED\u001b[0m\u001b[31m       [ 91%]\u001b[0m\n",
      "tests/test_pipeline.py::test_get_safe_worker_count \u001b[32mPASSED\u001b[0m\u001b[31m                [ 94%]\u001b[0m\n",
      "tests/test_pipeline.py::test_process_single_year_helper \u001b[32mPASSED\u001b[0m\u001b[31m           [ 97%]\u001b[0m\n",
      "tests/test_pipeline.py::test_process_single_year_helper_error \u001b[32mPASSED\u001b[0m\u001b[31m     [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________ test_prepare_training_data_success ______________________\u001b[0m\n",
      "\u001b[1m\u001b[31m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/unittest/mock.py\u001b[0m:1393: in patched\n",
      "    \u001b[0m\u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.decoration_helper(patched,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/contextlib.py\u001b[0m:137: in __enter__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mnext\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.gen)\u001b[90m\u001b[39;49;00m\n",
      "           ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/unittest/mock.py\u001b[0m:1375: in decoration_helper\n",
      "    \u001b[0marg = exit_stack.enter_context(patching)\u001b[90m\u001b[39;49;00m\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/contextlib.py\u001b[0m:526: in enter_context\n",
      "    \u001b[0mresult = _enter(cm)\u001b[90m\u001b[39;49;00m\n",
      "             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/unittest/mock.py\u001b[0m:1467: in __enter__\n",
      "    \u001b[0moriginal, local = \u001b[96mself\u001b[39;49;00m.get_original()\u001b[90m\u001b[39;49;00m\n",
      "                      ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/unittest/mock.py\u001b[0m:1437: in get_original\n",
      "    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   AttributeError: <module 'src.word2gm_fast.dataprep.pipeline' from '/scratch/edk202/word2gm-fast/src/word2gm_fast/dataprep/pipeline.py'> does not have the attribute 'write_vocab_to_tfrecord'\u001b[0m\n",
      "\u001b[31m\u001b[1m_________________ test_prepare_training_data_invalid_directory _________________\u001b[0m\n",
      "\u001b[1m\u001b[31mtests/test_pipeline.py\u001b[0m:168: in test_prepare_training_data_invalid_directory\n",
      "    \u001b[0mprepare_training_data(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31msrc/word2gm_fast/dataprep/pipeline.py\u001b[0m:129: in prepare_training_data\n",
      "    \u001b[0m\u001b[94mraise\u001b[39;49;00m \u001b[96mFileNotFoundError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCorpus file not found: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcorpus_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   FileNotFoundError: Corpus file not found: /nonexistent/directory/test.txt\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_pipeline.py::\u001b[1mtest_prepare_training_data_success\u001b[0m - AttributeError: <module 'src.word2gm_fast.dataprep.pipeline' from '/scratch...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_pipeline.py::\u001b[1mtest_prepare_training_data_invalid_directory\u001b[0m - FileNotFoundError: Corpus file not found: /nonexistent/directory/test.txt\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m33 passed\u001b[0m\u001b[31m in 4.56s\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "\n",
      "IO Tests Return code: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the new IO modules specifically\n",
    "print(\"Testing updated IO modules...\")\n",
    "\n",
    "# Run tests for the new modular IO structure\n",
    "io_tests = subprocess.run([\n",
    "    'python', '-m', 'pytest', \n",
    "    'tests/test_tfrecord_io.py',\n",
    "    'tests/test_io_integration.py', \n",
    "    'tests/test_pipeline.py',\n",
    "    '-v',\n",
    "    '--tb=short'\n",
    "], capture_output=True, text=True, cwd=PROJECT_ROOT)\n",
    "\n",
    "print(\"IO Module Tests:\")\n",
    "print(io_tests.stdout)\n",
    "if io_tests.stderr:\n",
    "    print(\"\\nErrors:\")\n",
    "    print(io_tests.stderr)\n",
    "    \n",
    "print(f\"IO Tests Return code: {io_tests.returncode}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af588a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual verification of new IO modules...\n",
      "✓ All new IO modules imported successfully\n",
      "✓ Old tfrecord_io module properly removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-63398538/ipykernel_4126438/2922263015.py:14: DeprecationWarning: The tfrecord_io module has been deprecated and split into focused modules in the io package. Please update your imports to use the new modules directly: word2gm_fast.io.vocab, word2gm_fast.io.triplets, word2gm_fast.io.tables, word2gm_fast.io.artifacts\n",
      "  from word2gm_fast.utils.tfrecord_io import write_vocab_to_tfrecord as old_write_vocab\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Writing vocabulary TFRecord to: /state/partition1/job-63398538/tmpw8ngy12q.tfrecord"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Vocabulary write complete. Words written: 3</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading token-to-index vocabulary TFRecord from: /state/partition1/job-63398538/tmpw8ngy12q.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Frequency-enabled vocab I/O working: 'test' -> 1\n",
      "✓ Manual verification passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 09:02:44.503294: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 134217728\n",
      "2025-07-05 09:02:44.507590: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Manual verification of key IO functionality\n",
    "print(\"Manual verification of new IO modules...\")\n",
    "\n",
    "try:\n",
    "    # Test importing the new modular structure\n",
    "    from word2gm_fast.io.vocab import write_vocab_to_tfrecord, parse_vocab_example\n",
    "    from word2gm_fast.io.triplets import write_triplets_to_tfrecord, load_triplets_from_tfrecord\n",
    "    from word2gm_fast.io.tables import create_token_to_index_table, create_index_to_token_table\n",
    "    from word2gm_fast.io.artifacts import save_pipeline_artifacts, load_pipeline_artifacts\n",
    "    print(\"✓ All new IO modules imported successfully\")\n",
    "    \n",
    "    # Test that the old module is deprecated\n",
    "    try:\n",
    "        from word2gm_fast.utils.tfrecord_io import write_vocab_to_tfrecord as old_write_vocab\n",
    "        print(\"⚠ Old tfrecord_io module still accessible (shows deprecation warning)\")\n",
    "    except ImportError:\n",
    "        print(\"✓ Old tfrecord_io module properly removed\")\n",
    "    \n",
    "    # Quick functionality test\n",
    "    vocab_words = [\"UNK\", \"test\", \"words\"]\n",
    "    vocab_table = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(\n",
    "            keys=tf.constant(vocab_words),\n",
    "            values=tf.constant([0, 1, 2], dtype=tf.int64)\n",
    "        ),\n",
    "        default_value=0\n",
    "    )\n",
    "    \n",
    "    # Test vocab with frequencies\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(suffix='.tfrecord', delete=False) as tmp:\n",
    "        frequencies = {\"UNK\": 100.0, \"test\": 50.0, \"words\": 25.0}\n",
    "        write_vocab_to_tfrecord(vocab_table, tmp.name, frequencies=frequencies)\n",
    "        \n",
    "        # Test loading with table creation\n",
    "        token_to_idx = create_token_to_index_table(tmp.name)\n",
    "        test_idx = token_to_idx.lookup(tf.constant(\"test\")).numpy()\n",
    "        print(f\"✓ Frequency-enabled vocab I/O working: 'test' -> {test_idx}\")\n",
    "        \n",
    "        # Clean up\n",
    "        import os\n",
    "        os.unlink(tmp.name)\n",
    "    \n",
    "    print(\"✓ Manual verification passed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Manual verification failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aad5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPREHENSIVE IO MODULE TEST\n",
      "============================================================\n",
      "✓ All new IO modules imported successfully\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Writing vocabulary TFRecord to: /state/partition1/job-63398538/tmp1q02mglm.tfrecord"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Vocabulary write complete. Words written: 5</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading token-to-index vocabulary TFRecord from: /state/partition1/job-63398538/tmp1q02mglm.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 09:05:48.670182: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading index-to-token vocab TFRecord from: /state/partition1/job-63398538/tmp1q02mglm.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ IO module test failed: Dtype of argument `keys` must be <dtype: 'int64'>, received: <dtype: 'int32'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/state/partition1/job-63398538/ipykernel_4126438/1575288815.py\", line 57, in <module>\n",
      "    back_token = idx_to_token.lookup(tf.constant([test_idx])).numpy()[0].decode('utf-8')\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ext3/miniforge3/envs/word2gm-fast2/lib/python3.12/site-packages/tensorflow/python/ops/lookup_ops.py\", line 253, in lookup\n",
      "    raise TypeError(f\"Dtype of argument `keys` must be {self._key_dtype}, \"\n",
      "TypeError: Dtype of argument `keys` must be <dtype: 'int64'>, received: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive test of all new IO modules\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE IO MODULE TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Test 1: Import all new modules\n",
    "    from word2gm_fast.io.vocab import (\n",
    "        write_vocab_to_tfrecord, \n",
    "        parse_vocab_example\n",
    "    )\n",
    "    from word2gm_fast.io.triplets import (\n",
    "        write_triplets_to_tfrecord, \n",
    "        load_triplets_from_tfrecord\n",
    "    )\n",
    "    from word2gm_fast.io.tables import (\n",
    "        create_token_to_index_table, \n",
    "        create_index_to_token_table\n",
    "    )\n",
    "    from word2gm_fast.io.artifacts import (\n",
    "        save_pipeline_artifacts, \n",
    "        load_pipeline_artifacts\n",
    "    )\n",
    "    print(\"✓ All new IO modules imported successfully\")\n",
    "    \n",
    "    # Test 2: Create sample data\n",
    "    vocab_words = [\"UNK\", \"the\", \"quick\", \"brown\", \"fox\"]\n",
    "    vocab_indices = list(range(len(vocab_words)))\n",
    "    vocab_frequencies = {word: 100.0 - i*10 for i, word in enumerate(vocab_words)}\n",
    "    \n",
    "    # Create vocab table\n",
    "    vocab_table = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(\n",
    "            keys=tf.constant(vocab_words),\n",
    "            values=tf.constant(vocab_indices, dtype=tf.int64)\n",
    "        ),\n",
    "        default_value=0\n",
    "    )\n",
    "    \n",
    "    # Test 3: Vocab I/O with frequencies\n",
    "    import tempfile\n",
    "    import os\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.tfrecord', delete=False) as vocab_file:\n",
    "        vocab_path = vocab_file.name\n",
    "        \n",
    "    # Write vocab with frequencies\n",
    "    write_vocab_to_tfrecord(vocab_table, vocab_path, frequencies=vocab_frequencies)\n",
    "    \n",
    "    # Read back and create tables\n",
    "    token_to_idx = create_token_to_index_table(vocab_path)\n",
    "    idx_to_token = create_index_to_token_table(vocab_path)\n",
    "    \n",
    "    # Test lookups\n",
    "    test_token = \"quick\"\n",
    "    test_idx = token_to_idx.lookup(tf.constant(test_token)).numpy()\n",
    "    back_token = idx_to_token.lookup(tf.constant([test_idx], dtype=tf.int64)).numpy()[0].decode('utf-8')\n",
    "    \n",
    "    print(f\"✓ Vocab I/O test: '{test_token}' -> {test_idx} -> '{back_token}'\")\n",
    "    \n",
    "    # Test 4: Triplet I/O\n",
    "    with tempfile.NamedTemporaryFile(suffix='.tfrecord', delete=False) as triplet_file:\n",
    "        triplet_path = triplet_file.name\n",
    "        \n",
    "    # Create sample triplets\n",
    "    sample_triplets = [\n",
    "        (1, 2, 3),  # (target, context, negative)\n",
    "        (2, 3, 4),\n",
    "        (3, 4, 1),\n",
    "    ]\n",
    "    \n",
    "    # Write triplets\n",
    "    write_triplets_to_tfrecord(sample_triplets, triplet_path)\n",
    "    \n",
    "    # Load triplets back\n",
    "    loaded_triplets = list(load_triplets_from_tfrecord(triplet_path))\n",
    "    \n",
    "    print(f\"✓ Triplet I/O test: wrote {len(sample_triplets)}, loaded {len(loaded_triplets)}\")\n",
    "    \n",
    "    # Test 5: Artifact I/O\n",
    "    with tempfile.NamedTemporaryFile(suffix='.gz', delete=False) as artifact_file:\n",
    "        artifact_path = artifact_file.name\n",
    "        \n",
    "    # Create sample artifacts\n",
    "    artifacts = {\n",
    "        'vocab_size': len(vocab_words),\n",
    "        'total_tokens': sum(vocab_frequencies.values()),\n",
    "        'model_config': {'embedding_dim': 128, 'epochs': 10},\n",
    "        'metadata': {'version': '1.0', 'timestamp': '2025-01-01'}\n",
    "    }\n",
    "    \n",
    "    # Save and load artifacts\n",
    "    save_pipeline_artifacts(artifacts, artifact_path)\n",
    "    loaded_artifacts = load_pipeline_artifacts(artifact_path)\n",
    "    \n",
    "    print(f\"✓ Artifact I/O test: saved {len(artifacts)} items, loaded {len(loaded_artifacts)} items\")\n",
    "    \n",
    "    # Test 6: Verify artifact contents\n",
    "    assert loaded_artifacts['vocab_size'] == len(vocab_words)\n",
    "    assert loaded_artifacts['model_config']['embedding_dim'] == 128\n",
    "    print(\"✓ Artifact contents verified\")\n",
    "    \n",
    "    # Test 7: Verify the old module shows deprecation warning\n",
    "    try:\n",
    "        from word2gm_fast.utils.tfrecord_io import write_vocab_to_tfrecord as old_write_vocab\n",
    "        print(\"✓ Old module still accessible (with deprecation warning)\")\n",
    "    except ImportError:\n",
    "        print(\"✓ Old module completely removed\")\n",
    "    \n",
    "    # Clean up\n",
    "    os.unlink(vocab_path)\n",
    "    os.unlink(triplet_path) \n",
    "    os.unlink(artifact_path)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ALL IO MODULE TESTS PASSED!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ IO module test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
