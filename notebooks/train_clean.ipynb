{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683781ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"  # Optional, may help with fragmentation\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not set memory growth: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7fd872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set project root directory and add `src` to path\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from word2gm_fast.models.word2gm_model import Word2GMModel\n",
    "from word2gm_fast.models.config import Word2GMConfig\n",
    "from word2gm_fast.training.notebook_training import run_notebook_training\n",
    "from word2gm_fast.utils.tfrecord_io import load_pipeline_artifacts\n",
    "from word2gm_fast.utils.resource_summary import print_resource_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec376e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>SYSTEM RESOURCE SUMMARY\n",
       "============================================================\n",
       "Hostname: cm020.hpc.nyu.edu\n",
       "\n",
       "Job Allocation:\n",
       "   CPUs: 14\n",
       "   Memory: 125.0 GB\n",
       "   Requested partitions: short,cs,cm,cpu_a100_2,cpu_a100_1,cpu_gpu\n",
       "   Running on: SSH failed: Host key verification failed.\n",
       "   Job ID: 63372384\n",
       "   Node list: cm020\n",
       "\n",
       "GPU Information:\n",
       "   Error: NVML Shared Library Not Found\n",
       "\n",
       "TensorFlow GPU Detection:\n",
       "   TensorFlow detects 0 GPU(s)\n",
       "   Built with CUDA: True\n",
       "============================================================</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a277420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>Loading pipeline artifacts from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1940_artifacts</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading token-to-index vocabulary TFRecord from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1940_artifacts/vocab.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 04:32:51.185089: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading index-to-token vocab TFRecord from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1940_artifacts/vocab.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading triplet TFRecord from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1940_artifacts/triplets.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Triplet TFRecord loaded and parsed</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>All artifacts loaded successfully!</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab_size: 42401\n"
     ]
    }
   ],
   "source": [
    "# Data loading and pipeline setup\n",
    "from word2gm_fast.utils.tfrecord_io import load_pipeline_artifacts\n",
    "\n",
    "# Define paths for your corpus artifacts and output\n",
    "dataset_artifacts_dir = (\n",
    "    '/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/'\n",
    "    '1940_artifacts'\n",
    ")\n",
    "output_dir = '/scratch/edk202/word2gm-fast/output/test_corpus'\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set TensorBoard log directory\n",
    "tensorboard_log_dir = output_dir + '/tensorboard'\n",
    "\n",
    "# Load pipeline artifacts (vocab, triplets, etc.)\n",
    "artifacts = load_pipeline_artifacts(dataset_artifacts_dir)\n",
    "token_to_index_table = artifacts['token_to_index_table']\n",
    "index_to_token_table = artifacts['index_to_token_table']\n",
    "triplets_ds = artifacts['triplets_ds']\n",
    "vocab_size = artifacts['vocab_size']\n",
    "\n",
    "# Build the dataset pipeline: cache -> shuffle -> batch -> prefetch\n",
    "triplets_ds = triplets_ds.cache()\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 10\n",
    "triplets_ds = triplets_ds.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "triplets_ds = triplets_ds.batch(BATCH_SIZE)\n",
    "triplets_ds = triplets_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f'Loaded vocab_size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bfff30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for token 'king': 20794\n",
      "Token for index 20794: king\n"
     ]
    }
   ],
   "source": [
    "# Example: Query the token_to_index_table and index_to_token_table\n",
    "test_token = 'king'\n",
    "test_index = 20794\n",
    "\n",
    "# Query token to index\n",
    "token_tensor = tf.constant([test_token])\n",
    "index_result = token_to_index_table.lookup(token_tensor).numpy()[0]\n",
    "print(f\"Index for token '{test_token}':\", index_result)\n",
    "\n",
    "# Query index to token\n",
    "index_tensor = tf.constant([test_index], dtype=tf.int64)\n",
    "token_result = index_to_token_table.lookup(index_tensor).numpy()[0].decode('utf-8')\n",
    "print(f\"Token for index {test_index}:\", token_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e5b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample of 50 triplets (anchor, pos, neg) from a single batch:\n",
      " 1: (anchor=319, pos=16745, neg=1102)\n",
      " 2: (anchor=1458, pos=22267, neg=14304)\n",
      " 3: (anchor=1566, pos=26393, neg=32382)\n",
      " 4: (anchor=1932, pos=25375, neg=16498)\n",
      " 5: (anchor=3709, pos=26393, neg=35175)\n",
      " 6: (anchor=1463, pos=26393, neg=11551)\n",
      " 7: (anchor=745, pos=26393, neg=3275)\n",
      " 8: (anchor=1458, pos=33724, neg=19295)\n",
      " 9: (anchor=1458, pos=26393, neg=32624)\n",
      "10: (anchor=3709, pos=26393, neg=27943)\n",
      "11: (anchor=1884, pos=21884, neg=4281)\n",
      "12: (anchor=3816, pos=41263, neg=39186)\n",
      "13: (anchor=1884, pos=26393, neg=2283)\n",
      "14: (anchor=1954, pos=26393, neg=20039)\n",
      "15: (anchor=3372, pos=26393, neg=26668)\n",
      "16: (anchor=1884, pos=26393, neg=10744)\n",
      "17: (anchor=4051, pos=26393, neg=18059)\n",
      "18: (anchor=2638, pos=26393, neg=28529)\n",
      "19: (anchor=756, pos=26393, neg=26104)\n",
      "20: (anchor=1458, pos=26393, neg=39008)\n",
      "21: (anchor=1458, pos=26393, neg=15866)\n",
      "22: (anchor=1884, pos=33760, neg=25919)\n",
      "23: (anchor=1932, pos=42000, neg=30216)\n",
      "24: (anchor=1458, pos=21836, neg=18509)\n",
      "25: (anchor=3709, pos=26393, neg=35920)\n",
      "26: (anchor=2638, pos=28701, neg=20280)\n",
      "27: (anchor=1458, pos=1117, neg=26747)\n",
      "28: (anchor=5612, pos=26393, neg=19507)\n",
      "29: (anchor=1458, pos=2527, neg=1055)\n",
      "30: (anchor=1555, pos=26393, neg=21770)\n",
      "31: (anchor=5209, pos=26393, neg=9318)\n",
      "32: (anchor=1458, pos=26393, neg=14911)\n",
      "33: (anchor=5145, pos=26393, neg=41502)\n",
      "34: (anchor=1458, pos=26393, neg=20960)\n",
      "35: (anchor=5384, pos=12883, neg=35555)\n",
      "36: (anchor=1884, pos=26393, neg=32614)\n",
      "37: (anchor=2534, pos=26393, neg=26187)\n",
      "38: (anchor=1458, pos=26393, neg=5853)\n",
      "39: (anchor=5384, pos=26393, neg=20229)\n",
      "40: (anchor=1458, pos=22857, neg=29119)\n",
      "41: (anchor=2791, pos=26393, neg=34321)\n",
      "42: (anchor=1402, pos=31358, neg=5177)\n",
      "43: (anchor=1086, pos=13929, neg=20431)\n",
      "44: (anchor=1458, pos=319, neg=3340)\n",
      "45: (anchor=1463, pos=26393, neg=16361)\n",
      "46: (anchor=430, pos=26393, neg=29422)\n",
      "47: (anchor=4565, pos=1018, neg=24232)\n",
      "48: (anchor=4031, pos=26393, neg=31372)\n",
      "49: (anchor=4503, pos=26393, neg=23560)\n",
      "50: (anchor=2820, pos=26393, neg=6802)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 04:39:55.621276: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Print a random sample of 50 triplets from a single batch of the current corpus, showing both indices and tokens\n",
    "import random\n",
    "\n",
    "# Take a single batch from the dataset\n",
    "for batch in triplets_ds.take(1):\n",
    "    # If batch is a tuple of tensors (anchor, pos, neg), stack and transpose to shape (batch_size, 3)\n",
    "    if isinstance(batch, tuple) and len(batch) == 3:\n",
    "        anchor, pos, neg = [t.numpy() for t in batch]\n",
    "        triplets_batch = list(zip(anchor, pos, neg))\n",
    "    else:\n",
    "        # If batch is a single tensor of shape (batch_size, 3)\n",
    "        triplets_batch = batch.numpy()\n",
    "    break\n",
    "\n",
    "sample_size = min(50, len(triplets_batch))\n",
    "sampled_indices = random.sample(range(len(triplets_batch)), sample_size)\n",
    "sampled_triplets = [triplets_batch[i] for i in sampled_indices]\n",
    "\n",
    "def idx_to_token(idx):\n",
    "    idx_tensor = tf.constant([idx], dtype=tf.int64)\n",
    "    token = index_to_token_table.lookup(idx_tensor).numpy()[0].decode('utf-8')\n",
    "    return token\n",
    "\n",
    "print(f\"Random sample of {sample_size} triplets from a single batch:\")\n",
    "print(\"Idx: (anchor, pos, neg)\\tTokens: (anchor, pos, neg)\")\n",
    "for i, triplet in enumerate(sampled_triplets):\n",
    "    anchor, pos, neg = triplet\n",
    "    anchor_token = idx_to_token(anchor)\n",
    "    pos_token = idx_to_token(pos)\n",
    "    neg_token = idx_to_token(neg)\n",
    "    print(f\"{i+1:2d}: ({anchor}, {pos}, {neg})\\t({anchor_token}, {pos_token}, {neg_token})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e3ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a69940b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Word2GMConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=200,\n",
    "    num_mixtures=1,\n",
    "    spherical=True,\n",
    "    norm_cap=20.0,         # Increased\n",
    "    lower_sig=0.01,        # Lowered\n",
    "    upper_sig=2.0,         # Raised\n",
    "    var_scale=0.1,         # Increased\n",
    "    loss_epsilon=1e-8,\n",
    "    wout=True,\n",
    "    max_pe=False,\n",
    ")\n",
    "\n",
    "run_notebook_training(\n",
    "    training_dataset=triplets_ds,\n",
    "    save_path=output_dir,\n",
    "    vocab_size=config.vocab_size,\n",
    "    embedding_size=config.embedding_size,\n",
    "    num_mixtures=config.num_mixtures,\n",
    "    spherical=config.spherical,\n",
    "    learning_rate=1.0,\n",
    "    epochs=30,\n",
    "    adagrad=True,\n",
    "    normclip=True,\n",
    "    norm_cap=config.norm_cap,\n",
    "    lower_sig=config.lower_sig,\n",
    "    upper_sig=config.upper_sig,\n",
    "    var_scale=config.var_scale,\n",
    "    loss_epsilon=config.loss_epsilon,\n",
    "    wout=config.wout,\n",
    "    tensorboard_log_path=tensorboard_log_dir,\n",
    "    monitor_interval=0.5,\n",
    "    profile=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a69940b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the small dataset to test for overfitting\n",
    "overfit_config = Word2GMConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=100,  # smaller for faster overfit\n",
    "    num_mixtures=1,\n",
    "    spherical=True,\n",
    "    norm_cap=100.0,      # very loose\n",
    "    lower_sig=1e-4,      # very loose\n",
    "    upper_sig=10.0,      # very loose\n",
    "    var_scale=0.0001,       # no regularization\n",
    "    loss_epsilon=1e-8,\n",
    "    wout=True,\n",
    "    max_pe=False,\n",
    "    # other params as needed\n",
    " )\n",
    "run_notebook_training(\n",
    "    training_dataset=small_triplets_ds,\n",
    "    save_path=output_dir + '/overfit',\n",
    "    vocab_size=overfit_config.vocab_size,\n",
    "    embedding_size=overfit_config.embedding_size,\n",
    "    num_mixtures=overfit_config.num_mixtures,\n",
    "    spherical=overfit_config.spherical,\n",
    "    learning_rate=1.0,\n",
    "    epochs=100,\n",
    "    adagrad=True,\n",
    "    normclip=False,\n",
    "    norm_cap=overfit_config.norm_cap,\n",
    "    lower_sig=overfit_config.lower_sig,\n",
    "    upper_sig=overfit_config.upper_sig,\n",
    "    var_scale=overfit_config.var_scale,\n",
    "    loss_epsilon=overfit_config.loss_epsilon,\n",
    "    wout=overfit_config.wout,\n",
    "    tensorboard_log_path=output_dir + '/overfit/tensorboard',\n",
    "    monitor_interval=0.2,\n",
    "    profile=False,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5178472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard in the notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $tensorboard_log_dir --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "187826d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest neighbors for a given word using Word2GMModel and vocab_list\n",
    "model = Word2GMModel(config)\n",
    "\n",
    "# Build the model by calling it on a dummy input (tuple of three tensors)\n",
    "dummy_input = (\n",
    "    tf.zeros([1], dtype=tf.int32),  # word_ids\n",
    "    tf.zeros([1], dtype=tf.int32),  # pos_ids\n",
    "    tf.zeros([1], dtype=tf.int32),  # neg_ids\n",
    ")\n",
    "model(dummy_input)\n",
    "\n",
    "model.load_weights(output_dir + '/model_weights_epoch30.weights.h5')\n",
    "\n",
    "# Choose a query word and get its index\n",
    "query_word = 'good'  # Change this to any word in your vocab\n",
    "try:\n",
    "    query_idx = vocab_list.index(query_word)\n",
    "except ValueError:\n",
    "    raise ValueError(f'Word \"{query_word}\" not found in vocab_list.')\n",
    "\n",
    "# Get nearest neighbor indices (returns indices, distances or a list of (index, distance) pairs)\n",
    "result = model.get_nearest_neighbors(query_idx, k=10)\n",
    "print(\"Result type:\", type(result))\n",
    "print(\"Result:\", result)\n",
    "\n",
    "# Try to unpack if possible, else treat as list of pairs\n",
    "try:\n",
    "    neighbor_indices, neighbor_distances = result\n",
    "    neighbors = [(vocab_list[i], float(d)) for i, d in zip(neighbor_indices, neighbor_distances)]\n",
    "except Exception:\n",
    "    neighbors = [(vocab_list[i], float(d)) for i, d in result]\n",
    "\n",
    "print(f'Nearest neighbors for \"{query_word}\":')\n",
    "for word, dist in neighbors:\n",
    "    print(f'{word}\\t{dist:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
