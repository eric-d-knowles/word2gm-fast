{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683781ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set project root and add src to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec376e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752860241.628617 4191902 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752860241.696337 4191902 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752860242.430524 4191902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752860242.430549 4191902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752860242.430551 4191902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752860242.430552 4191902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<pre>SYSTEM RESOURCE SUMMARY\n",
       "=============================================\n",
       "Hostname: gr035.hpc.nyu.edu\n",
       "\n",
       "Job Allocation:\n",
       "   CPUs: 14\n",
       "   Memory: 125.0 GB\n",
       "   Partition: rtx8000\n",
       "   Job ID: 63887006\n",
       "   Node list: gr035\n",
       "\n",
       "Physical GPU Hardware:\n",
       "   Physical GPUs available: 1\n",
       "   GPU 0: Quadro RTX 8000\n",
       "      Memory: 0.5/45.0 GB (44.5 GB free)\n",
       "      Temperature: 27Â°C\n",
       "      Utilization: GPU 0%, Memory 0%\n",
       "\n",
       "TensorFlow GPU Recognition:\n",
       "   TensorFlow can access 1 GPU(s)\n",
       "      /physical_device:GPU:0\n",
       "   Built with CUDA support: True\n",
       "=============================================</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print resource summary\n",
    "from word2gm_fast.utils.resource_summary import print_resource_summary\n",
    "\n",
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13750b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for your corpus artifacts and output\n",
    "dataset_artifacts_dir = (\n",
    "    '/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/'\n",
    "    '1850_artifacts'\n",
    ")\n",
    "output_dir = '/scratch/edk202/word2gm-fast/output/test_corpus'\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set TensorBoard log directory\n",
    "tensorboard_log_dir = output_dir + '/tensorboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c65a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-family: monospace; font-size: 120%; font-weight: normal;'>Loading pipeline artifacts from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1850_artifacts</span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='font-family: monospace; font-size: 120%; font-weight: normal;'>Loading token-to-index vocabulary TFRecord from:<br>&nbsp;&nbsp;/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1850_artifacts/vocab.tfrecord</span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752860287.860799 4191902 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 44333 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='font-family: monospace; font-size: 120%; font-weight: normal;'>Token-to-index lookup table created successfully.<br>Table contains 33668 tokens. Processing time: 1.91s</span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='font-family: monospace; font-size: 120%; font-weight: normal;'>Loading index-to-token vocab TFRecord from:<br>&nbsp;&nbsp;/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1850_artifacts/vocab.tfrecord</span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='font-family: monospace; font-size: 120%; font-weight: normal;'>Index-to-token lookup table created successfully.<br>Table contains 33668 tokens. Processing time: 0.29s</span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='font-family: monospace; font-size: 120%; font-weight: normal;'>All artifacts loaded successfully!</span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from word2gm_fast.io.artifacts import load_pipeline_artifacts\n",
    "\n",
    "# Load pipeline artifacts (vocab, triplets, etc.)\n",
    "artifacts = load_pipeline_artifacts(dataset_artifacts_dir)\n",
    "token_to_index_table = artifacts['token_to_index_table']\n",
    "index_to_token_table = artifacts['index_to_token_table']\n",
    "triplets_ds = artifacts['triplets_ds']\n",
    "vocab_size = artifacts['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a277420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2gm_fast.utils.tf_silence import import_tf_quietly\n",
    "\n",
    "tf = import_tf_quietly()\n",
    "\n",
    "# Build the dataset pipeline: cache -> shuffle -> batch -> prefetch\n",
    "triplets_ds = triplets_ds.cache()\n",
    "BATCH_SIZE = 1024\n",
    "SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 10\n",
    "triplets_ds = triplets_ds.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "triplets_ds = triplets_ds.batch(BATCH_SIZE)\n",
    "triplets_ds = triplets_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea708dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $tensorboard_log_dir --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0db78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Word2GM training...\n",
      "Vocab size: 33668\n",
      "Output: /scratch/edk202/word2gm-fast/output/test_corpus\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Word2GM Training Hyperparameters:**\n",
       "\n",
       "| Parameter         | Value                |\n",
       "|-------------------|----------------------|\n",
       "| Vocab size        | `33668`  |\n",
       "| Embedding size    | `200` |\n",
       "| Mixtures          | `1` |\n",
       "| Spherical         | `True`   |\n",
       "| Learning rate     | `0.1` |\n",
       "| Epochs            | `30` |\n",
       "| Adagrad           | `True`     |\n",
       "| Normclip          | `True`    |\n",
       "| Norm cap          | `10.0`    |\n",
       "| Lower sigma       | `0.05`   |\n",
       "| Upper sigma       | `1.5`   |\n",
       "| Wout              | `True`        |\n",
       "| Var scale         | `0.05`   |\n",
       "| Loss epsilon      | `1e-08`|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Epoch 1 finished. Loss:** `0.550286`  | **Duration:** `18.38` seconds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Epoch 2 finished. Loss:** `0.344627`  | **Duration:** `10.15` seconds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Epoch 3 finished. Loss:** `0.301065`  | **Duration:** `10.09` seconds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Epoch 4 finished. Loss:** `0.275225`  | **Duration:** `10.08` seconds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Epoch 5 finished. Loss:** `0.255608`  | **Duration:** `10.11` seconds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Epoch 6 finished. Loss:** `0.238897`  | **Duration:** `10.12` seconds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Starting epoch 7/30...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from word2gm_fast.training.notebook_training import run_notebook_training\n",
    "\n",
    "# Run General-Purpose Word2GM Training\n",
    "# Hardcoded stable configuration for reliable training\n",
    "\n",
    "print(f\"Starting Word2GM training...\")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Output: {output_dir}\")\n",
    "\n",
    "# Run training with hardcoded stable parameters\n",
    "training_results = run_notebook_training(\n",
    "    training_dataset=triplets_ds,\n",
    "    save_path=output_dir,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=200,           # Good balance of capacity and speed\n",
    "    num_mixtures=1,               # Single Gaussian for simplicity\n",
    "    spherical=True,               # Diagonal covariance\n",
    "    learning_rate=0.1,            # Proven stable rate for Word2GM\n",
    "    epochs=100,                    # Reasonable training duration\n",
    "    adagrad=True,                 # Essential for Word2GM\n",
    "    normclip=True,                # Prevents exploding gradients\n",
    "    norm_cap=10.0,                # Moderate gradient clipping\n",
    "    lower_sig=0.05,               # Balanced variance bounds\n",
    "    upper_sig=1.5,\n",
    "    wout=True,                    # Use output embeddings\n",
    "    tensorboard_log_path=tensorboard_log_dir,\n",
    "    monitor_interval=0.5,         # Regular monitoring\n",
    "    var_scale=0.05,               # Moderate regularization\n",
    "    loss_epsilon=1e-8             # Numerical stability\n",
    ")\n",
    "\n",
    "print(\"â Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a5914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
