{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97648f54",
   "metadata": {},
   "source": [
    "# Word2GM Training Notebook (Clean)\n",
    "\n",
    "This notebook provides a streamlined interface for training Word2GM models with pre-processed corpus data.\n",
    "\n",
    "## Contents:\n",
    "1. **Setup**: GPU configuration and imports\n",
    "2. **Data Loading**: Load pre-processed artifacts and setup training data\n",
    "3. **Training Configuration**: Multiple configuration options from conservative to aggressive\n",
    "4. **Model Training**: Execute training with selected configuration\n",
    "5. **Analysis**: TensorBoard visualization and nearest neighbors exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3f773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683781ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 13:06:48.869708: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-06 13:06:50.080054: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751821610.349422  445730 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751821610.406418  445730 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751821610.898982  445730 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751821610.899014  445730 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751821610.899015  445730 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751821610.899017  445730 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-06 13:06:50.908656: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-06 13:07:05.688306: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-07-06 13:07:05.688306: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"  # Optional, may help with fragmentation\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not set memory growth: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7fd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set project root directory and add `src` to path\n",
    "PROJECT_ROOT = '/scratch/edk202/word2gm-fast'\n",
    "project_root = Path(PROJECT_ROOT)\n",
    "src_path = project_root / 'src'\n",
    "\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from word2gm_fast.models.word2gm_model import Word2GMModel\n",
    "from word2gm_fast.models.config import Word2GMConfig\n",
    "from word2gm_fast.training.notebook_training import run_notebook_training\n",
    "from word2gm_fast.io.artifacts import load_pipeline_artifacts\n",
    "from word2gm_fast.utils.resource_summary import print_resource_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec376e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a277420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>Loading pipeline artifacts from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1850_artifacts</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading token-to-index vocabulary TFRecord from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1850_artifacts/vocab.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 13:07:34.982362: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 134217728\n",
      "2025-07-06 13:07:35.246804: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-07-06 13:07:35.246804: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading index-to-token vocab TFRecord from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1850_artifacts/vocab.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 13:07:35.522844: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Loading triplet TFRecord from: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/1850_artifacts/triplets.tfrecord</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>Triplet TFRecord loaded and parsed</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<pre>All artifacts loaded successfully!</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab_size: 33668\n"
     ]
    }
   ],
   "source": [
    "# Define paths for your corpus artifacts and output\n",
    "dataset_artifacts_dir = (\n",
    "    '/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/6corpus/yearly_files/data/'\n",
    "    '1850_artifacts'\n",
    ")\n",
    "output_dir = '/scratch/edk202/word2gm-fast/output/test_corpus'\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set TensorBoard log directory\n",
    "tensorboard_log_dir = output_dir + '/tensorboard'\n",
    "\n",
    "# Load pipeline artifacts (vocab, triplets, etc.)\n",
    "artifacts = load_pipeline_artifacts(dataset_artifacts_dir)\n",
    "token_to_index_table = artifacts['token_to_index_table']\n",
    "index_to_token_table = artifacts['index_to_token_table']\n",
    "triplets_ds = artifacts['triplets_ds']\n",
    "vocab_size = artifacts['vocab_size']\n",
    "\n",
    "# Build the dataset pipeline: cache -> shuffle -> batch -> prefetch\n",
    "triplets_ds = triplets_ds.cache()\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 10\n",
    "triplets_ds = triplets_ds.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "triplets_ds = triplets_ds.batch(BATCH_SIZE)\n",
    "triplets_ds = triplets_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f'Loaded vocab_size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfff30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Query the token_to_index_table and index_to_token_table\n",
    "test_token = 'king'\n",
    "test_index = 16702\n",
    "\n",
    "# Query token to index\n",
    "token_tensor = tf.constant([test_token])\n",
    "index_result = token_to_index_table.lookup(token_tensor).numpy()[0]\n",
    "print(f\"Index for token '{test_token}':\", index_result)\n",
    "\n",
    "# Query index to token\n",
    "index_tensor = tf.constant([test_index], dtype=tf.int64)\n",
    "token_result = index_to_token_table.lookup(index_tensor).numpy()[0].decode('utf-8')\n",
    "print(f\"Token for index {test_index}:\", token_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a random sample of 50 triplets from a single batch of the current corpus, showing both indices and tokens\n",
    "import random\n",
    "\n",
    "# Take a single batch from the dataset\n",
    "for batch in triplets_ds.take(1):\n",
    "    # If batch is a tuple of tensors (anchor, pos, neg), stack and transpose to shape (batch_size, 3)\n",
    "    if isinstance(batch, tuple) and len(batch) == 3:\n",
    "        anchor, pos, neg = [t.numpy() for t in batch]\n",
    "        triplets_batch = list(zip(anchor, pos, neg))\n",
    "    else:\n",
    "        # If batch is a single tensor of shape (batch_size, 3)\n",
    "        triplets_batch = batch.numpy()\n",
    "    break\n",
    "\n",
    "sample_size = min(50, len(triplets_batch))\n",
    "sampled_indices = random.sample(range(len(triplets_batch)), sample_size)\n",
    "sampled_triplets = [triplets_batch[i] for i in sampled_indices]\n",
    "\n",
    "def idx_to_token(idx):\n",
    "    idx_tensor = tf.constant([idx], dtype=tf.int64)\n",
    "    token = index_to_token_table.lookup(idx_tensor).numpy()[0].decode('utf-8')\n",
    "    return token\n",
    "\n",
    "print(f\"Random sample of {sample_size} triplets from a single batch:\")\n",
    "print(\"Idx: (anchor, pos, neg)\\tTokens: (anchor, pos, neg)\")\n",
    "for i, triplet in enumerate(sampled_triplets):\n",
    "    anchor, pos, neg = triplet\n",
    "    anchor_token = idx_to_token(anchor)\n",
    "    pos_token = idx_to_token(pos)\n",
    "    neg_token = idx_to_token(neg)\n",
    "    print(f\"{i+1:2d}: ({anchor}, {pos}, {neg})\\t({anchor_token}, {pos_token}, {neg_token})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69940b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Word2GMConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=200,\n",
    "    num_mixtures=1,\n",
    "    spherical=True,\n",
    "    norm_cap=20.0,         # Increased\n",
    "    lower_sig=0.01,        # Lowered\n",
    "    upper_sig=2.0,         # Raised\n",
    "    var_scale=0.1,         # Increased\n",
    "    loss_epsilon=1e-8,\n",
    "    wout=True,\n",
    "    max_pe=False,\n",
    ")\n",
    "\n",
    "run_notebook_training(\n",
    "    training_dataset=triplets_ds,\n",
    "    save_path=output_dir,\n",
    "    vocab_size=config.vocab_size,\n",
    "    embedding_size=config.embedding_size,\n",
    "    num_mixtures=config.num_mixtures,\n",
    "    spherical=config.spherical,\n",
    "    learning_rate=1.0,\n",
    "    epochs=30,\n",
    "    adagrad=True,\n",
    "    normclip=True,\n",
    "    norm_cap=config.norm_cap,\n",
    "    lower_sig=config.lower_sig,\n",
    "    upper_sig=config.upper_sig,\n",
    "    var_scale=config.var_scale,\n",
    "    loss_epsilon=config.loss_epsilon,\n",
    "    wout=config.wout,\n",
    "    tensorboard_log_path=tensorboard_log_dir,\n",
    "    monitor_interval=0.5,\n",
    "    profile=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f25c5",
   "metadata": {},
   "source": [
    "## Recommended Starting Training Configurations\n",
    "\n",
    "Here are several good starting parameter configurations for Word2GM training, ordered from conservative to aggressive:\n",
    "\n",
    "### **Configuration 1: Conservative/Stable**\n",
    "- Good for initial experiments and ensuring convergence\n",
    "- Lower learning rates and tighter regularization\n",
    "- Suitable for small to medium datasets\n",
    "\n",
    "### **Configuration 2: Balanced**  \n",
    "- Good middle ground for most use cases\n",
    "- Moderate regularization and learning rates\n",
    "- Recommended starting point for most experiments\n",
    "\n",
    "### **Configuration 3: Aggressive**\n",
    "- Higher learning rates and looser constraints\n",
    "- Good for large datasets with lots of training data\n",
    "- May require more careful monitoring\n",
    "\n",
    "### **Configuration 4: Large Scale**\n",
    "- Optimized for very large vocabularies (100K+ words)\n",
    "- Higher embedding dimensions and adjusted regularization\n",
    "- Suitable for full Google Books datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 1: Conservative/Stable\n",
    "conservative_config = Word2GMConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=128,        # Smaller embedding for stability\n",
    "    num_mixtures=1,\n",
    "    spherical=True,\n",
    "    norm_cap=5.0,             # Tight gradient clipping\n",
    "    lower_sig=0.1,            # Conservative variance bounds\n",
    "    upper_sig=1.0,\n",
    "    var_scale=0.01,           # Strong regularization\n",
    "    loss_epsilon=1e-8,\n",
    "    wout=True,\n",
    "    max_pe=False,\n",
    ")\n",
    "\n",
    "# Configuration 2: Balanced (Recommended Starting Point)\n",
    "balanced_config = Word2GMConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=200,        # Good balance of capacity and speed\n",
    "    num_mixtures=1,\n",
    "    spherical=True,\n",
    "    norm_cap=10.0,            # Moderate gradient clipping\n",
    "    lower_sig=0.05,           # Balanced variance bounds\n",
    "    upper_sig=1.5,\n",
    "    var_scale=0.05,           # Moderate regularization\n",
    "    loss_epsilon=1e-8,\n",
    "    wout=True,\n",
    "    max_pe=False,\n",
    ")\n",
    "\n",
    "# Configuration 3: Aggressive\n",
    "aggressive_config = Word2GMConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=300,        # Higher capacity\n",
    "    num_mixtures=1,\n",
    "    spherical=True,\n",
    "    norm_cap=20.0,            # Looser gradient clipping\n",
    "    lower_sig=0.01,           # Wider variance bounds\n",
    "    upper_sig=2.0,\n",
    "    var_scale=0.1,            # Light regularization\n",
    "    loss_epsilon=1e-8,\n",
    "    wout=True,\n",
    "    max_pe=False,\n",
    ")\n",
    "\n",
    "# Configuration 4: Large Scale\n",
    "large_scale_config = Word2GMConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=512,        # High capacity for large datasets\n",
    "    num_mixtures=1,\n",
    "    spherical=True,\n",
    "    norm_cap=50.0,            # Very loose clipping\n",
    "    lower_sig=0.001,          # Very wide variance bounds\n",
    "    upper_sig=5.0,\n",
    "    var_scale=0.2,            # Minimal regularization\n",
    "    loss_epsilon=1e-8,\n",
    "    wout=True,\n",
    "    max_pe=False,\n",
    ")\n",
    "\n",
    "# Choose which configuration to use\n",
    "config = balanced_config\n",
    "print(f\"Using configuration: embedding_size={config.embedding_size}, norm_cap={config.norm_cap}\")\n",
    "print(f\"Variance bounds: [{config.lower_sig}, {config.upper_sig}], var_scale={config.var_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameter Recommendations\n",
    "\n",
    "def get_training_params(config_name, dataset_size=\"medium\"):\n",
    "    \"\"\"\n",
    "    Get recommended training parameters based on configuration and dataset size.\n",
    "    \n",
    "    Args:\n",
    "        config_name: \"conservative\", \"balanced\", \"aggressive\", or \"large_scale\"\n",
    "        dataset_size: \"small\", \"medium\", \"large\", or \"very_large\"\n",
    "    \"\"\"\n",
    "    \n",
    "    base_params = {\n",
    "        \"conservative\": {\n",
    "            \"learning_rate\": 0.5,\n",
    "            \"epochs\": 15,\n",
    "            \"adagrad\": True,\n",
    "            \"normclip\": True,\n",
    "            \"monitor_interval\": 1.0,\n",
    "        },\n",
    "        \"balanced\": {\n",
    "            \"learning_rate\": 1.0,\n",
    "            \"epochs\": 30,\n",
    "            \"adagrad\": True,\n",
    "            \"normclip\": True,\n",
    "            \"monitor_interval\": 0.5,\n",
    "        },\n",
    "        \"aggressive\": {\n",
    "            \"learning_rate\": 1.5,\n",
    "            \"epochs\": 50,\n",
    "            \"adagrad\": True,\n",
    "            \"normclip\": True,\n",
    "            \"monitor_interval\": 0.25,\n",
    "        },\n",
    "        \"large_scale\": {\n",
    "            \"learning_rate\": 2.0,\n",
    "            \"epochs\": 100,\n",
    "            \"adagrad\": True,\n",
    "            \"normclip\": True,\n",
    "            \"monitor_interval\": 0.1,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Adjust for dataset size\n",
    "    params = base_params[config_name].copy()\n",
    "    \n",
    "    if dataset_size == \"small\":\n",
    "        params[\"epochs\"] = max(10, params[\"epochs\"] // 2)\n",
    "        params[\"learning_rate\"] *= 0.8\n",
    "    elif dataset_size == \"large\":\n",
    "        params[\"epochs\"] = min(100, params[\"epochs\"] * 2)\n",
    "        params[\"learning_rate\"] *= 1.2\n",
    "    elif dataset_size == \"very_large\":\n",
    "        params[\"epochs\"] = min(200, params[\"epochs\"] * 3)\n",
    "        params[\"learning_rate\"] *= 1.5\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Example usage - adjust based on your dataset size\n",
    "dataset_size = \"medium\"  # Change to \"small\", \"medium\", \"large\", or \"very_large\"\n",
    "config_name = \"balanced\"  # Change to match your chosen config\n",
    "\n",
    "training_params = get_training_params(config_name, dataset_size)\n",
    "print(f\"Recommended training parameters for {config_name} config with {dataset_size} dataset:\")\n",
    "for key, value in training_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce21b77",
   "metadata": {},
   "source": [
    "## Parameter Explanations and Tips\n",
    "\n",
    "### **Key Parameters to Tune:**\n",
    "\n",
    "**Model Architecture:**\n",
    "- `embedding_size`: Start with 128-200 for small datasets, 300-512 for large datasets\n",
    "- `num_mixtures`: Keep at 1 initially (Gaussian mixtures add complexity)\n",
    "- `spherical`: Keep True for simplicity (diagonal covariance)\n",
    "\n",
    "**Regularization (Critical for Word2GM):**\n",
    "- `norm_cap`: Gradient clipping threshold (5.0 conservative, 10.0 balanced, 20.0+ aggressive)\n",
    "- `lower_sig`: Minimum variance (0.1 conservative, 0.05 balanced, 0.01 aggressive)\n",
    "- `upper_sig`: Maximum variance (1.0 conservative, 1.5 balanced, 2.0+ aggressive)\n",
    "- `var_scale`: Regularization strength (0.01 strong, 0.05 moderate, 0.1+ light)\n",
    "\n",
    "**Training:**\n",
    "- `learning_rate`: Start with 1.0 (Adagrad will adapt)\n",
    "- `epochs`: 15-30 for initial experiments, 50-100 for final training\n",
    "- `adagrad`: Always use True (essential for Word2GM)\n",
    "- `normclip`: Always use True (prevents exploding gradients)\n",
    "\n",
    "### **What to Monitor:**\n",
    "- **Loss**: Should decrease steadily (watch for plateaus)\n",
    "- **Gradient norms**: Should stay below norm_cap\n",
    "- **Variance values**: Should stay within [lower_sig, upper_sig] bounds\n",
    "- **Training speed**: ~1-2 minutes per epoch for medium datasets\n",
    "\n",
    "### **Common Issues:**\n",
    "- **Loss not decreasing**: Increase learning_rate or decrease var_scale\n",
    "- **Training unstable**: Decrease norm_cap or increase regularization\n",
    "- **Underfitting**: Increase embedding_size or decrease regularization\n",
    "- **Overfitting**: Increase var_scale or decrease embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training with Selected Configuration\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING WORD2GM MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Configuration: {config_name}\")\n",
    "print(f\"Dataset size: {dataset_size}\")\n",
    "print(f\"Vocab size: {vocab_size:,}\")\n",
    "print(f\"Embedding size: {config.embedding_size}\")\n",
    "print(f\"Training epochs: {training_params['epochs']}\")\n",
    "print(f\"Learning rate: {training_params['learning_rate']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run the training\n",
    "run_notebook_training(\n",
    "    training_dataset=triplets_ds,\n",
    "    save_path=output_dir,\n",
    "    vocab_size=config.vocab_size,\n",
    "    embedding_size=config.embedding_size,\n",
    "    num_mixtures=config.num_mixtures,\n",
    "    spherical=config.spherical,\n",
    "    learning_rate=training_params['learning_rate'],\n",
    "    epochs=training_params['epochs'],\n",
    "    adagrad=training_params['adagrad'],\n",
    "    normclip=training_params['normclip'],\n",
    "    norm_cap=config.norm_cap,\n",
    "    lower_sig=config.lower_sig,\n",
    "    upper_sig=config.upper_sig,\n",
    "    var_scale=config.var_scale,\n",
    "    loss_epsilon=config.loss_epsilon,\n",
    "    wout=config.wout,\n",
    "    tensorboard_log_path=tensorboard_log_dir,\n",
    "    monitor_interval=training_params['monitor_interval'],\n",
    "    profile=False,\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model weights saved to: {output_dir}\")\n",
    "print(f\"TensorBoard logs: {tensorboard_log_dir}\")\n",
    "print(f\"Final epoch weights: model_weights_epoch{training_params['epochs']}.weights.h5\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea708dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $tensorboard_log_dir --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest neighbors for a given word using Word2GMModel\n",
    "model = Word2GMModel(config)\n",
    "\n",
    "# Build the model by calling it on a dummy input (tuple of three tensors)\n",
    "dummy_input = (\n",
    "    tf.zeros([1], dtype=tf.int32),  # word_ids\n",
    "    tf.zeros([1], dtype=tf.int32),  # pos_ids\n",
    "    tf.zeros([1], dtype=tf.int32),  # neg_ids\n",
    ")\n",
    "model(dummy_input)\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_weights(output_dir + '/model_weights_epoch30.weights.h5')\n",
    "\n",
    "# Extract vocabulary list from index_to_token_table\n",
    "vocab_indices = tf.range(vocab_size, dtype=tf.int64)\n",
    "vocab_tokens = index_to_token_table.lookup(vocab_indices).numpy()\n",
    "vocab_list = [token.decode('utf-8') if isinstance(token, bytes) else str(token) for token in vocab_tokens]\n",
    "\n",
    "# Choose a query word and get its index\n",
    "query_word = 'good'  # Change this to any word in your vocab\n",
    "try:\n",
    "    query_idx = vocab_list.index(query_word)\n",
    "except ValueError:\n",
    "    raise ValueError(f'Word \"{query_word}\" not found in vocab_list.')\n",
    "\n",
    "# Get nearest neighbor indices (returns indices, distances or a list of (index, distance) pairs)\n",
    "result = model.get_nearest_neighbors(query_idx, k=10)\n",
    "print(\"Result type:\", type(result))\n",
    "print(\"Result:\", result)\n",
    "\n",
    "# Try to unpack if possible, else treat as list of pairs\n",
    "try:\n",
    "    neighbor_indices, neighbor_distances = result\n",
    "    neighbors = [(vocab_list[i], float(d)) for i, d in zip(neighbor_indices, neighbor_distances)]\n",
    "except Exception:\n",
    "    neighbors = [(vocab_list[i], float(d)) for i, d in result]\n",
    "\n",
    "print(f'Nearest neighbors for \"{query_word}\":')\n",
    "for word, dist in neighbors:\n",
    "    print(f'{word}\\t{dist:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: word2gm-fast2",
   "language": "python",
   "name": "word2gm-fast2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
